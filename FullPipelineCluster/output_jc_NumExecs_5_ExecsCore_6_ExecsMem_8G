nohup: ignoring input
18/02/14 13:14:35 INFO SparkContext: Running Spark version 2.1.1.2.6.2.0-205
18/02/14 13:14:35 INFO SecurityManager: Changing view acls to: ciprian
18/02/14 13:14:35 INFO SecurityManager: Changing modify acls to: ciprian
18/02/14 13:14:35 INFO SecurityManager: Changing view acls groups to: 
18/02/14 13:14:35 INFO SecurityManager: Changing modify acls groups to: 
18/02/14 13:14:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ciprian); groups with view permissions: Set(); users  with modify permissions: Set(ciprian); groups with modify permissions: Set()
18/02/14 13:14:35 INFO Utils: Successfully started service 'sparkDriver' on port 39341.
18/02/14 13:14:35 INFO SparkEnv: Registering MapOutputTracker
18/02/14 13:14:35 INFO SparkEnv: Registering BlockManagerMaster
18/02/14 13:14:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/02/14 13:14:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/02/14 13:14:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a6a18600-ca8b-4e0f-9992-0228d6eb76c3
18/02/14 13:14:35 INFO MemoryStore: MemoryStore started with capacity 4.1 GB
18/02/14 13:14:35 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/14 13:14:35 INFO log: Logging initialized @1141ms
18/02/14 13:14:35 INFO Server: jetty-9.2.z-SNAPSHOT
18/02/14 13:14:35 INFO Server: Started @1189ms
18/02/14 13:14:35 INFO ServerConnector: Started ServerConnector@410e1277{HTTP/1.1}{0.0.0.0:4040}
18/02/14 13:14:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5fe8b721{/jobs,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@27e0f2f5{/jobs/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6db66836{/jobs/job,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3f093abe{/jobs/job/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4eeea57d{/stages,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@e24ddd0{/stages/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@548e76f1{/stages/stage,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3dd69f5a{/stages/stage/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1ee4730{/stages/pool,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5003041b{/stages/pool/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16fb356{/storage,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@23a9ba52{/storage/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@70ab80e3{/storage/rdd,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67427b69{/storage/rdd/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@544630b7{/environment,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1095f122{/environment/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3d6300e8{/executors,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@24a1c17f{/executors/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@73511076{/executors/threadDump,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@532721fd{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7fb9f71f{/static,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6815c5f2{/,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@60094a13{/api,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3546d80f{/jobs/job/kill,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3670f00{/stages/stage/kill,null,AVAILABLE,@Spark}
18/02/14 13:14:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.19.3.36:4040
18/02/14 13:14:35 INFO SparkContext: Added JAR file:/home/ciprian/MobileBigDataFramework/FullPipelineCluster/target/scala-2.11/fullpipelinecluster_2.11-0.1.jar at spark://172.19.3.36:39341/jars/fullpipelinecluster_2.11-0.1.jar with timestamp 1518606875988
18/02/14 13:14:36 INFO RMProxy: Connecting to ResourceManager at hadoop-master/172.19.3.36:8050
18/02/14 13:14:36 INFO Client: Requesting a new application from cluster with 5 NodeManagers
18/02/14 13:14:36 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (13824 MB per container)
18/02/14 13:14:36 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
18/02/14 13:14:36 INFO Client: Setting up container launch context for our AM
18/02/14 13:14:36 INFO Client: Setting up the launch environment for our AM container
18/02/14 13:14:36 INFO Client: Preparing resources for our AM container
18/02/14 13:14:37 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://hadoop-master:8020/hdp/apps/2.6.2.0-205/spark2/spark2-hdp-yarn-archive.tar.gz
18/02/14 13:14:37 INFO Client: Source and destination file systems are the same. Not copying hdfs://hadoop-master:8020/hdp/apps/2.6.2.0-205/spark2/spark2-hdp-yarn-archive.tar.gz
18/02/14 13:14:37 INFO Client: Uploading resource file:/tmp/spark-342a3d8c-5c9b-4dcb-b7b8-2e1615060bbd/__spark_conf__4927933563863180869.zip -> hdfs://hadoop-master:8020/user/ciprian/.sparkStaging/application_1518606550421_0002/__spark_conf__.zip
18/02/14 13:14:37 INFO SecurityManager: Changing view acls to: ciprian
18/02/14 13:14:37 INFO SecurityManager: Changing modify acls to: ciprian
18/02/14 13:14:37 INFO SecurityManager: Changing view acls groups to: 
18/02/14 13:14:37 INFO SecurityManager: Changing modify acls groups to: 
18/02/14 13:14:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ciprian); groups with view permissions: Set(); users  with modify permissions: Set(ciprian); groups with modify permissions: Set()
18/02/14 13:14:37 INFO Client: Submitting application application_1518606550421_0002 to ResourceManager
18/02/14 13:14:37 INFO YarnClientImpl: Submitted application application_1518606550421_0002
18/02/14 13:14:37 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1518606550421_0002 and attemptId None
18/02/14 13:14:38 INFO Client: Application report for application_1518606550421_0002 (state: ACCEPTED)
18/02/14 13:14:38 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1518606877613
	 final status: UNDEFINED
	 tracking URL: http://hadoop-master:8088/proxy/application_1518606550421_0002/
	 user: ciprian
18/02/14 13:14:39 INFO Client: Application report for application_1518606550421_0002 (state: ACCEPTED)
18/02/14 13:14:40 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)
18/02/14 13:14:40 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hadoop-master, PROXY_URI_BASES -> http://hadoop-master:8088/proxy/application_1518606550421_0002), /proxy/application_1518606550421_0002
18/02/14 13:14:40 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
18/02/14 13:14:40 INFO Client: Application report for application_1518606550421_0002 (state: RUNNING)
18/02/14 13:14:40 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.19.3.32
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1518606877613
	 final status: UNDEFINED
	 tracking URL: http://hadoop-master:8088/proxy/application_1518606550421_0002/
	 user: ciprian
18/02/14 13:14:40 INFO YarnClientSchedulerBackend: Application application_1518606550421_0002 has started running.
18/02/14 13:14:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33535.
18/02/14 13:14:40 INFO NettyBlockTransferService: Server created on 172.19.3.36:33535
18/02/14 13:14:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/02/14 13:14:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.3.36, 33535, None)
18/02/14 13:14:40 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.3.36:33535 with 4.1 GB RAM, BlockManagerId(driver, 172.19.3.36, 33535, None)
18/02/14 13:14:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.3.36, 33535, None)
18/02/14 13:14:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.3.36, 33535, None)
18/02/14 13:14:40 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d2c9361{/metrics/json,null,AVAILABLE,@Spark}
18/02/14 13:14:41 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/application_1518606550421_0002
18/02/14 13:14:43 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.34:58746) with ID 1
18/02/14 13:14:44 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave2:39849 with 4.1 GB RAM, BlockManagerId(1, hadoop-slave2, 39849, None)
18/02/14 13:14:44 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.35:59084) with ID 3
18/02/14 13:14:44 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.33:51692) with ID 2
18/02/14 13:14:44 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.31:55388) with ID 4
18/02/14 13:14:44 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave1:46007 with 4.1 GB RAM, BlockManagerId(3, hadoop-slave1, 46007, None)
18/02/14 13:14:44 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave3:45071 with 4.1 GB RAM, BlockManagerId(2, hadoop-slave3, 45071, None)
18/02/14 13:14:44 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave5:36807 with 4.1 GB RAM, BlockManagerId(4, hadoop-slave5, 36807, None)
18/02/14 13:14:44 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
18/02/14 13:14:44 INFO SharedState: Warehouse path is 'file:/home/ciprian/MobileBigDataFramework/FullPipelineCluster/spark-warehouse'.
18/02/14 13:14:44 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1eb85a47{/SQL,null,AVAILABLE,@Spark}
18/02/14 13:14:44 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@99a8de3{/SQL/json,null,AVAILABLE,@Spark}
18/02/14 13:14:44 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4863c8ac{/SQL/execution,null,AVAILABLE,@Spark}
18/02/14 13:14:44 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6cdee57{/SQL/execution/json,null,AVAILABLE,@Spark}
18/02/14 13:14:44 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1e56bc9b{/static/sql,null,AVAILABLE,@Spark}
18/02/14 13:14:44 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/02/14 13:14:44 INFO metastore: Trying to connect to metastore with URI thrift://hadoop-master:9083
18/02/14 13:14:44 INFO metastore: Connected to metastore.
18/02/14 13:14:44 INFO SessionState: Created local directory: /tmp/a1863b20-ea28-43ac-a75a-76cf1efc50bf_resources
18/02/14 13:14:44 INFO SessionState: Created HDFS directory: /tmp/hive/ciprian/a1863b20-ea28-43ac-a75a-76cf1efc50bf
18/02/14 13:14:44 INFO SessionState: Created local directory: /tmp/ciprian/a1863b20-ea28-43ac-a75a-76cf1efc50bf
18/02/14 13:14:44 INFO SessionState: Created HDFS directory: /tmp/hive/ciprian/a1863b20-ea28-43ac-a75a-76cf1efc50bf/_tmp_space.db
18/02/14 13:14:44 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/ciprian/MobileBigDataFramework/FullPipelineCluster/spark-warehouse
18/02/14 13:14:44 INFO SparkSqlParser: Parsing command: drop table if exists mi2mi.jaccardcoefficient
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: date
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: double
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: date
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:45 INFO CatalystSqlParser: Parsing command: double
18/02/14 13:14:46 INFO SparkSqlParser: Parsing command: mi2mi.edges
18/02/14 13:14:46 INFO CatalystSqlParser: Parsing command: date
18/02/14 13:14:46 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:46 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:46 INFO CatalystSqlParser: Parsing command: double
18/02/14 13:14:47 INFO CatalystSqlParser: Parsing command: date
18/02/14 13:14:47 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:47 INFO CatalystSqlParser: Parsing command: int
18/02/14 13:14:47 INFO CatalystSqlParser: Parsing command: double
18/02/14 13:14:47 INFO SparkSqlParser: Parsing command: edges
18/02/14 13:14:47 INFO SparkSqlParser: Parsing command: select c.MilanoDate, c.SID1, c.SID2, sum(c.mins) sum_mins from (select b.MilanoDate, b.SID1, b.SID2, b.common_node, min(b.EdgeCost) mins from (select g1.MilanoDate, g1.SID1, g1.SID2, a1.common_node, a1.EdgeCost from edges g1 inner join (select MilanoDate, SID1, SID2 common_node, EdgeCost from edges union all select MilanoDate, SID2, SID1, EdgeCost from edges) a1 on  a1.SID1 in (g1.SID1, g1.SID2) and a1.MilanoDate = g1.MilanoDate inner join (select MilanoDate, SID1, SID2 common_node from edges union all select MilanoDate, SID2, SID1 from edges) a2 on a2.SID1 = g1.SID1 and a2.MilanoDate = g1.MilanoDate inner join (select MilanoDate, SID1, SID2 common_node from edges union all select MilanoDate, SID2, SID1 from edges) a3 on a3.SID1 = g1.SID2 and a3.MilanoDate = g1.MilanoDate where a3.common_node = a1.common_node and a2.common_node = a1.common_node ) b  group by b.MilanoDate, b.SID1, b.SID2, b.common_node ) c  group by c.MilanoDate, c.SID1, c.SID2
18/02/14 13:14:47 INFO FileSourceStrategy: Pruning directories with: 
18/02/14 13:14:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(MilanoDate#10),isnotnull(SID1#11),isnotnull(SID2#12)
18/02/14 13:14:47 INFO FileSourceStrategy: Output Data Schema: struct<MilanoDate: date, SID1: int, SID2: int ... 1 more fields>
18/02/14 13:14:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(MilanoDate),IsNotNull(SID1),IsNotNull(SID2)
18/02/14 13:14:47 INFO FileSourceStrategy: Pruning directories with: 
18/02/14 13:14:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(MilanoDate#25),isnotnull(SID2#27)
18/02/14 13:14:47 INFO FileSourceStrategy: Output Data Schema: struct<MilanoDate: date, SID1: int, SID2: int, EdgeCost: double ... 2 more fields>
18/02/14 13:14:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(MilanoDate),IsNotNull(SID2)
18/02/14 13:14:47 INFO FileSourceStrategy: Pruning directories with: 
18/02/14 13:14:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(MilanoDate#25),isnotnull(SID1#26)
18/02/14 13:14:47 INFO FileSourceStrategy: Output Data Schema: struct<MilanoDate: date, SID1: int, SID2: int, EdgeCost: double ... 2 more fields>
18/02/14 13:14:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(MilanoDate),IsNotNull(SID1)
18/02/14 13:14:47 INFO FileSourceStrategy: Pruning directories with: 
18/02/14 13:14:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(MilanoDate#33),isnotnull(SID1#34),isnotnull(SID2#35)
18/02/14 13:14:47 INFO FileSourceStrategy: Output Data Schema: struct<MilanoDate: date, SID1: int, SID2: int ... 1 more fields>
18/02/14 13:14:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(MilanoDate),IsNotNull(SID1),IsNotNull(SID2)
18/02/14 13:14:47 INFO FileSourceStrategy: Pruning directories with: 
18/02/14 13:14:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(MilanoDate#33),isnotnull(SID2#35),isnotnull(SID1#34)
18/02/14 13:14:47 INFO FileSourceStrategy: Output Data Schema: struct<MilanoDate: date, SID1: int, SID2: int ... 1 more fields>
18/02/14 13:14:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(MilanoDate),IsNotNull(SID2),IsNotNull(SID1)
18/02/14 13:14:47 INFO FileSourceStrategy: Pruning directories with: 
18/02/14 13:14:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(MilanoDate#41),isnotnull(SID1#42),isnotnull(SID2#43)
18/02/14 13:14:47 INFO FileSourceStrategy: Output Data Schema: struct<MilanoDate: date, SID1: int, SID2: int ... 1 more fields>
18/02/14 13:14:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(MilanoDate),IsNotNull(SID1),IsNotNull(SID2)
18/02/14 13:14:47 INFO FileSourceStrategy: Pruning directories with: 
18/02/14 13:14:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(MilanoDate#41),isnotnull(SID2#43),isnotnull(SID1#42)
18/02/14 13:14:47 INFO FileSourceStrategy: Output Data Schema: struct<MilanoDate: date, SID1: int, SID2: int ... 1 more fields>
18/02/14 13:14:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(MilanoDate),IsNotNull(SID2),IsNotNull(SID1)
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 130.298109 ms
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 84.01976 ms
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 11.975073 ms
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 13.296182 ms
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 10.972119 ms
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 6.267751 ms
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 359.4 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.3.36:33535 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 0 from show at JaccardCoefficient.scala:74
18/02/14 13:14:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 6.164594 ms
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 359.4 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.3.36:33535 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 1 from show at JaccardCoefficient.scala:74
18/02/14 13:14:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 6.059474 ms
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 359.4 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.3.36:33535 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 2 from show at JaccardCoefficient.scala:74
18/02/14 13:14:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 359.4 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 32.5 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.3.36:33535 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 3 from show at JaccardCoefficient.scala:74
18/02/14 13:14:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
18/02/14 13:14:48 INFO CodeGenerator: Code generated in 5.416211 ms
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 359.4 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.3.36:33535 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 4 from show at JaccardCoefficient.scala:74
18/02/14 13:14:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
18/02/14 13:14:48 INFO SparkContext: Starting job: show at JaccardCoefficient.scala:74
18/02/14 13:14:48 INFO DAGScheduler: Registering RDD 4 (show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO DAGScheduler: Registering RDD 12 (show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO DAGScheduler: Registering RDD 17 (show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO DAGScheduler: Registering RDD 25 (show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO DAGScheduler: Registering RDD 30 (show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO DAGScheduler: Registering RDD 36 (show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO DAGScheduler: Got job 0 (show at JaccardCoefficient.scala:74) with 1 output partitions
18/02/14 13:14:48 INFO DAGScheduler: Final stage: ResultStage 6 (show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
18/02/14 13:14:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
18/02/14 13:14:48 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[25] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.1 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.7 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.3.36:33535 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
18/02/14 13:14:48 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[25] at show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO YarnScheduler: Adding task set 3.0 with 200 tasks
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 4
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 6
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 13
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 12
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 3
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 5
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 2
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 0
18/02/14 13:14:48 INFO ContextCleaner: Cleaned accumulator 1
18/02/14 13:14:48 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.3 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.1 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.19.3.36:33535 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
18/02/14 13:14:48 INFO DAGScheduler: Submitting 100 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO YarnScheduler: Adding task set 0.0 with 100 tasks
18/02/14 13:14:48 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 0, hadoop-slave3, executor 2, partition 2, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 1, hadoop-slave2, executor 1, partition 0, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 2, hadoop-slave1, executor 3, partition 1, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 3, hadoop-slave5, executor 4, partition 3, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 4, hadoop-slave3, executor 2, partition 4, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 5, hadoop-slave2, executor 1, partition 5, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 6, hadoop-slave1, executor 3, partition 7, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 7, hadoop-slave5, executor 4, partition 6, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 8, hadoop-slave3, executor 2, partition 11, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 9, hadoop-slave2, executor 1, partition 8, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 10, hadoop-slave1, executor 3, partition 9, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 11, hadoop-slave5, executor 4, partition 10, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 12, hadoop-slave3, executor 2, partition 13, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 13, hadoop-slave2, executor 1, partition 12, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 14, hadoop-slave1, executor 3, partition 14, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 15, hadoop-slave5, executor 4, partition 15, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 16, hadoop-slave3, executor 2, partition 16, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 17, hadoop-slave2, executor 1, partition 17, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 18, hadoop-slave1, executor 3, partition 18, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 19, hadoop-slave5, executor 4, partition 19, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 22.0 in stage 3.0 (TID 20, hadoop-slave3, executor 2, partition 22, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 20.0 in stage 3.0 (TID 21, hadoop-slave2, executor 1, partition 20, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 22, hadoop-slave1, executor 3, partition 21, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 23, hadoop-slave5, executor 4, partition 24, NODE_LOCAL, 6941 bytes)
18/02/14 13:14:48 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.7 KB, free 4.1 GB)
18/02/14 13:14:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KB, free 4.1 GB)
18/02/14 13:14:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.19.3.36:33535 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:14:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
18/02/14 13:14:48 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74)
18/02/14 13:14:48 INFO YarnScheduler: Adding task set 1.0 with 200 tasks
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave3:45071 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave1:46007 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave2:39849 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave5:36807 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave2:39849 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave1:46007 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave3:45071 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave5:36807 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 24, hadoop-slave2, executor 1, partition 0, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:54 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 13) in 6019 ms on hadoop-slave2 (executor 1) (1/200)
18/02/14 13:14:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-slave2:39849 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:14:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave2:39849 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 25, hadoop-slave1, executor 3, partition 1, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 2) in 6202 ms on hadoop-slave1 (executor 3) (2/200)
18/02/14 13:14:55 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-slave1:46007 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:14:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave1:46007 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 26, hadoop-slave2, executor 1, partition 3, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 5) in 6405 ms on hadoop-slave2 (executor 1) (3/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 27, hadoop-slave3, executor 2, partition 2, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 22.0 in stage 3.0 (TID 20) in 6481 ms on hadoop-slave3 (executor 2) (4/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 28, hadoop-slave5, executor 4, partition 6, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 11) in 6496 ms on hadoop-slave5 (executor 4) (5/200)
18/02/14 13:14:55 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-slave3:45071 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:14:55 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop-slave5:36807 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 29, hadoop-slave5, executor 4, partition 8, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 15) in 6515 ms on hadoop-slave5 (executor 4) (6/200)
18/02/14 13:14:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave5:36807 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 30, hadoop-slave3, executor 2, partition 4, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 16) in 6550 ms on hadoop-slave3 (executor 2) (7/200)
18/02/14 13:14:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave3:45071 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 31, hadoop-slave3, executor 2, partition 7, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 12) in 6614 ms on hadoop-slave3 (executor 2) (8/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 32, hadoop-slave2, executor 1, partition 5, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 20.0 in stage 3.0 (TID 21) in 6687 ms on hadoop-slave2 (executor 1) (9/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 33, hadoop-slave1, executor 3, partition 9, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 6) in 6763 ms on hadoop-slave1 (executor 3) (10/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 34, hadoop-slave2, executor 1, partition 10, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 9) in 6803 ms on hadoop-slave2 (executor 1) (11/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 35, hadoop-slave1, executor 3, partition 11, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 18) in 6807 ms on hadoop-slave1 (executor 3) (12/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 36, hadoop-slave5, executor 4, partition 14, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 3) in 6918 ms on hadoop-slave5 (executor 4) (13/200)
18/02/14 13:14:55 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 37, hadoop-slave3, executor 2, partition 13, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:55 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 8) in 6922 ms on hadoop-slave3 (executor 2) (14/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 38, hadoop-slave5, executor 4, partition 15, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 19) in 7199 ms on hadoop-slave5 (executor 4) (15/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 39, hadoop-slave3, executor 2, partition 16, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 4) in 7316 ms on hadoop-slave3 (executor 2) (16/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 40, hadoop-slave1, executor 3, partition 12, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 10) in 7384 ms on hadoop-slave1 (executor 3) (17/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 41, hadoop-slave5, executor 4, partition 19, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 23) in 7385 ms on hadoop-slave5 (executor 4) (18/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 42, hadoop-slave2, executor 1, partition 17, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 17) in 7390 ms on hadoop-slave2 (executor 1) (19/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 43, hadoop-slave1, executor 3, partition 18, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 14) in 7395 ms on hadoop-slave1 (executor 3) (20/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 44, hadoop-slave5, executor 4, partition 20, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 7) in 7432 ms on hadoop-slave5 (executor 4) (21/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 45, hadoop-slave2, executor 1, partition 22, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 1) in 7457 ms on hadoop-slave2 (executor 1) (22/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 46, hadoop-slave1, executor 3, partition 21, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 22) in 7487 ms on hadoop-slave1 (executor 3) (23/200)
18/02/14 13:14:56 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 47, hadoop-slave3, executor 2, partition 23, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:56 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 0) in 8042 ms on hadoop-slave3 (executor 2) (24/200)
18/02/14 13:14:58 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 48, hadoop-slave2, executor 1, partition 24, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 24) in 3570 ms on hadoop-slave2 (executor 1) (1/100)
18/02/14 13:14:58 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 49, hadoop-slave3, executor 2, partition 25, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:58 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 37) in 3065 ms on hadoop-slave3 (executor 2) (2/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 50, hadoop-slave5, executor 4, partition 26, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 29) in 3674 ms on hadoop-slave5 (executor 4) (3/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 51, hadoop-slave1, executor 3, partition 27, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 25) in 4382 ms on hadoop-slave1 (executor 3) (4/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 52, hadoop-slave5, executor 4, partition 28, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 28) in 4109 ms on hadoop-slave5 (executor 4) (5/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 53, hadoop-slave2, executor 1, partition 30, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 26) in 4235 ms on hadoop-slave2 (executor 1) (6/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 54, hadoop-slave3, executor 2, partition 29, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 31) in 4123 ms on hadoop-slave3 (executor 2) (7/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 55, hadoop-slave1, executor 3, partition 32, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 33) in 4086 ms on hadoop-slave1 (executor 3) (8/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 56, hadoop-slave1, executor 3, partition 33, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 35) in 4085 ms on hadoop-slave1 (executor 3) (9/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 57, hadoop-slave3, executor 2, partition 31, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 30) in 4460 ms on hadoop-slave3 (executor 2) (10/100)
18/02/14 13:14:59 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 58, hadoop-slave2, executor 1, partition 34, NODE_LOCAL, 6832 bytes)
18/02/14 13:14:59 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 34) in 4214 ms on hadoop-slave2 (executor 1) (11/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 59, hadoop-slave5, executor 4, partition 35, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 44) in 3885 ms on hadoop-slave5 (executor 4) (12/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 60, hadoop-slave3, executor 2, partition 37, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 27) in 4886 ms on hadoop-slave3 (executor 2) (13/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 61, hadoop-slave5, executor 4, partition 38, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 38) in 4197 ms on hadoop-slave5 (executor 4) (14/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 62, hadoop-slave1, executor 3, partition 36, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 46) in 4003 ms on hadoop-slave1 (executor 3) (15/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 63, hadoop-slave2, executor 1, partition 40, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 45) in 4083 ms on hadoop-slave2 (executor 1) (16/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 64, hadoop-slave2, executor 1, partition 41, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 32) in 4882 ms on hadoop-slave2 (executor 1) (17/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 65, hadoop-slave3, executor 2, partition 39, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 39) in 4434 ms on hadoop-slave3 (executor 2) (18/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 66, hadoop-slave5, executor 4, partition 44, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 36) in 4908 ms on hadoop-slave5 (executor 4) (19/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 67, hadoop-slave2, executor 1, partition 50, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 42) in 4562 ms on hadoop-slave2 (executor 1) (20/100)
18/02/14 13:15:00 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 68, hadoop-slave1, executor 3, partition 42, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:00 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 43) in 4578 ms on hadoop-slave1 (executor 3) (21/100)
18/02/14 13:15:01 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 69, hadoop-slave1, executor 3, partition 43, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:01 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 40) in 4985 ms on hadoop-slave1 (executor 3) (22/100)
18/02/14 13:15:01 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 70, hadoop-slave5, executor 4, partition 45, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:01 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 41) in 5149 ms on hadoop-slave5 (executor 4) (23/100)
18/02/14 13:15:01 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 71, hadoop-slave3, executor 2, partition 46, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:01 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 47) in 4923 ms on hadoop-slave3 (executor 2) (24/100)
18/02/14 13:15:02 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 72, hadoop-slave5, executor 4, partition 48, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:02 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 50) in 3500 ms on hadoop-slave5 (executor 4) (25/100)
18/02/14 13:15:02 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 73, hadoop-slave2, executor 1, partition 53, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:02 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 48) in 4519 ms on hadoop-slave2 (executor 1) (26/100)
18/02/14 13:15:02 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 74, hadoop-slave5, executor 4, partition 49, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:02 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 52) in 3529 ms on hadoop-slave5 (executor 4) (27/100)
18/02/14 13:15:03 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 75, hadoop-slave3, executor 2, partition 47, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:03 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 54) in 3519 ms on hadoop-slave3 (executor 2) (28/100)
18/02/14 13:15:03 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 76, hadoop-slave2, executor 1, partition 54, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:03 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 53) in 3861 ms on hadoop-slave2 (executor 1) (29/100)
18/02/14 13:15:03 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 77, hadoop-slave3, executor 2, partition 51, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:03 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 49) in 4689 ms on hadoop-slave3 (executor 2) (30/100)
18/02/14 13:15:03 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 78, hadoop-slave1, executor 3, partition 52, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:03 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 51) in 4310 ms on hadoop-slave1 (executor 3) (31/100)
18/02/14 13:15:03 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 79, hadoop-slave3, executor 2, partition 55, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:03 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 57) in 3959 ms on hadoop-slave3 (executor 2) (32/100)
18/02/14 13:15:03 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 80, hadoop-slave1, executor 3, partition 56, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:03 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 55) in 4135 ms on hadoop-slave1 (executor 3) (33/100)
18/02/14 13:15:03 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 81, hadoop-slave1, executor 3, partition 57, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:03 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 62) in 3548 ms on hadoop-slave1 (executor 3) (34/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 82, hadoop-slave2, executor 1, partition 61, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 58) in 4250 ms on hadoop-slave2 (executor 1) (35/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 83, hadoop-slave1, executor 3, partition 60, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 56) in 4389 ms on hadoop-slave1 (executor 3) (36/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 84, hadoop-slave3, executor 2, partition 58, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 60) in 3920 ms on hadoop-slave3 (executor 2) (37/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 85, hadoop-slave3, executor 2, partition 59, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 65) in 3975 ms on hadoop-slave3 (executor 2) (38/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 86, hadoop-slave2, executor 1, partition 62, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 67) in 3938 ms on hadoop-slave2 (executor 1) (39/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 87, hadoop-slave2, executor 1, partition 63, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 64) in 4364 ms on hadoop-slave2 (executor 1) (40/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 88, hadoop-slave5, executor 4, partition 64, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 61) in 4556 ms on hadoop-slave5 (executor 4) (41/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 89, hadoop-slave5, executor 4, partition 65, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 59) in 4748 ms on hadoop-slave5 (executor 4) (42/100)
18/02/14 13:15:04 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 90, hadoop-slave2, executor 1, partition 66, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:04 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 63) in 4591 ms on hadoop-slave2 (executor 1) (43/100)
18/02/14 13:15:05 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 91, hadoop-slave1, executor 3, partition 74, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:05 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 69) in 3974 ms on hadoop-slave1 (executor 3) (44/100)
18/02/14 13:15:05 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 92, hadoop-slave5, executor 4, partition 67, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:05 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 66) in 4742 ms on hadoop-slave5 (executor 4) (45/100)
18/02/14 13:15:05 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 93, hadoop-slave5, executor 4, partition 68, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:05 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 70) in 4203 ms on hadoop-slave5 (executor 4) (46/100)
18/02/14 13:15:06 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 94, hadoop-slave1, executor 3, partition 75, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:06 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 68) in 5174 ms on hadoop-slave1 (executor 3) (47/100)
18/02/14 13:15:06 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 95, hadoop-slave5, executor 4, partition 69, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:06 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 72) in 3707 ms on hadoop-slave5 (executor 4) (48/100)
18/02/14 13:15:06 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 96, hadoop-slave3, executor 2, partition 70, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:06 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 71) in 4660 ms on hadoop-slave3 (executor 2) (49/100)
18/02/14 13:15:06 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 97, hadoop-slave3, executor 2, partition 71, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:06 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 77) in 3234 ms on hadoop-slave3 (executor 2) (50/100)
18/02/14 13:15:07 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 98, hadoop-slave5, executor 4, partition 72, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:07 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 74) in 4283 ms on hadoop-slave5 (executor 4) (51/100)
18/02/14 13:15:07 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 99, hadoop-slave2, executor 1, partition 73, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:07 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 73) in 4343 ms on hadoop-slave2 (executor 1) (52/100)
18/02/14 13:15:07 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 100, hadoop-slave3, executor 2, partition 81, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:07 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 84) in 3269 ms on hadoop-slave3 (executor 2) (53/100)
18/02/14 13:15:07 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 101, hadoop-slave1, executor 3, partition 76, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:07 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 83) in 3695 ms on hadoop-slave1 (executor 3) (54/100)
18/02/14 13:15:07 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 102, hadoop-slave1, executor 3, partition 77, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:07 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 81) in 3938 ms on hadoop-slave1 (executor 3) (55/100)
18/02/14 13:15:08 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 103, hadoop-slave2, executor 1, partition 79, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:08 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 76) in 4978 ms on hadoop-slave2 (executor 1) (56/100)
18/02/14 13:15:08 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 104, hadoop-slave3, executor 2, partition 84, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:08 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 75) in 5489 ms on hadoop-slave3 (executor 2) (57/100)
18/02/14 13:15:08 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 105, hadoop-slave2, executor 1, partition 80, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:08 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 82) in 4620 ms on hadoop-slave2 (executor 1) (58/100)
18/02/14 13:15:08 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 106, hadoop-slave3, executor 2, partition 85, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:08 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 79) in 4997 ms on hadoop-slave3 (executor 2) (59/100)
18/02/14 13:15:08 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 107, hadoop-slave5, executor 4, partition 78, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:08 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 93) in 3236 ms on hadoop-slave5 (executor 4) (60/100)
18/02/14 13:15:08 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 108, hadoop-slave2, executor 1, partition 82, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:08 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 90) in 3861 ms on hadoop-slave2 (executor 1) (61/100)
18/02/14 13:15:08 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 109, hadoop-slave1, executor 3, partition 83, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:08 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 80) in 5029 ms on hadoop-slave1 (executor 3) (62/100)
18/02/14 13:15:09 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 110, hadoop-slave5, executor 4, partition 88, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:09 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 89) in 4575 ms on hadoop-slave5 (executor 4) (63/100)
18/02/14 13:15:09 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 111, hadoop-slave3, executor 2, partition 86, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:09 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 85) in 4978 ms on hadoop-slave3 (executor 2) (64/100)
18/02/14 13:15:09 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 112, hadoop-slave1, executor 3, partition 87, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:09 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 78) in 5945 ms on hadoop-slave1 (executor 3) (65/100)
18/02/14 13:15:09 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 113, hadoop-slave2, executor 1, partition 89, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:09 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 86) in 5148 ms on hadoop-slave2 (executor 1) (66/100)
18/02/14 13:15:10 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 114, hadoop-slave2, executor 1, partition 91, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:10 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 87) in 5267 ms on hadoop-slave2 (executor 1) (67/100)
18/02/14 13:15:10 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 115, hadoop-slave5, executor 4, partition 90, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:10 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 98) in 2964 ms on hadoop-slave5 (executor 4) (68/100)
18/02/14 13:15:10 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 116, hadoop-slave5, executor 4, partition 92, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:10 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 88) in 5523 ms on hadoop-slave5 (executor 4) (69/100)
18/02/14 13:15:10 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 117, hadoop-slave5, executor 4, partition 94, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:10 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 92) in 5101 ms on hadoop-slave5 (executor 4) (70/100)
18/02/14 13:15:10 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 118, hadoop-slave3, executor 2, partition 93, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:10 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 97) in 3825 ms on hadoop-slave3 (executor 2) (71/100)
18/02/14 13:15:10 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 119, hadoop-slave1, executor 3, partition 95, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:10 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 94) in 4756 ms on hadoop-slave1 (executor 3) (72/100)
18/02/14 13:15:10 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 120, hadoop-slave1, executor 3, partition 97, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:10 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 91) in 5748 ms on hadoop-slave1 (executor 3) (73/100)
18/02/14 13:15:11 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 121, hadoop-slave5, executor 4, partition 96, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:11 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 95) in 4810 ms on hadoop-slave5 (executor 4) (74/100)
18/02/14 13:15:11 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 122, hadoop-slave3, executor 2, partition 99, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:11 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 96) in 5236 ms on hadoop-slave3 (executor 2) (75/100)
18/02/14 13:15:12 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 123, hadoop-slave3, executor 2, partition 2, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:12 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 106) in 3427 ms on hadoop-slave3 (executor 2) (76/100)
18/02/14 13:15:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-slave3:45071 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:15:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave3:45071 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:12 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 124, hadoop-slave2, executor 1, partition 98, NODE_LOCAL, 6832 bytes)
18/02/14 13:15:12 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 103) in 4058 ms on hadoop-slave2 (executor 1) (77/100)
18/02/14 13:15:12 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 125, hadoop-slave3, executor 2, partition 3, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:12 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 100) in 5172 ms on hadoop-slave3 (executor 2) (78/100)
18/02/14 13:15:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 126, hadoop-slave1, executor 3, partition 0, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:12 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 102) in 4998 ms on hadoop-slave1 (executor 3) (79/100)
18/02/14 13:15:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-slave1:46007 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:15:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave1:46007 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:12 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 127, hadoop-slave3, executor 2, partition 4, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:12 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 104) in 4358 ms on hadoop-slave3 (executor 2) (80/100)
18/02/14 13:15:13 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 128, hadoop-slave2, executor 1, partition 1, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:13 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 99) in 5716 ms on hadoop-slave2 (executor 1) (81/100)
18/02/14 13:15:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-slave2:39849 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:15:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave2:39849 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:13 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 129, hadoop-slave2, executor 1, partition 5, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:13 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 105) in 4419 ms on hadoop-slave2 (executor 1) (82/100)
18/02/14 13:15:13 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 130, hadoop-slave5, executor 4, partition 6, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:13 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 110) in 3700 ms on hadoop-slave5 (executor 4) (83/100)
18/02/14 13:15:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop-slave5:36807 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:15:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave5:36807 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:13 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 131, hadoop-slave5, executor 4, partition 8, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:13 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 107) in 4923 ms on hadoop-slave5 (executor 4) (84/100)
18/02/14 13:15:13 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 132, hadoop-slave1, executor 3, partition 7, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:13 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 109) in 4913 ms on hadoop-slave1 (executor 3) (85/100)
18/02/14 13:15:13 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 133, hadoop-slave2, executor 1, partition 9, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:13 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 108) in 4950 ms on hadoop-slave2 (executor 1) (86/100)
18/02/14 13:15:14 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 134, hadoop-slave5, executor 4, partition 10, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:14 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 116) in 4355 ms on hadoop-slave5 (executor 4) (87/100)
18/02/14 13:15:14 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 135, hadoop-slave5, executor 4, partition 14, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:14 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 117) in 4180 ms on hadoop-slave5 (executor 4) (88/100)
18/02/14 13:15:14 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 136, hadoop-slave1, executor 3, partition 11, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:14 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 101) in 6981 ms on hadoop-slave1 (executor 3) (89/100)
18/02/14 13:15:14 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 137, hadoop-slave2, executor 1, partition 12, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:14 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 113) in 4963 ms on hadoop-slave2 (executor 1) (90/100)
18/02/14 13:15:14 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 138, hadoop-slave3, executor 2, partition 13, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:14 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 118) in 4281 ms on hadoop-slave3 (executor 2) (91/100)
18/02/14 13:15:15 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 139, hadoop-slave3, executor 2, partition 15, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:15 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 111) in 5570 ms on hadoop-slave3 (executor 2) (92/100)
18/02/14 13:15:15 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 140, hadoop-slave5, executor 4, partition 16, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:15 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 115) in 4966 ms on hadoop-slave5 (executor 4) (93/100)
18/02/14 13:15:15 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 141, hadoop-slave1, executor 3, partition 18, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:15 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 112) in 5615 ms on hadoop-slave1 (executor 3) (94/100)
18/02/14 13:15:15 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 142, hadoop-slave5, executor 4, partition 19, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:15 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 121) in 4263 ms on hadoop-slave5 (executor 4) (95/100)
18/02/14 13:15:15 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 143, hadoop-slave2, executor 1, partition 17, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:15 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 114) in 5508 ms on hadoop-slave2 (executor 1) (96/100)
18/02/14 13:15:15 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 144, hadoop-slave1, executor 3, partition 21, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:15 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 120) in 4635 ms on hadoop-slave1 (executor 3) (97/100)
18/02/14 13:15:16 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 145, hadoop-slave1, executor 3, partition 23, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:16 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 119) in 5845 ms on hadoop-slave1 (executor 3) (98/100)
18/02/14 13:15:16 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 146, hadoop-slave3, executor 2, partition 22, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:16 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 122) in 5093 ms on hadoop-slave3 (executor 2) (99/100)
18/02/14 13:15:17 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 147, hadoop-slave2, executor 1, partition 20, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:17 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 124) in 5336 ms on hadoop-slave2 (executor 1) (100/100)
18/02/14 13:15:17 INFO DAGScheduler: ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) finished in 28.858 s
18/02/14 13:15:17 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/02/14 13:15:17 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:15:17 INFO DAGScheduler: running: Set(ShuffleMapStage 1, ShuffleMapStage 3)
18/02/14 13:15:17 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:15:17 INFO DAGScheduler: failed: Set()
18/02/14 13:15:18 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 148, hadoop-slave2, executor 1, partition 24, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:18 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 128) in 5443 ms on hadoop-slave2 (executor 1) (1/200)
18/02/14 13:15:18 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 149, hadoop-slave3, executor 2, partition 25, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:18 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 125) in 5918 ms on hadoop-slave3 (executor 2) (2/200)
18/02/14 13:15:18 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 150, hadoop-slave5, executor 4, partition 26, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:18 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 131) in 4797 ms on hadoop-slave5 (executor 4) (3/200)
18/02/14 13:15:18 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 151, hadoop-slave3, executor 2, partition 29, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:18 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 127) in 5624 ms on hadoop-slave3 (executor 2) (4/200)
18/02/14 13:15:18 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 152, hadoop-slave1, executor 3, partition 27, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 126) in 5838 ms on hadoop-slave1 (executor 3) (5/200)
18/02/14 13:15:18 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 153, hadoop-slave3, executor 2, partition 31, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:18 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 123) in 6592 ms on hadoop-slave3 (executor 2) (6/200)
18/02/14 13:15:19 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 154, hadoop-slave5, executor 4, partition 28, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:19 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 134) in 5218 ms on hadoop-slave5 (executor 4) (7/200)
18/02/14 13:15:20 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 155, hadoop-slave3, executor 2, partition 32, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:20 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 138) in 5247 ms on hadoop-slave3 (executor 2) (8/200)
18/02/14 13:15:20 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 156, hadoop-slave1, executor 3, partition 30, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:20 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 132) in 6416 ms on hadoop-slave1 (executor 3) (9/200)
18/02/14 13:15:20 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 157, hadoop-slave1, executor 3, partition 33, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:20 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 136) in 5471 ms on hadoop-slave1 (executor 3) (10/200)
18/02/14 13:15:20 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 158, hadoop-slave2, executor 1, partition 34, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:20 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 133) in 6512 ms on hadoop-slave2 (executor 1) (11/200)
18/02/14 13:15:20 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 159, hadoop-slave5, executor 4, partition 35, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:20 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 142) in 5103 ms on hadoop-slave5 (executor 4) (12/200)
18/02/14 13:15:20 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 160, hadoop-slave2, executor 1, partition 36, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:20 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 129) in 7469 ms on hadoop-slave2 (executor 1) (13/200)
18/02/14 13:15:20 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 161, hadoop-slave5, executor 4, partition 38, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:20 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 130) in 7669 ms on hadoop-slave5 (executor 4) (14/200)
18/02/14 13:15:21 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 162, hadoop-slave2, executor 1, partition 40, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:21 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 137) in 6196 ms on hadoop-slave2 (executor 1) (15/200)
18/02/14 13:15:21 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 163, hadoop-slave3, executor 2, partition 37, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:21 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 146) in 4398 ms on hadoop-slave3 (executor 2) (16/200)
18/02/14 13:15:22 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 164, hadoop-slave1, executor 3, partition 39, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:22 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 144) in 6513 ms on hadoop-slave1 (executor 3) (17/200)
18/02/14 13:15:22 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 165, hadoop-slave3, executor 2, partition 41, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:22 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 139) in 7097 ms on hadoop-slave3 (executor 2) (18/200)
18/02/14 13:15:22 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 166, hadoop-slave5, executor 4, partition 44, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:22 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 135) in 7715 ms on hadoop-slave5 (executor 4) (19/200)
18/02/14 13:15:22 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 167, hadoop-slave5, executor 4, partition 45, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:22 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 140) in 7352 ms on hadoop-slave5 (executor 4) (20/200)
18/02/14 13:15:22 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 168, hadoop-slave1, executor 3, partition 42, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:22 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 145) in 6226 ms on hadoop-slave1 (executor 3) (21/200)
18/02/14 13:15:23 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 169, hadoop-slave1, executor 3, partition 43, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:23 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 152) in 5011 ms on hadoop-slave1 (executor 3) (22/200)
18/02/14 13:15:23 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 170, hadoop-slave3, executor 2, partition 46, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:23 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 149) in 5339 ms on hadoop-slave3 (executor 2) (23/200)
18/02/14 13:15:24 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 171, hadoop-slave2, executor 1, partition 50, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:24 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 143) in 8713 ms on hadoop-slave2 (executor 1) (24/200)
18/02/14 13:15:24 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 172, hadoop-slave1, executor 3, partition 47, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:24 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 156) in 4529 ms on hadoop-slave1 (executor 3) (25/200)
18/02/14 13:15:24 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 173, hadoop-slave3, executor 2, partition 48, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:24 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 153) in 6123 ms on hadoop-slave3 (executor 2) (26/200)
18/02/14 13:15:25 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 174, hadoop-slave5, executor 4, partition 49, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:25 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 159) in 4663 ms on hadoop-slave5 (executor 4) (27/200)
18/02/14 13:15:25 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 175, hadoop-slave2, executor 1, partition 53, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:25 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 160) in 4486 ms on hadoop-slave2 (executor 1) (28/200)
18/02/14 13:15:25 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 176, hadoop-slave5, executor 4, partition 51, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:25 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 150) in 6579 ms on hadoop-slave5 (executor 4) (29/200)
18/02/14 13:15:25 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 177, hadoop-slave2, executor 1, partition 54, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:25 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 158) in 4971 ms on hadoop-slave2 (executor 1) (30/200)
18/02/14 13:15:25 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 178, hadoop-slave1, executor 3, partition 52, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:25 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 141) in 10276 ms on hadoop-slave1 (executor 3) (31/200)
18/02/14 13:15:25 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 179, hadoop-slave5, executor 4, partition 58, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:25 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 154) in 5755 ms on hadoop-slave5 (executor 4) (32/200)
18/02/14 13:15:25 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 180, hadoop-slave3, executor 2, partition 55, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:25 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 155) in 5716 ms on hadoop-slave3 (executor 2) (33/200)
18/02/14 13:15:26 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 181, hadoop-slave3, executor 2, partition 56, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:26 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 163) in 5242 ms on hadoop-slave3 (executor 2) (34/200)
18/02/14 13:15:26 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 182, hadoop-slave2, executor 1, partition 57, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:26 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 162) in 5673 ms on hadoop-slave2 (executor 1) (35/200)
18/02/14 13:15:27 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 183, hadoop-slave1, executor 3, partition 60, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:27 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 157) in 6798 ms on hadoop-slave1 (executor 3) (36/200)
18/02/14 13:15:27 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 184, hadoop-slave1, executor 3, partition 63, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:27 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 164) in 5194 ms on hadoop-slave1 (executor 3) (37/200)
18/02/14 13:15:27 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 185, hadoop-slave3, executor 2, partition 59, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:27 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 165) in 5141 ms on hadoop-slave3 (executor 2) (38/200)
18/02/14 13:15:27 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 186, hadoop-slave5, executor 4, partition 64, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:27 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 161) in 7105 ms on hadoop-slave5 (executor 4) (39/200)
18/02/14 13:15:27 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 187, hadoop-slave2, executor 1, partition 61, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:27 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 147) in 10266 ms on hadoop-slave2 (executor 1) (40/200)
18/02/14 13:15:28 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 188, hadoop-slave3, executor 2, partition 62, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:28 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 151) in 9438 ms on hadoop-slave3 (executor 2) (41/200)
18/02/14 13:15:28 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 189, hadoop-slave1, executor 3, partition 66, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:28 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 168) in 5200 ms on hadoop-slave1 (executor 3) (42/200)
18/02/14 13:15:28 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 190, hadoop-slave2, executor 1, partition 65, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:28 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 148) in 9579 ms on hadoop-slave2 (executor 1) (43/200)
18/02/14 13:15:28 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 191, hadoop-slave2, executor 1, partition 67, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:28 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 171) in 4533 ms on hadoop-slave2 (executor 1) (44/200)
18/02/14 13:15:29 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 192, hadoop-slave3, executor 2, partition 68, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:29 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 170) in 5216 ms on hadoop-slave3 (executor 2) (45/200)
18/02/14 13:15:29 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 193, hadoop-slave5, executor 4, partition 69, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:29 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 166) in 6885 ms on hadoop-slave5 (executor 4) (46/200)
18/02/14 13:15:29 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 194, hadoop-slave5, executor 4, partition 71, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:29 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 167) in 6766 ms on hadoop-slave5 (executor 4) (47/200)
18/02/14 13:15:29 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 195, hadoop-slave1, executor 3, partition 74, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:29 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 169) in 5768 ms on hadoop-slave1 (executor 3) (48/200)
18/02/14 13:15:30 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 196, hadoop-slave2, executor 1, partition 70, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:30 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 175) in 5100 ms on hadoop-slave2 (executor 1) (49/200)
18/02/14 13:15:30 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 197, hadoop-slave2, executor 1, partition 72, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:30 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 177) in 4945 ms on hadoop-slave2 (executor 1) (50/200)
18/02/14 13:15:30 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 198, hadoop-slave3, executor 2, partition 75, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:30 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 180) in 4397 ms on hadoop-slave3 (executor 2) (51/200)
18/02/14 13:15:31 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 199, hadoop-slave5, executor 4, partition 73, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:31 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 174) in 6077 ms on hadoop-slave5 (executor 4) (52/200)
18/02/14 13:15:31 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 200, hadoop-slave1, executor 3, partition 76, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:31 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 172) in 6454 ms on hadoop-slave1 (executor 3) (53/200)
18/02/14 13:15:31 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 201, hadoop-slave2, executor 1, partition 77, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:31 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 182) in 4731 ms on hadoop-slave2 (executor 1) (54/200)
18/02/14 13:15:31 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 202, hadoop-slave5, executor 4, partition 78, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:31 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 176) in 6489 ms on hadoop-slave5 (executor 4) (55/200)
18/02/14 13:15:31 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 203, hadoop-slave3, executor 2, partition 81, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:31 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 181) in 5540 ms on hadoop-slave3 (executor 2) (56/200)
18/02/14 13:15:32 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 204, hadoop-slave3, executor 2, partition 84, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:32 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 185) in 4896 ms on hadoop-slave3 (executor 2) (57/200)
18/02/14 13:15:32 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 205, hadoop-slave3, executor 2, partition 85, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:32 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 173) in 7742 ms on hadoop-slave3 (executor 2) (58/200)
18/02/14 13:15:32 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 206, hadoop-slave1, executor 3, partition 79, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:32 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 178) in 7207 ms on hadoop-slave1 (executor 3) (59/200)
18/02/14 13:15:32 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 207, hadoop-slave5, executor 4, partition 80, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:32 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 179) in 7171 ms on hadoop-slave5 (executor 4) (60/200)
18/02/14 13:15:32 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 208, hadoop-slave1, executor 3, partition 82, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:32 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 184) in 5597 ms on hadoop-slave1 (executor 3) (61/200)
18/02/14 13:15:33 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 209, hadoop-slave1, executor 3, partition 83, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:33 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 189) in 5163 ms on hadoop-slave1 (executor 3) (62/200)
18/02/14 13:15:33 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 210, hadoop-slave2, executor 1, partition 87, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:33 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 187) in 5322 ms on hadoop-slave2 (executor 1) (63/200)
18/02/14 13:15:34 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 211, hadoop-slave1, executor 3, partition 86, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:34 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 183) in 6970 ms on hadoop-slave1 (executor 3) (64/200)
18/02/14 13:15:34 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 212, hadoop-slave2, executor 1, partition 89, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:34 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 190) in 6453 ms on hadoop-slave2 (executor 1) (65/200)
18/02/14 13:15:34 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 213, hadoop-slave3, executor 2, partition 88, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:34 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 188) in 6604 ms on hadoop-slave3 (executor 2) (66/200)
18/02/14 13:15:34 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 214, hadoop-slave2, executor 1, partition 91, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:34 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 196) in 4422 ms on hadoop-slave2 (executor 1) (67/200)
18/02/14 13:15:34 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 215, hadoop-slave5, executor 4, partition 90, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:34 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 186) in 6972 ms on hadoop-slave5 (executor 4) (68/200)
18/02/14 13:15:35 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 216, hadoop-slave3, executor 2, partition 92, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:35 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 192) in 6051 ms on hadoop-slave3 (executor 2) (69/200)
18/02/14 13:15:35 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 217, hadoop-slave5, executor 4, partition 94, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:35 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 194) in 6356 ms on hadoop-slave5 (executor 4) (70/200)
18/02/14 13:15:36 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 218, hadoop-slave2, executor 1, partition 93, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:36 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 201) in 4823 ms on hadoop-slave2 (executor 1) (71/200)
18/02/14 13:15:36 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 219, hadoop-slave5, executor 4, partition 95, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:36 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 193) in 7276 ms on hadoop-slave5 (executor 4) (72/200)
18/02/14 13:15:36 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 220, hadoop-slave1, executor 3, partition 97, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:36 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 195) in 7196 ms on hadoop-slave1 (executor 3) (73/200)
18/02/14 13:15:36 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 221, hadoop-slave2, executor 1, partition 96, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:36 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 197) in 6623 ms on hadoop-slave2 (executor 1) (74/200)
18/02/14 13:15:37 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 222, hadoop-slave3, executor 2, partition 99, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:37 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 203) in 5278 ms on hadoop-slave3 (executor 2) (75/200)
18/02/14 13:15:37 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 223, hadoop-slave3, executor 2, partition 102, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:37 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 198) in 7744 ms on hadoop-slave3 (executor 2) (76/200)
18/02/14 13:15:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave3:45071 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:38 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 224, hadoop-slave1, executor 3, partition 98, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:38 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 200) in 6927 ms on hadoop-slave1 (executor 3) (77/200)
18/02/14 13:15:38 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 225, hadoop-slave2, executor 1, partition 100, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:38 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 210) in 4911 ms on hadoop-slave2 (executor 1) (78/200)
18/02/14 13:15:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave2:39849 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:39 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 226, hadoop-slave3, executor 2, partition 103, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:39 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 204) in 6976 ms on hadoop-slave3 (executor 2) (79/200)
18/02/14 13:15:39 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 227, hadoop-slave3, executor 2, partition 104, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:39 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 205) in 6544 ms on hadoop-slave3 (executor 2) (80/200)
18/02/14 13:15:39 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 228, hadoop-slave1, executor 3, partition 101, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:39 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 208) in 6681 ms on hadoop-slave1 (executor 3) (81/200)
18/02/14 13:15:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave1:46007 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:40 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 229, hadoop-slave2, executor 1, partition 105, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:40 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 212) in 5799 ms on hadoop-slave2 (executor 1) (82/200)
18/02/14 13:15:40 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 230, hadoop-slave3, executor 2, partition 107, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:40 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 213) in 5821 ms on hadoop-slave3 (executor 2) (83/200)
18/02/14 13:15:40 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 231, hadoop-slave3, executor 2, partition 111, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:40 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 216) in 5425 ms on hadoop-slave3 (executor 2) (84/200)
18/02/14 13:15:40 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 232, hadoop-slave1, executor 3, partition 109, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:40 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 209) in 7388 ms on hadoop-slave1 (executor 3) (85/200)
18/02/14 13:15:40 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 233, hadoop-slave1, executor 3, partition 110, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:40 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 206) in 7795 ms on hadoop-slave1 (executor 3) (86/200)
18/02/14 13:15:40 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 234, hadoop-slave1, executor 3, partition 112, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:40 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 211) in 6542 ms on hadoop-slave1 (executor 3) (87/200)
18/02/14 13:15:40 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 235, hadoop-slave2, executor 1, partition 106, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:40 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 214) in 5970 ms on hadoop-slave2 (executor 1) (88/200)
18/02/14 13:15:41 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 236, hadoop-slave2, executor 1, partition 108, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:41 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 191) in 12495 ms on hadoop-slave2 (executor 1) (89/200)
18/02/14 13:15:41 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 237, hadoop-slave2, executor 1, partition 113, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:41 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 218) in 5577 ms on hadoop-slave2 (executor 1) (90/200)
18/02/14 13:15:41 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 238, hadoop-slave3, executor 2, partition 115, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:41 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 222) in 4696 ms on hadoop-slave3 (executor 2) (91/200)
18/02/14 13:15:42 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 239, hadoop-slave2, executor 1, partition 116, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:42 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 221) in 5404 ms on hadoop-slave2 (executor 1) (92/200)
18/02/14 13:15:44 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 240, hadoop-slave2, executor 1, partition 117, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:44 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 225) in 6278 ms on hadoop-slave2 (executor 1) (93/200)
18/02/14 13:15:45 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 241, hadoop-slave2, executor 1, partition 120, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:45 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 229) in 5032 ms on hadoop-slave2 (executor 1) (94/200)
18/02/14 13:15:45 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 242, hadoop-slave3, executor 2, partition 118, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:45 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 223) in 7856 ms on hadoop-slave3 (executor 2) (95/200)
18/02/14 13:15:45 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 243, hadoop-slave3, executor 2, partition 119, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:45 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 227) in 6608 ms on hadoop-slave3 (executor 2) (96/200)
18/02/14 13:15:46 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 244, hadoop-slave3, executor 2, partition 122, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:46 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 226) in 6770 ms on hadoop-slave3 (executor 2) (97/200)
18/02/14 13:15:46 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 245, hadoop-slave2, executor 1, partition 123, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:46 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 236) in 4814 ms on hadoop-slave2 (executor 1) (98/200)
18/02/14 13:15:46 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 246, hadoop-slave5, executor 4, partition 114, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:46 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 199) in 15165 ms on hadoop-slave5 (executor 4) (99/200)
18/02/14 13:15:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave5:36807 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:15:46 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 247, hadoop-slave1, executor 3, partition 121, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:46 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 224) in 8606 ms on hadoop-slave1 (executor 3) (100/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 248, hadoop-slave3, executor 2, partition 124, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 231) in 6489 ms on hadoop-slave3 (executor 2) (101/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 249, hadoop-slave1, executor 3, partition 126, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 220) in 10701 ms on hadoop-slave1 (executor 3) (102/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 250, hadoop-slave5, executor 4, partition 125, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 215) in 12471 ms on hadoop-slave5 (executor 4) (103/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 251, hadoop-slave3, executor 2, partition 129, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 230) in 7051 ms on hadoop-slave3 (executor 2) (104/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 252, hadoop-slave1, executor 3, partition 127, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 232) in 6943 ms on hadoop-slave1 (executor 3) (105/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 253, hadoop-slave3, executor 2, partition 131, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 238) in 5736 ms on hadoop-slave3 (executor 2) (106/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 254, hadoop-slave5, executor 4, partition 128, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 202) in 16331 ms on hadoop-slave5 (executor 4) (107/200)
18/02/14 13:15:47 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 255, hadoop-slave2, executor 1, partition 130, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:47 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 237) in 6104 ms on hadoop-slave2 (executor 1) (108/200)
18/02/14 13:15:48 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 256, hadoop-slave5, executor 4, partition 134, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:48 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 217) in 12934 ms on hadoop-slave5 (executor 4) (109/200)
18/02/14 13:15:48 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 257, hadoop-slave1, executor 3, partition 132, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:48 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 228) in 9230 ms on hadoop-slave1 (executor 3) (110/200)
18/02/14 13:15:48 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 258, hadoop-slave1, executor 3, partition 133, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:48 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 234) in 8211 ms on hadoop-slave1 (executor 3) (111/200)
18/02/14 13:15:48 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 259, hadoop-slave2, executor 1, partition 135, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:48 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 235) in 8245 ms on hadoop-slave2 (executor 1) (112/200)
18/02/14 13:15:49 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 260, hadoop-slave2, executor 1, partition 136, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:49 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 239) in 6768 ms on hadoop-slave2 (executor 1) (113/200)
18/02/14 13:15:49 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 261, hadoop-slave5, executor 4, partition 138, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:49 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 207) in 16321 ms on hadoop-slave5 (executor 4) (114/200)
18/02/14 13:15:49 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 262, hadoop-slave2, executor 1, partition 140, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:49 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 240) in 4820 ms on hadoop-slave2 (executor 1) (115/200)
18/02/14 13:15:49 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 263, hadoop-slave1, executor 3, partition 137, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:49 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 233) in 8889 ms on hadoop-slave1 (executor 3) (116/200)
18/02/14 13:15:49 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 264, hadoop-slave5, executor 4, partition 139, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:49 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 219) in 13296 ms on hadoop-slave5 (executor 4) (117/200)
18/02/14 13:15:51 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 265, hadoop-slave2, executor 1, partition 141, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:51 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 241) in 5940 ms on hadoop-slave2 (executor 1) (118/200)
18/02/14 13:15:56 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 266, hadoop-slave2, executor 1, partition 144, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:56 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 255) in 8797 ms on hadoop-slave2 (executor 1) (119/200)
18/02/14 13:15:57 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 267, hadoop-slave5, executor 4, partition 145, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:57 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 246) in 11422 ms on hadoop-slave5 (executor 4) (120/200)
18/02/14 13:15:58 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 268, hadoop-slave2, executor 1, partition 150, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:58 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 260) in 9556 ms on hadoop-slave2 (executor 1) (121/200)
18/02/14 13:15:58 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 269, hadoop-slave5, executor 4, partition 146, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:58 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 250) in 11286 ms on hadoop-slave5 (executor 4) (122/200)
18/02/14 13:15:59 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 270, hadoop-slave5, executor 4, partition 148, NODE_LOCAL, 6941 bytes)
18/02/14 13:15:59 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 254) in 11201 ms on hadoop-slave5 (executor 4) (123/200)
18/02/14 13:16:00 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 271, hadoop-slave2, executor 1, partition 153, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:00 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 262) in 10806 ms on hadoop-slave2 (executor 1) (124/200)
18/02/14 13:16:00 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 272, hadoop-slave2, executor 1, partition 154, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:00 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 265) in 8975 ms on hadoop-slave2 (executor 1) (125/200)
18/02/14 13:16:00 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 273, hadoop-slave2, executor 1, partition 155, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:00 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 245) in 14589 ms on hadoop-slave2 (executor 1) (126/200)
18/02/14 13:16:00 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 274, hadoop-slave2, executor 1, partition 156, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:00 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 259) in 11861 ms on hadoop-slave2 (executor 1) (127/200)
18/02/14 13:16:01 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 275, hadoop-slave5, executor 4, partition 149, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:01 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 261) in 12396 ms on hadoop-slave5 (executor 4) (128/200)
18/02/14 13:16:01 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 276, hadoop-slave1, executor 3, partition 142, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:01 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 252) in 14368 ms on hadoop-slave1 (executor 3) (129/200)
18/02/14 13:16:01 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 277, hadoop-slave5, executor 4, partition 151, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:01 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 264) in 12086 ms on hadoop-slave5 (executor 4) (130/200)
18/02/14 13:16:02 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 278, hadoop-slave5, executor 4, partition 152, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:02 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 256) in 13550 ms on hadoop-slave5 (executor 4) (131/200)
18/02/14 13:16:03 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 279, hadoop-slave3, executor 2, partition 143, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:03 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 244) in 17240 ms on hadoop-slave3 (executor 2) (132/200)
18/02/14 13:16:03 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 280, hadoop-slave3, executor 2, partition 147, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:03 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 242) in 17479 ms on hadoop-slave3 (executor 2) (133/200)
18/02/14 13:16:03 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 281, hadoop-slave1, executor 3, partition 157, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:03 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 257) in 14766 ms on hadoop-slave1 (executor 3) (134/200)
18/02/14 13:16:03 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 282, hadoop-slave1, executor 3, partition 160, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:03 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 258) in 14862 ms on hadoop-slave1 (executor 3) (135/200)
18/02/14 13:16:03 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 283, hadoop-slave3, executor 2, partition 158, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:03 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 251) in 16492 ms on hadoop-slave3 (executor 2) (136/200)
18/02/14 13:16:03 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 284, hadoop-slave1, executor 3, partition 163, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:03 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 247) in 17283 ms on hadoop-slave1 (executor 3) (137/200)
18/02/14 13:16:04 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 285, hadoop-slave1, executor 3, partition 166, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:04 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 249) in 17353 ms on hadoop-slave1 (executor 3) (138/200)
18/02/14 13:16:04 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 286, hadoop-slave3, executor 2, partition 159, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:04 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 253) in 17260 ms on hadoop-slave3 (executor 2) (139/200)
18/02/14 13:16:05 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 287, hadoop-slave2, executor 1, partition 161, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:05 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 268) in 6909 ms on hadoop-slave2 (executor 1) (140/200)
18/02/14 13:16:05 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 288, hadoop-slave3, executor 2, partition 162, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:05 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 243) in 19768 ms on hadoop-slave3 (executor 2) (141/200)
18/02/14 13:16:06 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 289, hadoop-slave2, executor 1, partition 165, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:06 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 266) in 9549 ms on hadoop-slave2 (executor 1) (142/200)
18/02/14 13:16:06 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 290, hadoop-slave3, executor 2, partition 164, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:06 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 248) in 19364 ms on hadoop-slave3 (executor 2) (143/200)
18/02/14 13:16:06 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 291, hadoop-slave2, executor 1, partition 167, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:06 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 271) in 6690 ms on hadoop-slave2 (executor 1) (144/200)
18/02/14 13:16:07 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 292, hadoop-slave5, executor 4, partition 168, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:07 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 267) in 9648 ms on hadoop-slave5 (executor 4) (145/200)
18/02/14 13:16:08 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 293, hadoop-slave5, executor 4, partition 169, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:08 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 270) in 9716 ms on hadoop-slave5 (executor 4) (146/200)
18/02/14 13:16:08 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 294, hadoop-slave2, executor 1, partition 170, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:08 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 272) in 8651 ms on hadoop-slave2 (executor 1) (147/200)
18/02/14 13:16:09 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 295, hadoop-slave2, executor 1, partition 171, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:09 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 274) in 8288 ms on hadoop-slave2 (executor 1) (148/200)
18/02/14 13:16:09 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 296, hadoop-slave1, executor 3, partition 174, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:09 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 263) in 19922 ms on hadoop-slave1 (executor 3) (149/200)
18/02/14 13:16:10 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 297, hadoop-slave3, executor 2, partition 172, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:10 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 283) in 6929 ms on hadoop-slave3 (executor 2) (150/200)
18/02/14 13:16:11 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 298, hadoop-slave2, executor 1, partition 173, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:11 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 287) in 5645 ms on hadoop-slave2 (executor 1) (151/200)
18/02/14 13:16:11 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 299, hadoop-slave5, executor 4, partition 176, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:11 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 275) in 9997 ms on hadoop-slave5 (executor 4) (152/200)
18/02/14 13:16:13 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 300, hadoop-slave1, executor 3, partition 175, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:13 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 276) in 11869 ms on hadoop-slave1 (executor 3) (153/200)
18/02/14 13:16:13 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 301, hadoop-slave2, executor 1, partition 177, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:13 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 289) in 7617 ms on hadoop-slave2 (executor 1) (154/200)
18/02/14 13:16:14 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 302, hadoop-slave2, executor 1, partition 179, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:14 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 273) in 13636 ms on hadoop-slave2 (executor 1) (155/200)
18/02/14 13:16:14 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 303, hadoop-slave2, executor 1, partition 180, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:14 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 291) in 7546 ms on hadoop-slave2 (executor 1) (156/200)
18/02/14 13:16:14 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 304, hadoop-slave5, executor 4, partition 178, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:14 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 269) in 16068 ms on hadoop-slave5 (executor 4) (157/200)
18/02/14 13:16:14 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 305, hadoop-slave2, executor 1, partition 182, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:14 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 294) in 5949 ms on hadoop-slave2 (executor 1) (158/200)
18/02/14 13:16:15 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 306, hadoop-slave3, executor 2, partition 181, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:15 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 279) in 12564 ms on hadoop-slave3 (executor 2) (159/200)
18/02/14 13:16:15 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 307, hadoop-slave1, executor 3, partition 183, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:15 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 281) in 12269 ms on hadoop-slave1 (executor 3) (160/200)
18/02/14 13:16:15 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 308, hadoop-slave1, executor 3, partition 185, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:15 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 282) in 12228 ms on hadoop-slave1 (executor 3) (161/200)
18/02/14 13:16:16 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 309, hadoop-slave3, executor 2, partition 184, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:16 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 286) in 11330 ms on hadoop-slave3 (executor 2) (162/200)
18/02/14 13:16:16 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 310, hadoop-slave2, executor 1, partition 187, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:16 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 295) in 7637 ms on hadoop-slave2 (executor 1) (163/200)
18/02/14 13:16:16 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 311, hadoop-slave3, executor 2, partition 186, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:16 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 280) in 13445 ms on hadoop-slave3 (executor 2) (164/200)
18/02/14 13:16:17 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 312, hadoop-slave2, executor 1, partition 189, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:17 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 298) in 6453 ms on hadoop-slave2 (executor 1) (165/200)
18/02/14 13:16:17 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 313, hadoop-slave5, executor 4, partition 188, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:17 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 292) in 10208 ms on hadoop-slave5 (executor 4) (166/200)
18/02/14 13:16:17 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 314, hadoop-slave5, executor 4, partition 190, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:17 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 278) in 15781 ms on hadoop-slave5 (executor 4) (167/200)
18/02/14 13:16:18 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 315, hadoop-slave5, executor 4, partition 192, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:18 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 293) in 9534 ms on hadoop-slave5 (executor 4) (168/200)
18/02/14 13:16:19 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 316, hadoop-slave3, executor 2, partition 193, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:19 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 290) in 12838 ms on hadoop-slave3 (executor 2) (169/200)
18/02/14 13:16:19 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 317, hadoop-slave3, executor 2, partition 194, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:19 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 297) in 8374 ms on hadoop-slave3 (executor 2) (170/200)
18/02/14 13:16:19 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 318, hadoop-slave3, executor 2, partition 195, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:19 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 288) in 13762 ms on hadoop-slave3 (executor 2) (171/200)
18/02/14 13:16:19 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 319, hadoop-slave2, executor 1, partition 191, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:19 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 301) in 5758 ms on hadoop-slave2 (executor 1) (172/200)
18/02/14 13:16:19 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 320, hadoop-slave1, executor 3, partition 197, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:19 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 284) in 15777 ms on hadoop-slave1 (executor 3) (173/200)
18/02/14 13:16:20 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 321, hadoop-slave1, executor 3, partition 198, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:20 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 285) in 15315 ms on hadoop-slave1 (executor 3) (174/200)
18/02/14 13:16:20 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 322, hadoop-slave2, executor 1, partition 196, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:20 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 302) in 5729 ms on hadoop-slave2 (executor 1) (175/200)
18/02/14 13:16:20 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 323, hadoop-slave5, executor 4, partition 199, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:20 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 277) in 18645 ms on hadoop-slave5 (executor 4) (176/200)
18/02/14 13:16:20 INFO TaskSetManager: Starting task 23.0 in stage 3.0 (TID 324, hadoop-slave2, executor 1, partition 23, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:20 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 305) in 5926 ms on hadoop-slave2 (executor 1) (177/200)
18/02/14 13:16:21 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 325, hadoop-slave1, executor 3, partition 26, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:21 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 296) in 11674 ms on hadoop-slave1 (executor 3) (178/200)
18/02/14 13:16:22 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 326, hadoop-slave3, executor 2, partition 25, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:22 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 311) in 5272 ms on hadoop-slave3 (executor 2) (179/200)
18/02/14 13:16:22 INFO TaskSetManager: Starting task 27.0 in stage 3.0 (TID 327, hadoop-slave2, executor 1, partition 27, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:22 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 310) in 5436 ms on hadoop-slave2 (executor 1) (180/200)
18/02/14 13:16:23 INFO TaskSetManager: Starting task 29.0 in stage 3.0 (TID 328, hadoop-slave3, executor 2, partition 29, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:23 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 309) in 7532 ms on hadoop-slave3 (executor 2) (181/200)
18/02/14 13:16:24 INFO TaskSetManager: Starting task 28.0 in stage 3.0 (TID 329, hadoop-slave2, executor 1, partition 28, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:24 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 319) in 4657 ms on hadoop-slave2 (executor 1) (182/200)
18/02/14 13:16:24 INFO TaskSetManager: Starting task 30.0 in stage 3.0 (TID 330, hadoop-slave2, executor 1, partition 30, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:24 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 322) in 4300 ms on hadoop-slave2 (executor 1) (183/200)
18/02/14 13:16:24 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 331, hadoop-slave2, executor 1, partition 31, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:24 INFO TaskSetManager: Finished task 23.0 in stage 3.0 (TID 324) in 3700 ms on hadoop-slave2 (executor 1) (25/200)
18/02/14 13:16:24 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 332, hadoop-slave3, executor 2, partition 32, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:24 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 316) in 5256 ms on hadoop-slave3 (executor 2) (184/200)
18/02/14 13:16:24 INFO TaskSetManager: Starting task 33.0 in stage 3.0 (TID 333, hadoop-slave1, executor 3, partition 33, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:24 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 300) in 11201 ms on hadoop-slave1 (executor 3) (185/200)
18/02/14 13:16:25 INFO TaskSetManager: Starting task 34.0 in stage 3.0 (TID 334, hadoop-slave5, executor 4, partition 34, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:25 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 299) in 13484 ms on hadoop-slave5 (executor 4) (186/200)
18/02/14 13:16:26 INFO TaskSetManager: Starting task 35.0 in stage 3.0 (TID 335, hadoop-slave5, executor 4, partition 35, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:26 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 313) in 9081 ms on hadoop-slave5 (executor 4) (187/200)
18/02/14 13:16:27 INFO TaskSetManager: Starting task 37.0 in stage 3.0 (TID 336, hadoop-slave3, executor 2, partition 37, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:27 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 317) in 7861 ms on hadoop-slave3 (executor 2) (188/200)
18/02/14 13:16:27 INFO TaskSetManager: Starting task 39.0 in stage 3.0 (TID 337, hadoop-slave3, executor 2, partition 39, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:27 INFO TaskSetManager: Finished task 25.0 in stage 3.0 (TID 326) in 5769 ms on hadoop-slave3 (executor 2) (26/200)
18/02/14 13:16:28 INFO TaskSetManager: Starting task 38.0 in stage 3.0 (TID 338, hadoop-slave5, executor 4, partition 38, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:28 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 315) in 9632 ms on hadoop-slave5 (executor 4) (189/200)
18/02/14 13:16:28 INFO TaskSetManager: Starting task 36.0 in stage 3.0 (TID 339, hadoop-slave2, executor 1, partition 36, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:28 INFO TaskSetManager: Finished task 30.0 in stage 3.0 (TID 330) in 3772 ms on hadoop-slave2 (executor 1) (27/200)
18/02/14 13:16:28 INFO TaskSetManager: Starting task 40.0 in stage 3.0 (TID 340, hadoop-slave3, executor 2, partition 40, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:28 INFO TaskSetManager: Finished task 29.0 in stage 3.0 (TID 328) in 4445 ms on hadoop-slave3 (executor 2) (28/200)
18/02/14 13:16:28 INFO TaskSetManager: Starting task 41.0 in stage 3.0 (TID 341, hadoop-slave2, executor 1, partition 41, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:28 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 303) in 13967 ms on hadoop-slave2 (executor 1) (190/200)
18/02/14 13:16:28 INFO TaskSetManager: Starting task 44.0 in stage 3.0 (TID 342, hadoop-slave2, executor 1, partition 44, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:28 INFO TaskSetManager: Finished task 27.0 in stage 3.0 (TID 327) in 6380 ms on hadoop-slave2 (executor 1) (29/200)
18/02/14 13:16:28 INFO TaskSetManager: Starting task 42.0 in stage 3.0 (TID 343, hadoop-slave3, executor 2, partition 42, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:28 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 318) in 9538 ms on hadoop-slave3 (executor 2) (191/200)
18/02/14 13:16:29 INFO TaskSetManager: Starting task 43.0 in stage 3.0 (TID 344, hadoop-slave3, executor 2, partition 43, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:29 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 306) in 13780 ms on hadoop-slave3 (executor 2) (192/200)
18/02/14 13:16:29 INFO TaskSetManager: Starting task 50.0 in stage 3.0 (TID 345, hadoop-slave2, executor 1, partition 50, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:29 INFO TaskSetManager: Finished task 31.0 in stage 3.0 (TID 331) in 5276 ms on hadoop-slave2 (executor 1) (30/200)
18/02/14 13:16:30 INFO TaskSetManager: Starting task 45.0 in stage 3.0 (TID 346, hadoop-slave1, executor 3, partition 45, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:30 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 325) in 9296 ms on hadoop-slave1 (executor 3) (31/200)
18/02/14 13:16:30 INFO TaskSetManager: Starting task 46.0 in stage 3.0 (TID 347, hadoop-slave3, executor 2, partition 46, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:30 INFO TaskSetManager: Finished task 32.0 in stage 3.0 (TID 332) in 5924 ms on hadoop-slave3 (executor 2) (32/200)
18/02/14 13:16:30 INFO TaskSetManager: Starting task 53.0 in stage 3.0 (TID 348, hadoop-slave2, executor 1, partition 53, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:30 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 312) in 13375 ms on hadoop-slave2 (executor 1) (193/200)
18/02/14 13:16:31 INFO TaskSetManager: Starting task 54.0 in stage 3.0 (TID 349, hadoop-slave2, executor 1, partition 54, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:31 INFO TaskSetManager: Finished task 28.0 in stage 3.0 (TID 329) in 7113 ms on hadoop-slave2 (executor 1) (33/200)
18/02/14 13:16:31 INFO TaskSetManager: Starting task 47.0 in stage 3.0 (TID 350, hadoop-slave3, executor 2, partition 47, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:31 INFO TaskSetManager: Finished task 37.0 in stage 3.0 (TID 336) in 4568 ms on hadoop-slave3 (executor 2) (34/200)
18/02/14 13:16:32 INFO TaskSetManager: Starting task 48.0 in stage 3.0 (TID 351, hadoop-slave5, executor 4, partition 48, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:32 INFO TaskSetManager: Finished task 35.0 in stage 3.0 (TID 335) in 5525 ms on hadoop-slave5 (executor 4) (35/200)
18/02/14 13:16:32 INFO TaskSetManager: Starting task 49.0 in stage 3.0 (TID 352, hadoop-slave1, executor 3, partition 49, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:32 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 320) in 12509 ms on hadoop-slave1 (executor 3) (194/200)
18/02/14 13:16:32 INFO TaskSetManager: Starting task 55.0 in stage 3.0 (TID 353, hadoop-slave2, executor 1, partition 55, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:32 INFO TaskSetManager: Finished task 41.0 in stage 3.0 (TID 341) in 4051 ms on hadoop-slave2 (executor 1) (36/200)
18/02/14 13:16:32 INFO TaskSetManager: Starting task 51.0 in stage 3.0 (TID 354, hadoop-slave5, executor 4, partition 51, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:32 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 314) in 14527 ms on hadoop-slave5 (executor 4) (195/200)
18/02/14 13:16:32 INFO TaskSetManager: Starting task 52.0 in stage 3.0 (TID 355, hadoop-slave1, executor 3, partition 52, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:32 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 321) in 12465 ms on hadoop-slave1 (executor 3) (196/200)
18/02/14 13:16:32 INFO TaskSetManager: Starting task 58.0 in stage 3.0 (TID 356, hadoop-slave5, executor 4, partition 58, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:32 INFO TaskSetManager: Finished task 34.0 in stage 3.0 (TID 334) in 7764 ms on hadoop-slave5 (executor 4) (37/200)
18/02/14 13:16:33 INFO TaskSetManager: Starting task 56.0 in stage 3.0 (TID 357, hadoop-slave2, executor 1, partition 56, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:33 INFO TaskSetManager: Finished task 36.0 in stage 3.0 (TID 339) in 4897 ms on hadoop-slave2 (executor 1) (38/200)
18/02/14 13:16:33 INFO TaskSetManager: Starting task 57.0 in stage 3.0 (TID 358, hadoop-slave3, executor 2, partition 57, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:33 INFO TaskSetManager: Finished task 39.0 in stage 3.0 (TID 337) in 5841 ms on hadoop-slave3 (executor 2) (39/200)
18/02/14 13:16:33 INFO TaskSetManager: Starting task 59.0 in stage 3.0 (TID 359, hadoop-slave5, executor 4, partition 59, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:33 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 304) in 18894 ms on hadoop-slave5 (executor 4) (197/200)
18/02/14 13:16:34 INFO TaskSetManager: Starting task 60.0 in stage 3.0 (TID 360, hadoop-slave1, executor 3, partition 60, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:34 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 307) in 18318 ms on hadoop-slave1 (executor 3) (198/200)
18/02/14 13:16:34 INFO TaskSetManager: Starting task 61.0 in stage 3.0 (TID 361, hadoop-slave2, executor 1, partition 61, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:34 INFO TaskSetManager: Finished task 50.0 in stage 3.0 (TID 345) in 4464 ms on hadoop-slave2 (executor 1) (40/200)
18/02/14 13:16:34 INFO TaskSetManager: Starting task 63.0 in stage 3.0 (TID 362, hadoop-slave1, executor 3, partition 63, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:34 INFO TaskSetManager: Finished task 33.0 in stage 3.0 (TID 333) in 9549 ms on hadoop-slave1 (executor 3) (41/200)
18/02/14 13:16:34 INFO TaskSetManager: Starting task 66.0 in stage 3.0 (TID 363, hadoop-slave1, executor 3, partition 66, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:34 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 308) in 18701 ms on hadoop-slave1 (executor 3) (199/200)
18/02/14 13:16:35 INFO TaskSetManager: Starting task 62.0 in stage 3.0 (TID 364, hadoop-slave2, executor 1, partition 62, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:35 INFO TaskSetManager: Finished task 44.0 in stage 3.0 (TID 342) in 6758 ms on hadoop-slave2 (executor 1) (42/200)
18/02/14 13:16:35 INFO TaskSetManager: Starting task 64.0 in stage 3.0 (TID 365, hadoop-slave3, executor 2, partition 64, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:35 INFO TaskSetManager: Finished task 40.0 in stage 3.0 (TID 340) in 7068 ms on hadoop-slave3 (executor 2) (43/200)
18/02/14 13:16:35 INFO TaskSetManager: Starting task 65.0 in stage 3.0 (TID 366, hadoop-slave2, executor 1, partition 65, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:35 INFO TaskSetManager: Finished task 53.0 in stage 3.0 (TID 348) in 4653 ms on hadoop-slave2 (executor 1) (44/200)
18/02/14 13:16:35 INFO TaskSetManager: Starting task 67.0 in stage 3.0 (TID 367, hadoop-slave3, executor 2, partition 67, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:35 INFO TaskSetManager: Finished task 46.0 in stage 3.0 (TID 347) in 5405 ms on hadoop-slave3 (executor 2) (45/200)
18/02/14 13:16:35 INFO TaskSetManager: Starting task 68.0 in stage 3.0 (TID 368, hadoop-slave3, executor 2, partition 68, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:35 INFO TaskSetManager: Finished task 43.0 in stage 3.0 (TID 344) in 6285 ms on hadoop-slave3 (executor 2) (46/200)
18/02/14 13:16:36 INFO TaskSetManager: Starting task 69.0 in stage 3.0 (TID 369, hadoop-slave3, executor 2, partition 69, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:36 INFO TaskSetManager: Finished task 42.0 in stage 3.0 (TID 343) in 7899 ms on hadoop-slave3 (executor 2) (47/200)
18/02/14 13:16:36 INFO TaskSetManager: Starting task 70.0 in stage 3.0 (TID 370, hadoop-slave3, executor 2, partition 70, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:36 INFO TaskSetManager: Finished task 47.0 in stage 3.0 (TID 350) in 5176 ms on hadoop-slave3 (executor 2) (48/200)
18/02/14 13:16:36 INFO TaskSetManager: Starting task 71.0 in stage 3.0 (TID 371, hadoop-slave2, executor 1, partition 71, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:36 INFO TaskSetManager: Finished task 54.0 in stage 3.0 (TID 349) in 5506 ms on hadoop-slave2 (executor 1) (49/200)
18/02/14 13:16:37 INFO TaskSetManager: Starting task 72.0 in stage 3.0 (TID 372, hadoop-slave2, executor 1, partition 72, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:37 INFO TaskSetManager: Finished task 55.0 in stage 3.0 (TID 353) in 4915 ms on hadoop-slave2 (executor 1) (50/200)
18/02/14 13:16:37 INFO TaskSetManager: Starting task 73.0 in stage 3.0 (TID 373, hadoop-slave5, executor 4, partition 73, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:37 INFO TaskSetManager: Finished task 38.0 in stage 3.0 (TID 338) in 9452 ms on hadoop-slave5 (executor 4) (51/200)
18/02/14 13:16:38 INFO TaskSetManager: Starting task 74.0 in stage 3.0 (TID 374, hadoop-slave5, executor 4, partition 74, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:38 INFO TaskSetManager: Finished task 48.0 in stage 3.0 (TID 351) in 6408 ms on hadoop-slave5 (executor 4) (52/200)
18/02/14 13:16:38 INFO TaskSetManager: Starting task 76.0 in stage 3.0 (TID 375, hadoop-slave2, executor 1, partition 76, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:38 INFO TaskSetManager: Finished task 56.0 in stage 3.0 (TID 357) in 5708 ms on hadoop-slave2 (executor 1) (53/200)
18/02/14 13:16:38 INFO TaskSetManager: Starting task 78.0 in stage 3.0 (TID 376, hadoop-slave5, executor 4, partition 78, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:38 INFO TaskSetManager: Finished task 51.0 in stage 3.0 (TID 354) in 6498 ms on hadoop-slave5 (executor 4) (54/200)
18/02/14 13:16:39 INFO TaskSetManager: Starting task 80.0 in stage 3.0 (TID 377, hadoop-slave5, executor 4, partition 80, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:39 INFO TaskSetManager: Finished task 58.0 in stage 3.0 (TID 356) in 6513 ms on hadoop-slave5 (executor 4) (55/200)
18/02/14 13:16:39 INFO TaskSetManager: Starting task 75.0 in stage 3.0 (TID 378, hadoop-slave1, executor 3, partition 75, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:39 INFO TaskSetManager: Finished task 45.0 in stage 3.0 (TID 346) in 9629 ms on hadoop-slave1 (executor 3) (56/200)
18/02/14 13:16:40 INFO TaskSetManager: Starting task 77.0 in stage 3.0 (TID 379, hadoop-slave1, executor 3, partition 77, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:40 INFO TaskSetManager: Finished task 52.0 in stage 3.0 (TID 355) in 7641 ms on hadoop-slave1 (executor 3) (57/200)
18/02/14 13:16:40 INFO TaskSetManager: Starting task 79.0 in stage 3.0 (TID 380, hadoop-slave2, executor 1, partition 79, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:40 INFO TaskSetManager: Finished task 62.0 in stage 3.0 (TID 364) in 5072 ms on hadoop-slave2 (executor 1) (58/200)
18/02/14 13:16:40 INFO TaskSetManager: Starting task 82.0 in stage 3.0 (TID 381, hadoop-slave2, executor 1, partition 82, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:40 INFO TaskSetManager: Finished task 65.0 in stage 3.0 (TID 366) in 4729 ms on hadoop-slave2 (executor 1) (59/200)
18/02/14 13:16:40 INFO TaskSetManager: Starting task 83.0 in stage 3.0 (TID 382, hadoop-slave2, executor 1, partition 83, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:40 INFO TaskSetManager: Finished task 61.0 in stage 3.0 (TID 361) in 6237 ms on hadoop-slave2 (executor 1) (60/200)
18/02/14 13:16:40 INFO TaskSetManager: Starting task 81.0 in stage 3.0 (TID 383, hadoop-slave3, executor 2, partition 81, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:40 INFO TaskSetManager: Finished task 64.0 in stage 3.0 (TID 365) in 5465 ms on hadoop-slave3 (executor 2) (61/200)
18/02/14 13:16:40 INFO TaskSetManager: Starting task 85.0 in stage 3.0 (TID 384, hadoop-slave1, executor 3, partition 85, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:40 INFO TaskSetManager: Finished task 49.0 in stage 3.0 (TID 352) in 8593 ms on hadoop-slave1 (executor 3) (62/200)
18/02/14 13:16:41 INFO TaskSetManager: Starting task 86.0 in stage 3.0 (TID 385, hadoop-slave1, executor 3, partition 86, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:41 INFO TaskSetManager: Finished task 60.0 in stage 3.0 (TID 360) in 6950 ms on hadoop-slave1 (executor 3) (63/200)
18/02/14 13:16:41 INFO TaskSetManager: Starting task 87.0 in stage 3.0 (TID 386, hadoop-slave1, executor 3, partition 87, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:41 INFO TaskSetManager: Finished task 66.0 in stage 3.0 (TID 363) in 6622 ms on hadoop-slave1 (executor 3) (64/200)
18/02/14 13:16:41 INFO TaskSetManager: Starting task 84.0 in stage 3.0 (TID 387, hadoop-slave3, executor 2, partition 84, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:41 INFO TaskSetManager: Finished task 68.0 in stage 3.0 (TID 368) in 5353 ms on hadoop-slave3 (executor 2) (65/200)
18/02/14 13:16:41 INFO TaskSetManager: Starting task 89.0 in stage 3.0 (TID 388, hadoop-slave1, executor 3, partition 89, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:41 INFO TaskSetManager: Finished task 63.0 in stage 3.0 (TID 362) in 6745 ms on hadoop-slave1 (executor 3) (66/200)
18/02/14 13:16:41 INFO TaskSetManager: Starting task 88.0 in stage 3.0 (TID 389, hadoop-slave3, executor 2, partition 88, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:41 INFO TaskSetManager: Finished task 57.0 in stage 3.0 (TID 358) in 7740 ms on hadoop-slave3 (executor 2) (67/200)
18/02/14 13:16:41 INFO TaskSetManager: Starting task 90.0 in stage 3.0 (TID 390, hadoop-slave5, executor 4, partition 90, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:41 INFO TaskSetManager: Finished task 59.0 in stage 3.0 (TID 359) in 8180 ms on hadoop-slave5 (executor 4) (68/200)
18/02/14 13:16:41 INFO TaskSetManager: Starting task 91.0 in stage 3.0 (TID 391, hadoop-slave2, executor 1, partition 91, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:41 INFO TaskSetManager: Finished task 71.0 in stage 3.0 (TID 371) in 4941 ms on hadoop-slave2 (executor 1) (69/200)
18/02/14 13:16:42 INFO TaskSetManager: Starting task 93.0 in stage 3.0 (TID 392, hadoop-slave2, executor 1, partition 93, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:42 INFO TaskSetManager: Finished task 72.0 in stage 3.0 (TID 372) in 4877 ms on hadoop-slave2 (executor 1) (70/200)
18/02/14 13:16:42 INFO TaskSetManager: Starting task 92.0 in stage 3.0 (TID 393, hadoop-slave3, executor 2, partition 92, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:42 INFO TaskSetManager: Finished task 67.0 in stage 3.0 (TID 367) in 6496 ms on hadoop-slave3 (executor 2) (71/200)
18/02/14 13:16:42 INFO TaskSetManager: Starting task 94.0 in stage 3.0 (TID 394, hadoop-slave5, executor 4, partition 94, NODE_LOCAL, 6941 bytes)
18/02/14 13:16:42 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 323) in 21940 ms on hadoop-slave5 (executor 4) (200/200)
18/02/14 13:16:42 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/02/14 13:16:42 INFO DAGScheduler: ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) finished in 113.652 s
18/02/14 13:16:42 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:16:42 INFO DAGScheduler: running: Set(ShuffleMapStage 3)
18/02/14 13:16:42 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:16:42 INFO DAGScheduler: failed: Set()
18/02/14 13:16:42 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:16:42 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 29.8 KB, free 4.1 GB)
18/02/14 13:16:42 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 12.0 KB, free 4.1 GB)
18/02/14 13:16:42 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.19.3.36:33535 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:16:42 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
18/02/14 13:16:42 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74)
18/02/14 13:16:42 INFO YarnScheduler: Adding task set 2.0 with 200 tasks
18/02/14 13:16:43 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 395, hadoop-slave3, executor 2, partition 5, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:43 INFO TaskSetManager: Finished task 69.0 in stage 3.0 (TID 369) in 7031 ms on hadoop-slave3 (executor 2) (72/200)
18/02/14 13:16:43 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop-slave3:45071 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:16:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.33:51692
18/02/14 13:16:43 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 628 bytes
18/02/14 13:16:44 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.33:51692
18/02/14 13:16:44 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 841 bytes
18/02/14 13:16:44 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 396, hadoop-slave2, executor 1, partition 9, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:44 INFO TaskSetManager: Finished task 79.0 in stage 3.0 (TID 380) in 4058 ms on hadoop-slave2 (executor 1) (73/200)
18/02/14 13:16:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop-slave2:39849 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:16:44 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.34:58746
18/02/14 13:16:44 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 397, hadoop-slave5, executor 4, partition 16, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:44 INFO TaskSetManager: Finished task 73.0 in stage 3.0 (TID 373) in 7256 ms on hadoop-slave5 (executor 4) (74/200)
18/02/14 13:16:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop-slave5:36807 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:16:44 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 398, hadoop-slave3, executor 2, partition 18, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:44 INFO TaskSetManager: Finished task 70.0 in stage 3.0 (TID 370) in 7907 ms on hadoop-slave3 (executor 2) (75/200)
18/02/14 13:16:44 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.31:55388
18/02/14 13:16:44 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 399, hadoop-slave2, executor 1, partition 34, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:44 INFO TaskSetManager: Finished task 76.0 in stage 3.0 (TID 375) in 6112 ms on hadoop-slave2 (executor 1) (76/200)
18/02/14 13:16:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.34:58746
18/02/14 13:16:45 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 400, hadoop-slave5, executor 4, partition 35, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:45 INFO TaskSetManager: Finished task 78.0 in stage 3.0 (TID 376) in 6469 ms on hadoop-slave5 (executor 4) (77/200)
18/02/14 13:16:45 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 401, hadoop-slave1, executor 3, partition 57, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:45 INFO TaskSetManager: Finished task 77.0 in stage 3.0 (TID 379) in 5437 ms on hadoop-slave1 (executor 3) (78/200)
18/02/14 13:16:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop-slave1:46007 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:16:45 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 402, hadoop-slave2, executor 1, partition 66, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:45 INFO TaskSetManager: Finished task 82.0 in stage 3.0 (TID 381) in 5249 ms on hadoop-slave2 (executor 1) (79/200)
18/02/14 13:16:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.35:59084
18/02/14 13:16:45 INFO TaskSetManager: Starting task 77.0 in stage 2.0 (TID 403, hadoop-slave1, executor 3, partition 77, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:45 INFO TaskSetManager: Finished task 75.0 in stage 3.0 (TID 378) in 5755 ms on hadoop-slave1 (executor 3) (80/200)
18/02/14 13:16:46 INFO TaskSetManager: Starting task 81.0 in stage 2.0 (TID 404, hadoop-slave3, executor 2, partition 81, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:46 INFO TaskSetManager: Finished task 81.0 in stage 3.0 (TID 383) in 5373 ms on hadoop-slave3 (executor 2) (81/200)
18/02/14 13:16:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.31:55388
18/02/14 13:16:46 INFO TaskSetManager: Starting task 82.0 in stage 2.0 (TID 405, hadoop-slave5, executor 4, partition 82, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:46 INFO TaskSetManager: Finished task 80.0 in stage 3.0 (TID 377) in 7028 ms on hadoop-slave5 (executor 4) (82/200)
18/02/14 13:16:46 INFO TaskSetManager: Starting task 86.0 in stage 2.0 (TID 406, hadoop-slave2, executor 1, partition 86, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:46 INFO TaskSetManager: Finished task 91.0 in stage 3.0 (TID 391) in 4716 ms on hadoop-slave2 (executor 1) (83/200)
18/02/14 13:16:46 INFO TaskSetManager: Starting task 89.0 in stage 2.0 (TID 407, hadoop-slave1, executor 3, partition 89, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:46 INFO TaskSetManager: Finished task 86.0 in stage 3.0 (TID 385) in 5521 ms on hadoop-slave1 (executor 3) (84/200)
18/02/14 13:16:46 INFO TaskSetManager: Starting task 92.0 in stage 2.0 (TID 408, hadoop-slave2, executor 1, partition 92, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:46 INFO TaskSetManager: Finished task 93.0 in stage 3.0 (TID 392) in 4452 ms on hadoop-slave2 (executor 1) (85/200)
18/02/14 13:16:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.35:59084
18/02/14 13:16:46 INFO TaskSetManager: Starting task 95.0 in stage 2.0 (TID 409, hadoop-slave3, executor 2, partition 95, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:46 INFO TaskSetManager: Finished task 88.0 in stage 3.0 (TID 389) in 5488 ms on hadoop-slave3 (executor 2) (86/200)
18/02/14 13:16:46 INFO TaskSetManager: Starting task 100.0 in stage 2.0 (TID 410, hadoop-slave5, executor 4, partition 100, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:46 INFO TaskSetManager: Finished task 74.0 in stage 3.0 (TID 374) in 8286 ms on hadoop-slave5 (executor 4) (87/200)
18/02/14 13:16:47 INFO TaskSetManager: Starting task 101.0 in stage 2.0 (TID 411, hadoop-slave3, executor 2, partition 101, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:47 INFO TaskSetManager: Finished task 92.0 in stage 3.0 (TID 393) in 5249 ms on hadoop-slave3 (executor 2) (88/200)
18/02/14 13:16:47 INFO TaskSetManager: Starting task 126.0 in stage 2.0 (TID 412, hadoop-slave2, executor 1, partition 126, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:47 INFO TaskSetManager: Finished task 83.0 in stage 3.0 (TID 382) in 7404 ms on hadoop-slave2 (executor 1) (89/200)
18/02/14 13:16:47 INFO TaskSetManager: Starting task 129.0 in stage 2.0 (TID 413, hadoop-slave1, executor 3, partition 129, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:47 INFO TaskSetManager: Finished task 85.0 in stage 3.0 (TID 384) in 7020 ms on hadoop-slave1 (executor 3) (90/200)
18/02/14 13:16:47 INFO TaskSetManager: Starting task 142.0 in stage 2.0 (TID 414, hadoop-slave5, executor 4, partition 142, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:47 INFO TaskSetManager: Finished task 90.0 in stage 3.0 (TID 390) in 6135 ms on hadoop-slave5 (executor 4) (91/200)
18/02/14 13:16:48 INFO TaskSetManager: Starting task 153.0 in stage 2.0 (TID 415, hadoop-slave1, executor 3, partition 153, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:48 INFO TaskSetManager: Finished task 87.0 in stage 3.0 (TID 386) in 6845 ms on hadoop-slave1 (executor 3) (92/200)
18/02/14 13:16:48 INFO TaskSetManager: Starting task 154.0 in stage 2.0 (TID 416, hadoop-slave1, executor 3, partition 154, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:48 INFO TaskSetManager: Finished task 89.0 in stage 3.0 (TID 388) in 6999 ms on hadoop-slave1 (executor 3) (93/200)
18/02/14 13:16:48 INFO TaskSetManager: Starting task 165.0 in stage 2.0 (TID 417, hadoop-slave5, executor 4, partition 165, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:48 INFO TaskSetManager: Finished task 94.0 in stage 3.0 (TID 394) in 6032 ms on hadoop-slave5 (executor 4) (94/200)
18/02/14 13:16:49 INFO TaskSetManager: Starting task 169.0 in stage 2.0 (TID 418, hadoop-slave3, executor 2, partition 169, NODE_LOCAL, 6201 bytes)
18/02/14 13:16:49 INFO TaskSetManager: Finished task 84.0 in stage 3.0 (TID 387) in 8281 ms on hadoop-slave3 (executor 2) (95/200)
18/02/14 13:21:35 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 145275 ms exceeds timeout 120000 ms
18/02/14 13:21:35 ERROR YarnScheduler: Lost executor 2 on hadoop-slave3: Executor heartbeat timed out after 145275 ms
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 67), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 57), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 11), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 40), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 70), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 42), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 22), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 39), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 29), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 69), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 84), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 37), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 4), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 16), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 92), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 81), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 32), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 68), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 47), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 88), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 13), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 43), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 25), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 46), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 64), so marking it as still running
18/02/14 13:21:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 2), so marking it as still running
18/02/14 13:21:36 WARN TaskSetManager: Lost task 169.0 in stage 2.0 (TID 418, hadoop-slave3, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 145275 ms
18/02/14 13:21:36 WARN TaskSetManager: Lost task 95.0 in stage 2.0 (TID 409, hadoop-slave3, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 145275 ms
18/02/14 13:21:36 WARN TaskSetManager: Lost task 101.0 in stage 2.0 (TID 411, hadoop-slave3, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 145275 ms
18/02/14 13:21:36 WARN TaskSetManager: Lost task 5.0 in stage 2.0 (TID 395, hadoop-slave3, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 145275 ms
18/02/14 13:21:36 WARN TaskSetManager: Lost task 81.0 in stage 2.0 (TID 404, hadoop-slave3, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 145275 ms
18/02/14 13:21:36 WARN TaskSetManager: Lost task 18.0 in stage 2.0 (TID 398, hadoop-slave3, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 145275 ms
18/02/14 13:21:36 INFO YarnClientSchedulerBackend: Requesting to kill executor(s) 2
18/02/14 13:21:36 INFO DAGScheduler: Executor lost: 2 (epoch 2)
18/02/14 13:21:36 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
18/02/14 13:21:36 INFO YarnClientSchedulerBackend: Actual list of executor(s) to be killed is 2
18/02/14 13:21:36 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, hadoop-slave3, 45071, None)
18/02/14 13:21:36 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
18/02/14 13:21:36 INFO DAGScheduler: Shuffle files lost for executor: 2 (epoch 2)
18/02/14 13:21:36 INFO ShuffleMapStage: ShuffleMapStage 1 is now unavailable on executor 2 (149/200, false)
18/02/14 13:21:36 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 2 (69/200, false)
18/02/14 13:21:36 INFO ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 2 (75/100, false)
18/02/14 13:21:36 INFO DAGScheduler: Host added was in lost list earlier: hadoop-slave3
18/02/14 13:21:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 2.
18/02/14 13:21:39 INFO DAGScheduler: Executor lost: 2 (epoch 8)
18/02/14 13:21:39 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
18/02/14 13:21:39 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
18/02/14 13:21:39 INFO DAGScheduler: Shuffle files lost for executor: 2 (epoch 8)
18/02/14 13:21:39 ERROR YarnScheduler: Lost executor 2 on hadoop-slave3: Container container_e41_1518606550421_0002_01_000004 exited from explicit termination request.
18/02/14 13:21:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.33:51972) with ID 5
18/02/14 13:21:47 INFO TaskSetManager: Starting task 18.1 in stage 2.0 (TID 419, hadoop-slave3, executor 5, partition 18, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:47 INFO TaskSetManager: Starting task 81.1 in stage 2.0 (TID 420, hadoop-slave3, executor 5, partition 81, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:47 INFO TaskSetManager: Starting task 5.1 in stage 2.0 (TID 421, hadoop-slave3, executor 5, partition 5, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:47 INFO TaskSetManager: Starting task 101.1 in stage 2.0 (TID 422, hadoop-slave3, executor 5, partition 101, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:47 INFO TaskSetManager: Starting task 95.1 in stage 2.0 (TID 423, hadoop-slave3, executor 5, partition 95, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:47 INFO TaskSetManager: Starting task 169.1 in stage 2.0 (TID 424, hadoop-slave3, executor 5, partition 169, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:47 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave3:33203 with 4.1 GB RAM, BlockManagerId(5, hadoop-slave3, 33203, None)
18/02/14 13:21:54 INFO TaskSetManager: Starting task 174.0 in stage 2.0 (TID 425, hadoop-slave1, executor 3, partition 174, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:54 INFO TaskSetManager: Starting task 184.0 in stage 2.0 (TID 426, hadoop-slave1, executor 3, partition 184, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:54 INFO TaskSetManager: Starting task 188.0 in stage 2.0 (TID 427, hadoop-slave1, executor 3, partition 188, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:54 INFO TaskSetManager: Starting task 190.0 in stage 2.0 (TID 428, hadoop-slave1, executor 3, partition 190, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:54 INFO TaskSetManager: Starting task 192.0 in stage 2.0 (TID 429, hadoop-slave1, executor 3, partition 192, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:54 INFO TaskSetManager: Starting task 194.0 in stage 2.0 (TID 430, hadoop-slave1, executor 3, partition 194, NODE_LOCAL, 6201 bytes)
18/02/14 13:21:54 WARN TaskSetManager: Lost task 77.0 in stage 2.0 (TID 403, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=1, mapId=147, reduceId=77, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:21:54 INFO TaskSetManager: Task 77.0 in stage 2.0 (TID 403) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:21:54 INFO DAGScheduler: Marking ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) as failed due to a fetch failure from ShuffleMapStage 1 (show at JaccardCoefficient.scala:74)
18/02/14 13:21:54 WARN TaskSetManager: Lost task 129.0 in stage 2.0 (TID 413, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=1, mapId=15, reduceId=129, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:21:54 INFO TaskSetManager: Task 129.0 in stage 2.0 (TID 413) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:21:54 WARN TaskSetManager: Lost task 153.0 in stage 2.0 (TID 415, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=1, mapId=41, reduceId=153, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:21:54 INFO TaskSetManager: Task 153.0 in stage 2.0 (TID 415) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:21:54 WARN TaskSetManager: Lost task 154.0 in stage 2.0 (TID 416, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=1, mapId=15, reduceId=154, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:21:54 INFO TaskSetManager: Task 154.0 in stage 2.0 (TID 416) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:21:54 WARN TaskSetManager: Lost task 57.0 in stage 2.0 (TID 401, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=1, mapId=195, reduceId=57, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:21:54 INFO TaskSetManager: Task 57.0 in stage 2.0 (TID 401) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:21:54 WARN TaskSetManager: Lost task 89.0 in stage 2.0 (TID 407, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=1, mapId=158, reduceId=89, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:21:54 INFO DAGScheduler: ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) failed in 312.370 s due to org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

18/02/14 13:21:54 INFO TaskSetManager: Task 89.0 in stage 2.0 (TID 407) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:21:54 INFO DAGScheduler: Resubmitting ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:21:55 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:21:55 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:21:55 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.7 KB, free 4.1 GB)
18/02/14 13:21:55 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.9 KB, free 4.1 GB)
18/02/14 13:21:55 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.19.3.36:33535 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:21:55 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:996
18/02/14 13:21:55 INFO DAGScheduler: Submitting 51 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74)
18/02/14 13:21:55 INFO YarnScheduler: Adding task set 1.1 with 51 tasks
18/02/14 13:21:55 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:21:55 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 10.3 KB, free 4.1 GB)
18/02/14 13:21:55 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.1 KB, free 4.1 GB)
18/02/14 13:21:55 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.19.3.36:33535 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:21:55 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:996
18/02/14 13:21:55 INFO DAGScheduler: Submitting 25 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74)
18/02/14 13:21:55 INFO YarnScheduler: Adding task set 0.1 with 25 tasks
18/02/14 13:22:09 INFO TaskSetManager: Starting task 1.0 in stage 0.1 (TID 431, hadoop-slave1, executor 3, partition 4, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:09 WARN TaskSetManager: Lost task 174.0 in stage 2.0 (TID 425, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=0, mapId=85, reduceId=174, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:22:09 INFO TaskSetManager: Task 174.0 in stage 2.0 (TID 425) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:22:09 INFO DAGScheduler: Resubmitting ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:22:09 INFO TaskSetManager: Starting task 2.0 in stage 0.1 (TID 432, hadoop-slave1, executor 3, partition 7, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:09 WARN TaskSetManager: Lost task 184.0 in stage 2.0 (TID 426, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=0, mapId=2, reduceId=184, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:22:09 INFO TaskSetManager: Task 184.0 in stage 2.0 (TID 426) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:22:09 INFO TaskSetManager: Starting task 5.0 in stage 0.1 (TID 433, hadoop-slave1, executor 3, partition 23, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:09 WARN TaskSetManager: Lost task 188.0 in stage 2.0 (TID 427, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=0, mapId=23, reduceId=188, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:22:09 INFO TaskSetManager: Task 188.0 in stage 2.0 (TID 427) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:22:09 INFO TaskSetManager: Starting task 7.0 in stage 0.1 (TID 434, hadoop-slave1, executor 3, partition 29, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:09 WARN TaskSetManager: Lost task 194.0 in stage 2.0 (TID 430, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=0, mapId=84, reduceId=194, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:22:09 INFO TaskSetManager: Task 194.0 in stage 2.0 (TID 430) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:22:09 INFO TaskSetManager: Starting task 9.0 in stage 0.1 (TID 435, hadoop-slave1, executor 3, partition 37, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:09 WARN TaskSetManager: Lost task 192.0 in stage 2.0 (TID 429, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(2, hadoop-slave3, 45071, None), shuffleId=0, mapId=2, reduceId=192, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:45071
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:45071
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:22:09 INFO TaskSetManager: Task 192.0 in stage 2.0 (TID 429) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:22:09 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on hadoop-slave1:46007 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:22:10 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:22:14 INFO TaskSetManager: Starting task 10.0 in stage 0.1 (TID 436, hadoop-slave1, executor 3, partition 39, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:14 INFO TaskSetManager: Finished task 2.0 in stage 0.1 (TID 432) in 4421 ms on hadoop-slave1 (executor 3) (1/25)
18/02/14 13:22:14 INFO TaskSetManager: Starting task 11.0 in stage 0.1 (TID 437, hadoop-slave1, executor 3, partition 46, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:14 INFO TaskSetManager: Finished task 5.0 in stage 0.1 (TID 433) in 4857 ms on hadoop-slave1 (executor 3) (2/25)
18/02/14 13:22:14 INFO TaskSetManager: Starting task 12.0 in stage 0.1 (TID 438, hadoop-slave1, executor 3, partition 47, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:14 INFO TaskSetManager: Finished task 7.0 in stage 0.1 (TID 434) in 5009 ms on hadoop-slave1 (executor 3) (3/25)
18/02/14 13:22:15 INFO TaskSetManager: Starting task 13.0 in stage 0.1 (TID 439, hadoop-slave1, executor 3, partition 51, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:15 INFO TaskSetManager: Finished task 9.0 in stage 0.1 (TID 435) in 5117 ms on hadoop-slave1 (executor 3) (4/25)
18/02/14 13:22:15 INFO TaskSetManager: Starting task 14.0 in stage 0.1 (TID 440, hadoop-slave1, executor 3, partition 55, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:15 INFO TaskSetManager: Finished task 1.0 in stage 0.1 (TID 431) in 5552 ms on hadoop-slave1 (executor 3) (5/25)
18/02/14 13:22:17 INFO TaskSetManager: Starting task 21.0 in stage 0.1 (TID 441, hadoop-slave1, executor 3, partition 85, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:17 INFO TaskSetManager: Finished task 10.0 in stage 0.1 (TID 436) in 3607 ms on hadoop-slave1 (executor 3) (6/25)
18/02/14 13:22:18 INFO TaskSetManager: Starting task 22.0 in stage 0.1 (TID 442, hadoop-slave1, executor 3, partition 86, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:18 INFO TaskSetManager: Finished task 11.0 in stage 0.1 (TID 437) in 3955 ms on hadoop-slave1 (executor 3) (7/25)
18/02/14 13:22:19 INFO TaskSetManager: Starting task 23.0 in stage 0.1 (TID 443, hadoop-slave1, executor 3, partition 93, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:19 INFO TaskSetManager: Finished task 12.0 in stage 0.1 (TID 438) in 4088 ms on hadoop-slave1 (executor 3) (8/25)
18/02/14 13:22:19 INFO TaskSetManager: Starting task 2.0 in stage 1.1 (TID 444, hadoop-slave1, executor 3, partition 4, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:19 INFO TaskSetManager: Finished task 13.0 in stage 0.1 (TID 439) in 4029 ms on hadoop-slave1 (executor 3) (9/25)
18/02/14 13:22:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on hadoop-slave1:46007 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:22:19 INFO TaskSetManager: Starting task 4.0 in stage 1.1 (TID 445, hadoop-slave1, executor 3, partition 15, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:19 INFO TaskSetManager: Finished task 14.0 in stage 0.1 (TID 440) in 4087 ms on hadoop-slave1 (executor 3) (10/25)
18/02/14 13:22:21 INFO TaskSetManager: Starting task 7.0 in stage 1.1 (TID 446, hadoop-slave1, executor 3, partition 29, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:21 INFO TaskSetManager: Finished task 21.0 in stage 0.1 (TID 441) in 3553 ms on hadoop-slave1 (executor 3) (11/25)
18/02/14 13:22:22 INFO TaskSetManager: Starting task 0.0 in stage 0.1 (TID 447, hadoop-slave1, executor 3, partition 2, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:22 INFO TaskSetManager: Finished task 22.0 in stage 0.1 (TID 442) in 4039 ms on hadoop-slave1 (executor 3) (12/25)
18/02/14 13:22:23 INFO TaskSetManager: Starting task 3.0 in stage 0.1 (TID 448, hadoop-slave1, executor 3, partition 13, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:23 INFO TaskSetManager: Finished task 23.0 in stage 0.1 (TID 443) in 4022 ms on hadoop-slave1 (executor 3) (13/25)
18/02/14 13:22:23 INFO TaskSetManager: Starting task 4.0 in stage 0.1 (TID 449, hadoop-slave1, executor 3, partition 16, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:23 INFO TaskSetManager: Finished task 2.0 in stage 1.1 (TID 444) in 4714 ms on hadoop-slave1 (executor 3) (1/51)
18/02/14 13:22:27 INFO TaskSetManager: Starting task 6.0 in stage 0.1 (TID 450, hadoop-slave1, executor 3, partition 25, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:27 INFO TaskSetManager: Finished task 0.0 in stage 0.1 (TID 447) in 4328 ms on hadoop-slave1 (executor 3) (14/25)
18/02/14 13:22:27 INFO TaskSetManager: Starting task 8.0 in stage 0.1 (TID 451, hadoop-slave1, executor 3, partition 31, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:27 INFO TaskSetManager: Finished task 7.0 in stage 1.1 (TID 446) in 5612 ms on hadoop-slave1 (executor 3) (2/51)
18/02/14 13:22:27 INFO TaskSetManager: Starting task 15.0 in stage 0.1 (TID 452, hadoop-slave1, executor 3, partition 58, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:27 INFO TaskSetManager: Finished task 3.0 in stage 0.1 (TID 448) in 4830 ms on hadoop-slave1 (executor 3) (15/25)
18/02/14 13:22:28 INFO TaskSetManager: Starting task 16.0 in stage 0.1 (TID 453, hadoop-slave1, executor 3, partition 59, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:28 INFO TaskSetManager: Finished task 4.0 in stage 1.1 (TID 445) in 8745 ms on hadoop-slave1 (executor 3) (3/51)
18/02/14 13:22:28 INFO TaskSetManager: Starting task 17.0 in stage 0.1 (TID 454, hadoop-slave1, executor 3, partition 70, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:28 INFO TaskSetManager: Finished task 4.0 in stage 0.1 (TID 449) in 4528 ms on hadoop-slave1 (executor 3) (16/25)
18/02/14 13:22:30 INFO TaskSetManager: Starting task 18.0 in stage 0.1 (TID 455, hadoop-slave1, executor 3, partition 71, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:30 INFO TaskSetManager: Finished task 8.0 in stage 0.1 (TID 451) in 3697 ms on hadoop-slave1 (executor 3) (17/25)
18/02/14 13:22:31 INFO TaskSetManager: Starting task 19.0 in stage 0.1 (TID 456, hadoop-slave1, executor 3, partition 81, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:31 INFO TaskSetManager: Finished task 6.0 in stage 0.1 (TID 450) in 4157 ms on hadoop-slave1 (executor 3) (18/25)
18/02/14 13:22:31 INFO TaskSetManager: Starting task 20.0 in stage 0.1 (TID 457, hadoop-slave1, executor 3, partition 84, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:31 INFO TaskSetManager: Finished task 16.0 in stage 0.1 (TID 453) in 3476 ms on hadoop-slave1 (executor 3) (19/25)
18/02/14 13:22:32 INFO TaskSetManager: Starting task 24.0 in stage 0.1 (TID 458, hadoop-slave1, executor 3, partition 99, RACK_LOCAL, 6832 bytes)
18/02/14 13:22:32 INFO TaskSetManager: Finished task 17.0 in stage 0.1 (TID 454) in 3822 ms on hadoop-slave1 (executor 3) (20/25)
18/02/14 13:22:32 INFO TaskSetManager: Starting task 9.0 in stage 1.1 (TID 459, hadoop-slave1, executor 3, partition 32, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:32 INFO TaskSetManager: Finished task 15.0 in stage 0.1 (TID 452) in 4350 ms on hadoop-slave1 (executor 3) (21/25)
18/02/14 13:22:34 INFO TaskSetManager: Starting task 10.0 in stage 1.1 (TID 460, hadoop-slave1, executor 3, partition 37, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:34 INFO TaskSetManager: Finished task 18.0 in stage 0.1 (TID 455) in 3462 ms on hadoop-slave1 (executor 3) (22/25)
18/02/14 13:22:35 INFO TaskSetManager: Starting task 12.0 in stage 1.1 (TID 461, hadoop-slave1, executor 3, partition 46, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:35 INFO TaskSetManager: Finished task 19.0 in stage 0.1 (TID 456) in 4705 ms on hadoop-slave1 (executor 3) (23/25)
18/02/14 13:22:35 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 163789 ms exceeds timeout 120000 ms
18/02/14 13:22:35 ERROR YarnScheduler: Lost executor 1 on hadoop-slave2: Executor heartbeat timed out after 163789 ms
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 91), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 83), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 62), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 54), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 17), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 31), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 61), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 5), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 82), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 12), so marking it as still running
18/02/14 13:22:35 WARN TaskSetManager: Lost task 86.0 in stage 2.0 (TID 406, hadoop-slave2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163789 ms
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 50), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 72), so marking it as still running
18/02/14 13:22:35 WARN TaskSetManager: Lost task 126.0 in stage 2.0 (TID 412, hadoop-slave2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163789 ms
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 27), so marking it as still running
18/02/14 13:22:35 WARN TaskSetManager: Lost task 9.0 in stage 2.0 (TID 396, hadoop-slave2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163789 ms
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 56), so marking it as still running
18/02/14 13:22:35 WARN TaskSetManager: Lost task 34.0 in stage 2.0 (TID 399, hadoop-slave2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163789 ms
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 53), so marking it as still running
18/02/14 13:22:35 WARN TaskSetManager: Lost task 92.0 in stage 2.0 (TID 408, hadoop-slave2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163789 ms
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 76), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 65), so marking it as still running
18/02/14 13:22:35 WARN TaskSetManager: Lost task 66.0 in stage 2.0 (TID 402, hadoop-slave2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 163789 ms
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 30), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 36), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 0), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 44), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 23), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 93), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 41), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 8), so marking it as still running
18/02/14 13:22:35 INFO YarnClientSchedulerBackend: Requesting to kill executor(s) 1
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 20), so marking it as still running
18/02/14 13:22:35 INFO YarnClientSchedulerBackend: Actual list of executor(s) to be killed is 1
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 55), so marking it as still running
18/02/14 13:22:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 79), so marking it as still running
18/02/14 13:22:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 71), so marking it as still running
18/02/14 13:22:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 28), so marking it as still running
18/02/14 13:22:36 INFO DAGScheduler: Executor lost: 1 (epoch 25)
18/02/14 13:22:36 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/02/14 13:22:36 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, hadoop-slave2, 39849, None)
18/02/14 13:22:36 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
18/02/14 13:22:36 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 25)
18/02/14 13:22:36 INFO ShuffleMapStage: ShuffleMapStage 1 is now unavailable on executor 1 (93/200, false)
18/02/14 13:22:36 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 1 (39/200, false)
18/02/14 13:22:36 INFO ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 1 (73/100, false)
18/02/14 13:22:36 INFO DAGScheduler: Host added was in lost list earlier: hadoop-slave2
18/02/14 13:22:36 INFO TaskSetManager: Starting task 14.0 in stage 1.1 (TID 462, hadoop-slave1, executor 3, partition 55, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:36 INFO TaskSetManager: Finished task 20.0 in stage 0.1 (TID 457) in 4928 ms on hadoop-slave1 (executor 3) (24/25)
18/02/14 13:22:37 INFO TaskSetManager: Starting task 15.0 in stage 1.1 (TID 463, hadoop-slave1, executor 3, partition 56, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:37 INFO TaskSetManager: Finished task 24.0 in stage 0.1 (TID 458) in 5607 ms on hadoop-slave1 (executor 3) (25/25)
18/02/14 13:22:37 INFO YarnScheduler: Removed TaskSet 0.1, whose tasks have all completed, from pool 
18/02/14 13:22:37 INFO DAGScheduler: ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) finished in 42.612 s
18/02/14 13:22:37 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:22:37 INFO DAGScheduler: running: Set(ShuffleMapStage 1, ShuffleMapStage 3)
18/02/14 13:22:37 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:22:37 INFO DAGScheduler: failed: Set()
18/02/14 13:22:37 INFO DAGScheduler: Resubmitting ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) because some of its tasks had failed: 0, 3, 5, 10, 17, 22, 24, 30, 34, 40, 41, 50, 53, 54, 61, 62, 63, 66, 73, 79, 80, 82, 89, 91, 98
18/02/14 13:22:37 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:22:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 10.3 KB, free 4.1 GB)
18/02/14 13:22:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.1 KB, free 4.1 GB)
18/02/14 13:22:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.19.3.36:33535 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:22:37 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:996
18/02/14 13:22:37 INFO DAGScheduler: Submitting 25 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74)
18/02/14 13:22:37 INFO YarnScheduler: Adding task set 0.2 with 25 tasks
18/02/14 13:22:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 1.
18/02/14 13:22:38 INFO DAGScheduler: Executor lost: 1 (epoch 32)
18/02/14 13:22:38 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
18/02/14 13:22:38 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
18/02/14 13:22:38 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 32)
18/02/14 13:22:38 ERROR YarnScheduler: Lost executor 1 on hadoop-slave2: Container container_e41_1518606550421_0002_01_000003 exited from explicit termination request.
18/02/14 13:22:42 INFO TaskSetManager: Starting task 0.0 in stage 0.2 (TID 464, hadoop-slave1, executor 3, partition 0, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:42 INFO TaskSetManager: Finished task 12.0 in stage 1.1 (TID 461) in 6242 ms on hadoop-slave1 (executor 3) (4/51)
18/02/14 13:22:42 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on hadoop-slave1:46007 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:22:44 INFO TaskSetManager: Starting task 2.0 in stage 0.2 (TID 465, hadoop-slave1, executor 3, partition 5, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:44 INFO TaskSetManager: Finished task 10.0 in stage 1.1 (TID 460) in 10558 ms on hadoop-slave1 (executor 3) (5/51)
18/02/14 13:22:46 INFO TaskSetManager: Starting task 3.0 in stage 0.2 (TID 466, hadoop-slave1, executor 3, partition 10, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO TaskSetManager: Finished task 14.0 in stage 1.1 (TID 462) in 9437 ms on hadoop-slave1 (executor 3) (6/51)
18/02/14 13:22:46 INFO TaskSetManager: Starting task 7.0 in stage 0.2 (TID 467, hadoop-slave1, executor 3, partition 30, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO TaskSetManager: Finished task 9.0 in stage 1.1 (TID 459) in 13944 ms on hadoop-slave1 (executor 3) (7/51)
18/02/14 13:22:46 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.34:59054) with ID 6
18/02/14 13:22:46 INFO TaskSetManager: Starting task 1.0 in stage 0.2 (TID 468, hadoop-slave2, executor 6, partition 3, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO TaskSetManager: Starting task 4.0 in stage 0.2 (TID 469, hadoop-slave2, executor 6, partition 17, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO TaskSetManager: Starting task 5.0 in stage 0.2 (TID 470, hadoop-slave2, executor 6, partition 22, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO TaskSetManager: Starting task 6.0 in stage 0.2 (TID 471, hadoop-slave2, executor 6, partition 24, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO TaskSetManager: Starting task 8.0 in stage 0.2 (TID 472, hadoop-slave2, executor 6, partition 34, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO TaskSetManager: Starting task 9.0 in stage 0.2 (TID 473, hadoop-slave2, executor 6, partition 40, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:46 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave2:33289 with 4.1 GB RAM, BlockManagerId(6, hadoop-slave2, 33289, None)
18/02/14 13:22:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on hadoop-slave2:33289 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:22:49 INFO TaskSetManager: Starting task 12.0 in stage 0.2 (TID 474, hadoop-slave1, executor 3, partition 53, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:49 INFO TaskSetManager: Finished task 2.0 in stage 0.2 (TID 465) in 4404 ms on hadoop-slave1 (executor 3) (1/25)
18/02/14 13:22:49 INFO TaskSetManager: Starting task 16.0 in stage 0.2 (TID 475, hadoop-slave1, executor 3, partition 63, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:49 INFO TaskSetManager: Finished task 15.0 in stage 1.1 (TID 463) in 11700 ms on hadoop-slave1 (executor 3) (8/51)
18/02/14 13:22:50 INFO TaskSetManager: Starting task 17.0 in stage 0.2 (TID 476, hadoop-slave1, executor 3, partition 66, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:50 INFO TaskSetManager: Finished task 0.0 in stage 0.2 (TID 464) in 7839 ms on hadoop-slave1 (executor 3) (2/25)
18/02/14 13:22:50 INFO TaskSetManager: Starting task 19.0 in stage 0.2 (TID 477, hadoop-slave1, executor 3, partition 79, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:50 INFO TaskSetManager: Finished task 3.0 in stage 0.2 (TID 466) in 4727 ms on hadoop-slave1 (executor 3) (3/25)
18/02/14 13:22:53 INFO TaskSetManager: Starting task 21.0 in stage 0.2 (TID 478, hadoop-slave1, executor 3, partition 82, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:53 WARN TaskSetManager: Lost task 190.0 in stage 2.0 (TID 428, hadoop-slave1, executor 3): FetchFailed(BlockManagerId(1, hadoop-slave2, 39849, None), shuffleId=0, mapId=80, reduceId=190, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave2/172.19.3.34:39849
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave2/172.19.3.34:39849
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave2/172.19.3.34:39849
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:22:53 INFO TaskSetManager: Task 190.0 in stage 2.0 (TID 428) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:22:53 INFO DAGScheduler: Resubmitting ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:22:53 INFO TaskSetManager: Starting task 22.0 in stage 0.2 (TID 479, hadoop-slave1, executor 3, partition 89, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:53 INFO TaskSetManager: Finished task 7.0 in stage 0.2 (TID 467) in 7481 ms on hadoop-slave1 (executor 3) (4/25)
18/02/14 13:22:53 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:22:55 INFO TaskSetManager: Starting task 23.0 in stage 0.2 (TID 480, hadoop-slave1, executor 3, partition 91, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:55 INFO TaskSetManager: Finished task 16.0 in stage 0.2 (TID 475) in 5958 ms on hadoop-slave1 (executor 3) (5/25)
18/02/14 13:22:55 INFO TaskSetManager: Starting task 24.0 in stage 0.2 (TID 481, hadoop-slave1, executor 3, partition 98, NODE_LOCAL, 6832 bytes)
18/02/14 13:22:55 INFO TaskSetManager: Finished task 12.0 in stage 0.2 (TID 474) in 6649 ms on hadoop-slave1 (executor 3) (6/25)
18/02/14 13:22:55 INFO TaskSetManager: Starting task 19.0 in stage 1.1 (TID 482, hadoop-slave1, executor 3, partition 75, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:55 INFO TaskSetManager: Finished task 19.0 in stage 0.2 (TID 477) in 5095 ms on hadoop-slave1 (executor 3) (7/25)
18/02/14 13:22:57 INFO TaskSetManager: Starting task 22.0 in stage 1.1 (TID 483, hadoop-slave1, executor 3, partition 85, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:57 INFO TaskSetManager: Finished task 17.0 in stage 0.2 (TID 476) in 7063 ms on hadoop-slave1 (executor 3) (8/25)
18/02/14 13:22:58 INFO TaskSetManager: Starting task 28.0 in stage 1.1 (TID 484, hadoop-slave1, executor 3, partition 104, NODE_LOCAL, 6941 bytes)
18/02/14 13:22:58 INFO TaskSetManager: Finished task 21.0 in stage 0.2 (TID 478) in 4674 ms on hadoop-slave1 (executor 3) (9/25)
18/02/14 13:23:00 INFO TaskSetManager: Starting task 10.0 in stage 0.2 (TID 485, hadoop-slave1, executor 3, partition 41, RACK_LOCAL, 6832 bytes)
18/02/14 13:23:00 INFO TaskSetManager: Finished task 22.0 in stage 0.2 (TID 479) in 6390 ms on hadoop-slave1 (executor 3) (10/25)
18/02/14 13:23:01 INFO TaskSetManager: Starting task 11.0 in stage 0.2 (TID 486, hadoop-slave1, executor 3, partition 50, RACK_LOCAL, 6832 bytes)
18/02/14 13:23:01 INFO TaskSetManager: Finished task 23.0 in stage 0.2 (TID 480) in 6116 ms on hadoop-slave1 (executor 3) (11/25)
18/02/14 13:23:03 INFO TaskSetManager: Starting task 13.0 in stage 0.2 (TID 487, hadoop-slave1, executor 3, partition 54, RACK_LOCAL, 6832 bytes)
18/02/14 13:23:03 INFO TaskSetManager: Finished task 24.0 in stage 0.2 (TID 481) in 7259 ms on hadoop-slave1 (executor 3) (12/25)
18/02/14 13:23:04 INFO TaskSetManager: Starting task 14.0 in stage 0.2 (TID 488, hadoop-slave1, executor 3, partition 61, RACK_LOCAL, 6832 bytes)
18/02/14 13:23:04 INFO TaskSetManager: Finished task 10.0 in stage 0.2 (TID 485) in 4546 ms on hadoop-slave1 (executor 3) (13/25)
18/02/14 13:23:05 INFO TaskSetManager: Starting task 15.0 in stage 0.2 (TID 489, hadoop-slave1, executor 3, partition 62, RACK_LOCAL, 6832 bytes)
18/02/14 13:23:05 INFO TaskSetManager: Finished task 28.0 in stage 1.1 (TID 484) in 7504 ms on hadoop-slave1 (executor 3) (9/51)
18/02/14 13:23:07 INFO TaskSetManager: Starting task 18.0 in stage 0.2 (TID 490, hadoop-slave1, executor 3, partition 73, RACK_LOCAL, 6832 bytes)
18/02/14 13:23:07 INFO TaskSetManager: Finished task 11.0 in stage 0.2 (TID 486) in 6456 ms on hadoop-slave1 (executor 3) (14/25)
18/02/14 13:23:08 INFO TaskSetManager: Starting task 20.0 in stage 0.2 (TID 491, hadoop-slave1, executor 3, partition 80, RACK_LOCAL, 6832 bytes)
18/02/14 13:23:08 INFO TaskSetManager: Finished task 22.0 in stage 1.1 (TID 483) in 11391 ms on hadoop-slave1 (executor 3) (10/51)
18/02/14 13:23:08 INFO TaskSetManager: Starting task 29.0 in stage 1.1 (TID 492, hadoop-slave1, executor 3, partition 107, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:08 INFO TaskSetManager: Finished task 13.0 in stage 0.2 (TID 487) in 5540 ms on hadoop-slave1 (executor 3) (15/25)
18/02/14 13:23:10 INFO TaskSetManager: Starting task 30.0 in stage 1.1 (TID 493, hadoop-slave1, executor 3, partition 111, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:10 INFO TaskSetManager: Finished task 14.0 in stage 0.2 (TID 488) in 5734 ms on hadoop-slave1 (executor 3) (16/25)
18/02/14 13:23:10 INFO TaskSetManager: Starting task 31.0 in stage 1.1 (TID 494, hadoop-slave1, executor 3, partition 115, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:10 INFO TaskSetManager: Finished task 15.0 in stage 0.2 (TID 489) in 4776 ms on hadoop-slave1 (executor 3) (17/25)
18/02/14 13:23:13 INFO TaskSetManager: Starting task 32.0 in stage 1.1 (TID 495, hadoop-slave1, executor 3, partition 118, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:13 INFO TaskSetManager: Finished task 18.0 in stage 0.2 (TID 490) in 5318 ms on hadoop-slave1 (executor 3) (18/25)
18/02/14 13:23:14 INFO TaskSetManager: Starting task 36.0 in stage 1.1 (TID 496, hadoop-slave1, executor 3, partition 129, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:14 INFO TaskSetManager: Finished task 19.0 in stage 1.1 (TID 482) in 18357 ms on hadoop-slave1 (executor 3) (11/51)
18/02/14 13:23:21 INFO TaskSetManager: Starting task 38.0 in stage 1.1 (TID 497, hadoop-slave1, executor 3, partition 143, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:21 INFO TaskSetManager: Finished task 20.0 in stage 0.2 (TID 491) in 12645 ms on hadoop-slave1 (executor 3) (19/25)
18/02/14 13:23:27 INFO TaskSetManager: Starting task 39.0 in stage 1.1 (TID 498, hadoop-slave1, executor 3, partition 147, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:27 INFO TaskSetManager: Finished task 31.0 in stage 1.1 (TID 494) in 17110 ms on hadoop-slave1 (executor 3) (12/51)
18/02/14 13:23:31 INFO TaskSetManager: Starting task 47.0 in stage 1.1 (TID 499, hadoop-slave1, executor 3, partition 186, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:31 INFO TaskSetManager: Finished task 30.0 in stage 1.1 (TID 493) in 20858 ms on hadoop-slave1 (executor 3) (13/51)
18/02/14 13:23:32 INFO TaskSetManager: Starting task 48.0 in stage 1.1 (TID 500, hadoop-slave1, executor 3, partition 193, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:32 INFO TaskSetManager: Finished task 29.0 in stage 1.1 (TID 492) in 24019 ms on hadoop-slave1 (executor 3) (14/51)
18/02/14 13:23:35 WARN HeartbeatReceiver: Removing executor 4 with no recent heartbeats: 151416 ms exceeds timeout 120000 ms
18/02/14 13:23:35 ERROR YarnScheduler: Lost executor 4 on hadoop-slave5: Executor heartbeat timed out after 151416 ms
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 24), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 73), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 94), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 78), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 10), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 34), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 90), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 51), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 6), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 48), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 19), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 80), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 59), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 3), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 35), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 58), so marking it as still running
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 15), so marking it as still running
18/02/14 13:23:35 WARN TaskSetManager: Lost task 35.0 in stage 2.0 (TID 400, hadoop-slave5, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 151416 ms
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 74), so marking it as still running
18/02/14 13:23:35 WARN TaskSetManager: Lost task 16.0 in stage 2.0 (TID 397, hadoop-slave5, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 151416 ms
18/02/14 13:23:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 38), so marking it as still running
18/02/14 13:23:35 WARN TaskSetManager: Lost task 142.0 in stage 2.0 (TID 414, hadoop-slave5, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 151416 ms
18/02/14 13:23:35 WARN TaskSetManager: Lost task 82.0 in stage 2.0 (TID 405, hadoop-slave5, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 151416 ms
18/02/14 13:23:35 WARN TaskSetManager: Lost task 165.0 in stage 2.0 (TID 417, hadoop-slave5, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 151416 ms
18/02/14 13:23:35 WARN TaskSetManager: Lost task 100.0 in stage 2.0 (TID 410, hadoop-slave5, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 151416 ms
18/02/14 13:23:35 INFO DAGScheduler: Executor lost: 4 (epoch 39)
18/02/14 13:23:35 INFO YarnClientSchedulerBackend: Requesting to kill executor(s) 4
18/02/14 13:23:35 INFO BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
18/02/14 13:23:35 INFO YarnClientSchedulerBackend: Actual list of executor(s) to be killed is 4
18/02/14 13:23:35 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(4, hadoop-slave5, 36807, None)
18/02/14 13:23:35 INFO BlockManagerMaster: Removed 4 successfully in removeExecutor
18/02/14 13:23:35 INFO DAGScheduler: Shuffle files lost for executor: 4 (epoch 39)
18/02/14 13:23:36 INFO ShuffleMapStage: ShuffleMapStage 1 is now unavailable on executor 4 (60/200, false)
18/02/14 13:23:36 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 4 (20/200, false)
18/02/14 13:23:36 INFO ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 4 (68/100, false)
18/02/14 13:23:36 INFO DAGScheduler: Host added was in lost list earlier: hadoop-slave5
18/02/14 13:23:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 4.
18/02/14 13:23:39 INFO DAGScheduler: Executor lost: 4 (epoch 45)
18/02/14 13:23:39 INFO BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
18/02/14 13:23:39 INFO BlockManagerMaster: Removed 4 successfully in removeExecutor
18/02/14 13:23:39 INFO DAGScheduler: Shuffle files lost for executor: 4 (epoch 45)
18/02/14 13:23:39 ERROR YarnScheduler: Lost executor 4 on hadoop-slave5: Container container_e41_1518606550421_0002_01_000006 exited from explicit termination request.
18/02/14 13:23:40 INFO TaskSetManager: Starting task 49.0 in stage 1.1 (TID 501, hadoop-slave1, executor 3, partition 194, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:40 INFO TaskSetManager: Finished task 38.0 in stage 1.1 (TID 497) in 19585 ms on hadoop-slave1 (executor 3) (15/51)
18/02/14 13:23:40 INFO TaskSetManager: Starting task 50.0 in stage 1.1 (TID 502, hadoop-slave1, executor 3, partition 195, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:40 INFO TaskSetManager: Finished task 32.0 in stage 1.1 (TID 495) in 27561 ms on hadoop-slave1 (executor 3) (16/51)
18/02/14 13:23:41 INFO TaskSetManager: Starting task 74.1 in stage 3.0 (TID 503, hadoop-slave1, executor 3, partition 74, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:41 INFO TaskSetManager: Finished task 36.0 in stage 1.1 (TID 496) in 27533 ms on hadoop-slave1 (executor 3) (17/51)
18/02/14 13:23:45 INFO TaskSetManager: Starting task 0.0 in stage 1.1 (TID 504, hadoop-slave1, executor 3, partition 2, RACK_LOCAL, 6941 bytes)
18/02/14 13:23:45 INFO TaskSetManager: Finished task 39.0 in stage 1.1 (TID 498) in 17989 ms on hadoop-slave1 (executor 3) (18/51)
18/02/14 13:23:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.31:55688) with ID 7
18/02/14 13:23:47 INFO TaskSetManager: Starting task 1.0 in stage 1.1 (TID 505, hadoop-slave5, executor 7, partition 3, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:47 INFO TaskSetManager: Starting task 5.0 in stage 1.1 (TID 506, hadoop-slave5, executor 7, partition 22, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:47 INFO TaskSetManager: Starting task 6.0 in stage 1.1 (TID 507, hadoop-slave5, executor 7, partition 25, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:47 INFO TaskSetManager: Starting task 8.0 in stage 1.1 (TID 508, hadoop-slave5, executor 7, partition 31, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:47 INFO TaskSetManager: Starting task 11.0 in stage 1.1 (TID 509, hadoop-slave5, executor 7, partition 41, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:47 INFO TaskSetManager: Starting task 13.0 in stage 1.1 (TID 510, hadoop-slave5, executor 7, partition 48, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:47 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave5:35799 with 4.1 GB RAM, BlockManagerId(7, hadoop-slave5, 35799, None)
18/02/14 13:23:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on hadoop-slave5:35799 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:23:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave5:35799 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:23:50 INFO TaskSetManager: Starting task 15.1 in stage 3.0 (TID 511, hadoop-slave1, executor 3, partition 15, NODE_LOCAL, 6941 bytes)
18/02/14 13:23:50 INFO TaskSetManager: Finished task 48.0 in stage 1.1 (TID 500) in 17331 ms on hadoop-slave1 (executor 3) (19/51)
18/02/14 13:23:50 INFO TaskSetManager: Starting task 3.0 in stage 1.1 (TID 512, hadoop-slave1, executor 3, partition 13, RACK_LOCAL, 6941 bytes)
18/02/14 13:23:50 INFO TaskSetManager: Finished task 47.0 in stage 1.1 (TID 499) in 19628 ms on hadoop-slave1 (executor 3) (20/51)
18/02/14 13:23:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave2:33289 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:23:57 INFO TaskSetManager: Starting task 16.0 in stage 1.1 (TID 513, hadoop-slave1, executor 3, partition 59, RACK_LOCAL, 6941 bytes)
18/02/14 13:23:57 INFO TaskSetManager: Finished task 74.1 in stage 3.0 (TID 503) in 15328 ms on hadoop-slave1 (executor 3) (21/200)
18/02/14 13:23:58 INFO TaskSetManager: Starting task 17.0 in stage 1.1 (TID 514, hadoop-slave1, executor 3, partition 62, RACK_LOCAL, 6941 bytes)
18/02/14 13:23:58 INFO TaskSetManager: Finished task 15.1 in stage 3.0 (TID 511) in 8593 ms on hadoop-slave1 (executor 3) (22/200)
18/02/14 13:23:59 INFO TaskSetManager: Starting task 18.0 in stage 1.1 (TID 515, hadoop-slave1, executor 3, partition 68, RACK_LOCAL, 6941 bytes)
18/02/14 13:23:59 INFO TaskSetManager: Finished task 49.0 in stage 1.1 (TID 501) in 18653 ms on hadoop-slave1 (executor 3) (21/51)
18/02/14 13:24:00 INFO TaskSetManager: Starting task 20.0 in stage 1.1 (TID 516, hadoop-slave5, executor 7, partition 81, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:00 INFO TaskSetManager: Finished task 11.0 in stage 1.1 (TID 509) in 13124 ms on hadoop-slave5 (executor 7) (22/51)
18/02/14 13:24:00 INFO TaskSetManager: Starting task 21.0 in stage 1.1 (TID 517, hadoop-slave5, executor 7, partition 84, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:00 INFO TaskSetManager: Finished task 6.0 in stage 1.1 (TID 507) in 13125 ms on hadoop-slave5 (executor 7) (23/51)
18/02/14 13:24:00 INFO TaskSetManager: Starting task 23.0 in stage 1.1 (TID 518, hadoop-slave5, executor 7, partition 88, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:00 INFO TaskSetManager: Finished task 1.0 in stage 1.1 (TID 505) in 13127 ms on hadoop-slave5 (executor 7) (24/51)
18/02/14 13:24:00 INFO TaskSetManager: Starting task 24.0 in stage 1.1 (TID 519, hadoop-slave5, executor 7, partition 92, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:00 INFO TaskSetManager: Finished task 13.0 in stage 1.1 (TID 510) in 13182 ms on hadoop-slave5 (executor 7) (25/51)
18/02/14 13:24:00 INFO TaskSetManager: Starting task 25.0 in stage 1.1 (TID 520, hadoop-slave5, executor 7, partition 99, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:00 INFO TaskSetManager: Finished task 8.0 in stage 1.1 (TID 508) in 13276 ms on hadoop-slave5 (executor 7) (26/51)
18/02/14 13:24:01 INFO TaskSetManager: Starting task 27.0 in stage 1.1 (TID 521, hadoop-slave2, executor 6, partition 103, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:01 INFO TaskSetManager: Finished task 5.0 in stage 0.2 (TID 470) in 74963 ms on hadoop-slave2 (executor 6) (20/25)
18/02/14 13:24:01 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on hadoop-slave2:33289 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:24:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave2:33289 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:01 INFO TaskSetManager: Starting task 34.0 in stage 1.1 (TID 522, hadoop-slave2, executor 6, partition 122, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:01 INFO TaskSetManager: Finished task 1.0 in stage 0.2 (TID 468) in 75127 ms on hadoop-slave2 (executor 6) (21/25)
18/02/14 13:24:01 INFO TaskSetManager: Starting task 35.0 in stage 1.1 (TID 523, hadoop-slave2, executor 6, partition 124, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:01 INFO TaskSetManager: Finished task 9.0 in stage 0.2 (TID 473) in 75189 ms on hadoop-slave2 (executor 6) (22/25)
18/02/14 13:24:01 INFO TaskSetManager: Starting task 37.0 in stage 1.1 (TID 524, hadoop-slave2, executor 6, partition 131, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:01 INFO TaskSetManager: Finished task 8.0 in stage 0.2 (TID 472) in 75419 ms on hadoop-slave2 (executor 6) (23/25)
18/02/14 13:24:03 INFO TaskSetManager: Starting task 26.0 in stage 1.1 (TID 525, hadoop-slave5, executor 7, partition 102, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:03 INFO TaskSetManager: Finished task 5.0 in stage 1.1 (TID 506) in 15986 ms on hadoop-slave5 (executor 7) (27/51)
18/02/14 13:24:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave5:35799 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:04 INFO TaskSetManager: Starting task 51.1 in stage 3.0 (TID 526, hadoop-slave1, executor 3, partition 51, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:04 INFO TaskSetManager: Finished task 0.0 in stage 1.1 (TID 504) in 19068 ms on hadoop-slave1 (executor 3) (28/51)
18/02/14 13:24:04 INFO TaskSetManager: Starting task 42.0 in stage 1.1 (TID 527, hadoop-slave2, executor 6, partition 162, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:04 INFO TaskSetManager: Finished task 4.0 in stage 0.2 (TID 469) in 78280 ms on hadoop-slave2 (executor 6) (24/25)
18/02/14 13:24:04 INFO TaskSetManager: Starting task 44.0 in stage 1.1 (TID 528, hadoop-slave2, executor 6, partition 172, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:04 INFO TaskSetManager: Finished task 6.0 in stage 0.2 (TID 471) in 78324 ms on hadoop-slave2 (executor 6) (25/25)
18/02/14 13:24:04 INFO YarnScheduler: Removed TaskSet 0.2, whose tasks have all completed, from pool 
18/02/14 13:24:04 INFO DAGScheduler: ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) finished in 87.063 s
18/02/14 13:24:04 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:24:04 INFO DAGScheduler: running: Set(ShuffleMapStage 1, ShuffleMapStage 3)
18/02/14 13:24:04 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:24:04 INFO DAGScheduler: failed: Set()
18/02/14 13:24:04 INFO DAGScheduler: Resubmitting ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) because some of its tasks had failed: 6, 8, 14, 15, 19, 20, 26, 28, 35, 38, 44, 45, 48, 49, 64, 65, 67, 68, 69, 72, 78, 88, 90, 92, 94, 96
18/02/14 13:24:04 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:24:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 10.3 KB, free 4.1 GB)
18/02/14 13:24:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.1 KB, free 4.1 GB)
18/02/14 13:24:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.19.3.36:33535 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:04 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:996
18/02/14 13:24:04 INFO DAGScheduler: Submitting 26 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74)
18/02/14 13:24:04 INFO YarnScheduler: Adding task set 0.3 with 26 tasks
18/02/14 13:24:06 INFO TaskSetManager: Starting task 2.0 in stage 0.3 (TID 529, hadoop-slave1, executor 3, partition 14, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:06 INFO TaskSetManager: Finished task 50.0 in stage 1.1 (TID 502) in 25168 ms on hadoop-slave1 (executor 3) (29/51)
18/02/14 13:24:06 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on hadoop-slave1:46007 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:07 INFO TaskSetManager: Starting task 0.0 in stage 0.3 (TID 530, hadoop-slave5, executor 7, partition 6, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:07 INFO TaskSetManager: Finished task 24.0 in stage 1.1 (TID 519) in 7116 ms on hadoop-slave5 (executor 7) (30/51)
18/02/14 13:24:07 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on hadoop-slave5:35799 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave5:35799 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:08 INFO TaskSetManager: Starting task 3.0 in stage 0.3 (TID 531, hadoop-slave1, executor 3, partition 15, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:08 INFO TaskSetManager: Finished task 3.0 in stage 1.1 (TID 512) in 17192 ms on hadoop-slave1 (executor 3) (31/51)
18/02/14 13:24:08 INFO TaskSetManager: Starting task 1.0 in stage 0.3 (TID 532, hadoop-slave5, executor 7, partition 8, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:08 INFO TaskSetManager: Finished task 26.0 in stage 1.1 (TID 525) in 4853 ms on hadoop-slave5 (executor 7) (32/51)
18/02/14 13:24:08 INFO TaskSetManager: Starting task 4.0 in stage 0.3 (TID 533, hadoop-slave5, executor 7, partition 19, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:08 INFO TaskSetManager: Finished task 20.0 in stage 1.1 (TID 516) in 7759 ms on hadoop-slave5 (executor 7) (33/51)
18/02/14 13:24:08 INFO TaskSetManager: Starting task 5.0 in stage 0.3 (TID 534, hadoop-slave2, executor 6, partition 20, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:08 INFO TaskSetManager: Finished task 34.0 in stage 1.1 (TID 522) in 6681 ms on hadoop-slave2 (executor 6) (34/51)
18/02/14 13:24:08 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on hadoop-slave2:33289 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:08 INFO TaskSetManager: Starting task 7.0 in stage 0.3 (TID 535, hadoop-slave2, executor 6, partition 28, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:08 INFO TaskSetManager: Finished task 27.0 in stage 1.1 (TID 521) in 6893 ms on hadoop-slave2 (executor 6) (35/51)
18/02/14 13:24:08 INFO TaskSetManager: Starting task 8.0 in stage 0.3 (TID 536, hadoop-slave2, executor 6, partition 35, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:08 INFO TaskSetManager: Finished task 35.0 in stage 1.1 (TID 523) in 7040 ms on hadoop-slave2 (executor 6) (36/51)
18/02/14 13:24:09 INFO TaskSetManager: Starting task 9.0 in stage 0.3 (TID 537, hadoop-slave2, executor 6, partition 38, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:09 INFO TaskSetManager: Finished task 37.0 in stage 1.1 (TID 524) in 7548 ms on hadoop-slave2 (executor 6) (37/51)
18/02/14 13:24:09 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop-slave3:33203 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:24:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.33:51972
18/02/14 13:24:10 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 508 bytes
18/02/14 13:24:10 INFO TaskSetManager: Starting task 10.0 in stage 0.3 (TID 538, hadoop-slave3, executor 5, partition 44, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:10 WARN TaskSetManager: Lost task 95.1 in stage 2.0 (TID 423, hadoop-slave3, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=95, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:24:10 INFO TaskSetManager: Task 95.1 in stage 2.0 (TID 423) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:24:10 INFO DAGScheduler: Resubmitting ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:24:10 INFO TaskSetManager: Starting task 11.0 in stage 0.3 (TID 539, hadoop-slave3, executor 5, partition 45, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:10 WARN TaskSetManager: Lost task 18.1 in stage 2.0 (TID 419, hadoop-slave3, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=18, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:24:10 INFO TaskSetManager: Task 18.1 in stage 2.0 (TID 419) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:24:10 INFO TaskSetManager: Starting task 12.0 in stage 0.3 (TID 540, hadoop-slave3, executor 5, partition 48, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:10 WARN TaskSetManager: Lost task 101.1 in stage 2.0 (TID 422, hadoop-slave3, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=101, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:24:10 INFO TaskSetManager: Task 101.1 in stage 2.0 (TID 422) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:24:10 INFO TaskSetManager: Starting task 14.0 in stage 0.3 (TID 541, hadoop-slave3, executor 5, partition 64, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:10 WARN TaskSetManager: Lost task 169.1 in stage 2.0 (TID 424, hadoop-slave3, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=169, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:24:10 INFO TaskSetManager: Task 169.1 in stage 2.0 (TID 424) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:24:10 INFO TaskSetManager: Starting task 15.0 in stage 0.3 (TID 542, hadoop-slave3, executor 5, partition 65, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:10 WARN TaskSetManager: Lost task 81.1 in stage 2.0 (TID 420, hadoop-slave3, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=81, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:24:10 INFO TaskSetManager: Task 81.1 in stage 2.0 (TID 420) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:24:10 INFO TaskSetManager: Starting task 16.0 in stage 0.3 (TID 543, hadoop-slave3, executor 5, partition 67, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:10 WARN TaskSetManager: Lost task 5.1 in stage 2.0 (TID 421, hadoop-slave3, executor 5): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:24:10 INFO TaskSetManager: Task 5.1 in stage 2.0 (TID 421) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:24:10 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/02/14 13:24:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on hadoop-slave3:33203 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:10 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:24:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave3:33203 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:11 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.19.3.36:33535 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:11 INFO BlockManagerInfo: Removed broadcast_11_piece0 on hadoop-slave2:33289 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:11 INFO TaskSetManager: Starting task 18.0 in stage 0.3 (TID 544, hadoop-slave2, executor 6, partition 69, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:11 INFO TaskSetManager: Finished task 42.0 in stage 1.1 (TID 527) in 7060 ms on hadoop-slave2 (executor 6) (38/51)
18/02/14 13:24:12 INFO TaskSetManager: Starting task 19.0 in stage 0.3 (TID 545, hadoop-slave2, executor 6, partition 72, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:12 INFO TaskSetManager: Finished task 44.0 in stage 1.1 (TID 528) in 7291 ms on hadoop-slave2 (executor 6) (39/51)
18/02/14 13:24:12 INFO TaskSetManager: Starting task 6.0 in stage 0.3 (TID 546, hadoop-slave1, executor 3, partition 26, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:12 INFO TaskSetManager: Finished task 18.0 in stage 1.1 (TID 515) in 13116 ms on hadoop-slave1 (executor 3) (40/51)
18/02/14 13:24:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on hadoop-slave1:46007 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:12 INFO TaskSetManager: Starting task 25.0 in stage 0.3 (TID 547, hadoop-slave2, executor 6, partition 96, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:12 INFO TaskSetManager: Finished task 5.0 in stage 0.3 (TID 534) in 4409 ms on hadoop-slave2 (executor 6) (1/26)
18/02/14 13:24:12 INFO TaskSetManager: Starting task 33.0 in stage 1.1 (TID 548, hadoop-slave2, executor 6, partition 119, RACK_LOCAL, 6941 bytes)
18/02/14 13:24:12 INFO TaskSetManager: Finished task 8.0 in stage 0.3 (TID 536) in 4192 ms on hadoop-slave2 (executor 6) (2/26)
18/02/14 13:24:13 INFO TaskSetManager: Starting task 13.0 in stage 0.3 (TID 549, hadoop-slave1, executor 3, partition 49, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:13 INFO TaskSetManager: Finished task 2.0 in stage 0.3 (TID 529) in 7013 ms on hadoop-slave1 (executor 3) (3/26)
18/02/14 13:24:13 INFO TaskSetManager: Starting task 20.0 in stage 0.3 (TID 550, hadoop-slave1, executor 3, partition 78, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:13 INFO TaskSetManager: Finished task 17.0 in stage 1.1 (TID 514) in 14480 ms on hadoop-slave1 (executor 3) (41/51)
18/02/14 13:24:13 INFO TaskSetManager: Starting task 22.0 in stage 0.3 (TID 551, hadoop-slave1, executor 3, partition 90, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:13 INFO TaskSetManager: Finished task 3.0 in stage 0.3 (TID 531) in 5257 ms on hadoop-slave1 (executor 3) (4/26)
18/02/14 13:24:13 INFO TaskSetManager: Starting task 24.0 in stage 0.3 (TID 552, hadoop-slave1, executor 3, partition 94, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:13 INFO TaskSetManager: Finished task 51.1 in stage 3.0 (TID 526) in 8750 ms on hadoop-slave1 (executor 3) (23/200)
18/02/14 13:24:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.19.3.36:33535 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on hadoop-slave1:46007 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:14 INFO TaskSetManager: Starting task 17.0 in stage 0.3 (TID 553, hadoop-slave5, executor 7, partition 68, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:14 INFO TaskSetManager: Finished task 25.0 in stage 1.1 (TID 520) in 13716 ms on hadoop-slave5 (executor 7) (42/51)
18/02/14 13:24:14 INFO TaskSetManager: Starting task 40.0 in stage 1.1 (TID 554, hadoop-slave1, executor 3, partition 158, RACK_LOCAL, 6941 bytes)
18/02/14 13:24:14 INFO TaskSetManager: Finished task 16.0 in stage 1.1 (TID 513) in 17209 ms on hadoop-slave1 (executor 3) (43/51)
18/02/14 13:24:14 INFO TaskSetManager: Starting task 21.0 in stage 0.3 (TID 555, hadoop-slave5, executor 7, partition 88, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:14 INFO TaskSetManager: Finished task 0.0 in stage 0.3 (TID 530) in 6947 ms on hadoop-slave5 (executor 7) (5/26)
18/02/14 13:24:15 INFO TaskSetManager: Starting task 23.0 in stage 0.3 (TID 556, hadoop-slave5, executor 7, partition 92, NODE_LOCAL, 6832 bytes)
18/02/14 13:24:15 INFO TaskSetManager: Finished task 1.0 in stage 0.3 (TID 532) in 7069 ms on hadoop-slave5 (executor 7) (6/26)
18/02/14 13:24:15 INFO TaskSetManager: Starting task 41.0 in stage 1.1 (TID 557, hadoop-slave5, executor 7, partition 159, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:15 INFO TaskSetManager: Finished task 23.0 in stage 1.1 (TID 518) in 15342 ms on hadoop-slave5 (executor 7) (44/51)
18/02/14 13:24:17 INFO TaskSetManager: Starting task 43.0 in stage 1.1 (TID 558, hadoop-slave5, executor 7, partition 164, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:17 INFO TaskSetManager: Finished task 21.0 in stage 1.1 (TID 517) in 16721 ms on hadoop-slave5 (executor 7) (45/51)
18/02/14 13:24:17 INFO TaskSetManager: Starting task 38.1 in stage 3.0 (TID 559, hadoop-slave2, executor 6, partition 38, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:17 INFO TaskSetManager: Finished task 25.0 in stage 0.3 (TID 547) in 4370 ms on hadoop-slave2 (executor 6) (7/26)
18/02/14 13:24:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave2:33289 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:24:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave2:33289 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:17 INFO TaskSetManager: Starting task 35.1 in stage 3.0 (TID 560, hadoop-slave2, executor 6, partition 35, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:17 INFO TaskSetManager: Finished task 18.0 in stage 0.3 (TID 544) in 5337 ms on hadoop-slave2 (executor 6) (8/26)
18/02/14 13:24:17 INFO TaskSetManager: Starting task 3.1 in stage 3.0 (TID 561, hadoop-slave2, executor 6, partition 3, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:17 INFO TaskSetManager: Finished task 33.0 in stage 1.1 (TID 548) in 4710 ms on hadoop-slave2 (executor 6) (46/51)
18/02/14 13:24:18 INFO TaskSetManager: Starting task 45.0 in stage 1.1 (TID 562, hadoop-slave3, executor 5, partition 181, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:18 INFO TaskSetManager: Finished task 15.0 in stage 0.3 (TID 542) in 7780 ms on hadoop-slave3 (executor 5) (9/26)
18/02/14 13:24:18 INFO TaskSetManager: Starting task 46.0 in stage 1.1 (TID 563, hadoop-slave3, executor 5, partition 184, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:18 INFO TaskSetManager: Finished task 16.0 in stage 0.3 (TID 543) in 7781 ms on hadoop-slave3 (executor 5) (10/26)
18/02/14 13:24:18 INFO TaskSetManager: Starting task 58.1 in stage 3.0 (TID 564, hadoop-slave5, executor 7, partition 58, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:18 INFO TaskSetManager: Finished task 21.0 in stage 0.3 (TID 555) in 3794 ms on hadoop-slave5 (executor 7) (11/26)
18/02/14 13:24:18 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on hadoop-slave3:33203 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:24:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave5:35799 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:24:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave3:33203 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave5:35799 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:18 INFO TaskSetManager: Starting task 59.1 in stage 3.0 (TID 565, hadoop-slave3, executor 5, partition 59, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:18 INFO TaskSetManager: Finished task 10.0 in stage 0.3 (TID 538) in 8361 ms on hadoop-slave3 (executor 5) (12/26)
18/02/14 13:24:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave3:33203 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:24:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave3:33203 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:18 INFO TaskSetManager: Starting task 80.1 in stage 3.0 (TID 566, hadoop-slave2, executor 6, partition 80, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:18 INFO TaskSetManager: Finished task 7.0 in stage 0.3 (TID 535) in 10572 ms on hadoop-slave2 (executor 6) (13/26)
18/02/14 13:24:19 INFO TaskSetManager: Starting task 19.1 in stage 3.0 (TID 567, hadoop-slave3, executor 5, partition 19, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:19 INFO TaskSetManager: Finished task 12.0 in stage 0.3 (TID 540) in 8582 ms on hadoop-slave3 (executor 5) (14/26)
18/02/14 13:24:19 INFO TaskSetManager: Starting task 48.1 in stage 3.0 (TID 568, hadoop-slave3, executor 5, partition 48, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:19 INFO TaskSetManager: Finished task 11.0 in stage 0.3 (TID 539) in 9209 ms on hadoop-slave3 (executor 5) (15/26)
18/02/14 13:24:19 INFO TaskSetManager: Starting task 6.1 in stage 3.0 (TID 569, hadoop-slave5, executor 7, partition 6, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:19 INFO TaskSetManager: Finished task 4.0 in stage 0.3 (TID 533) in 11616 ms on hadoop-slave5 (executor 7) (16/26)
18/02/14 13:24:19 INFO TaskSetManager: Starting task 90.1 in stage 3.0 (TID 570, hadoop-slave5, executor 7, partition 90, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:19 INFO TaskSetManager: Finished task 23.0 in stage 0.3 (TID 556) in 4882 ms on hadoop-slave5 (executor 7) (17/26)
18/02/14 13:24:20 INFO TaskSetManager: Starting task 34.1 in stage 3.0 (TID 571, hadoop-slave2, executor 6, partition 34, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:20 INFO TaskSetManager: Finished task 19.0 in stage 0.3 (TID 545) in 8134 ms on hadoop-slave2 (executor 6) (18/26)
18/02/14 13:24:20 INFO TaskSetManager: Starting task 10.1 in stage 3.0 (TID 572, hadoop-slave1, executor 3, partition 10, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:20 INFO TaskSetManager: Finished task 20.0 in stage 0.3 (TID 550) in 7216 ms on hadoop-slave1 (executor 3) (19/26)
18/02/14 13:24:20 INFO TaskSetManager: Starting task 78.1 in stage 3.0 (TID 573, hadoop-slave1, executor 3, partition 78, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:20 INFO TaskSetManager: Finished task 22.0 in stage 0.3 (TID 551) in 7308 ms on hadoop-slave1 (executor 3) (20/26)
18/02/14 13:24:20 INFO TaskSetManager: Starting task 73.1 in stage 3.0 (TID 574, hadoop-slave2, executor 6, partition 73, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:20 INFO TaskSetManager: Finished task 9.0 in stage 0.3 (TID 537) in 11110 ms on hadoop-slave2 (executor 6) (21/26)
18/02/14 13:24:21 INFO TaskSetManager: Starting task 94.1 in stage 3.0 (TID 575, hadoop-slave5, executor 7, partition 94, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:21 INFO TaskSetManager: Finished task 17.0 in stage 0.3 (TID 553) in 6808 ms on hadoop-slave5 (executor 7) (22/26)
18/02/14 13:24:21 INFO TaskSetManager: Starting task 79.1 in stage 3.0 (TID 576, hadoop-slave1, executor 3, partition 79, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:21 INFO TaskSetManager: Finished task 13.0 in stage 0.3 (TID 549) in 8189 ms on hadoop-slave1 (executor 3) (23/26)
18/02/14 13:24:21 INFO TaskSetManager: Starting task 24.1 in stage 3.0 (TID 577, hadoop-slave5, executor 7, partition 24, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:21 INFO TaskSetManager: Finished task 41.0 in stage 1.1 (TID 557) in 5834 ms on hadoop-slave5 (executor 7) (47/51)
18/02/14 13:24:21 INFO TaskSetManager: Starting task 28.1 in stage 3.0 (TID 578, hadoop-slave2, executor 6, partition 28, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:21 INFO TaskSetManager: Finished task 38.1 in stage 3.0 (TID 559) in 4502 ms on hadoop-slave2 (executor 6) (24/200)
18/02/14 13:24:21 INFO TaskSetManager: Starting task 55.1 in stage 3.0 (TID 579, hadoop-slave1, executor 3, partition 55, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:21 INFO TaskSetManager: Finished task 24.0 in stage 0.3 (TID 552) in 8428 ms on hadoop-slave1 (executor 3) (24/26)
18/02/14 13:24:22 INFO TaskSetManager: Starting task 71.1 in stage 3.0 (TID 580, hadoop-slave2, executor 6, partition 71, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:22 INFO TaskSetManager: Finished task 35.1 in stage 3.0 (TID 560) in 4958 ms on hadoop-slave2 (executor 6) (25/200)
18/02/14 13:24:22 INFO TaskSetManager: Starting task 20.1 in stage 3.0 (TID 581, hadoop-slave2, executor 6, partition 20, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:22 INFO TaskSetManager: Finished task 3.1 in stage 3.0 (TID 561) in 4860 ms on hadoop-slave2 (executor 6) (26/200)
18/02/14 13:24:22 INFO TaskSetManager: Starting task 93.1 in stage 3.0 (TID 582, hadoop-slave1, executor 3, partition 93, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:22 INFO TaskSetManager: Finished task 6.0 in stage 0.3 (TID 546) in 10176 ms on hadoop-slave1 (executor 3) (25/26)
18/02/14 13:24:23 INFO TaskSetManager: Starting task 8.1 in stage 3.0 (TID 583, hadoop-slave2, executor 6, partition 8, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:23 INFO TaskSetManager: Finished task 80.1 in stage 3.0 (TID 566) in 4664 ms on hadoop-slave2 (executor 6) (27/200)
18/02/14 13:24:23 INFO TaskSetManager: Starting task 41.1 in stage 3.0 (TID 584, hadoop-slave3, executor 5, partition 41, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:23 INFO TaskSetManager: Finished task 19.1 in stage 3.0 (TID 567) in 4847 ms on hadoop-slave3 (executor 5) (28/200)
18/02/14 13:24:23 INFO TaskSetManager: Starting task 23.1 in stage 3.0 (TID 585, hadoop-slave3, executor 5, partition 23, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:23 INFO TaskSetManager: Finished task 45.0 in stage 1.1 (TID 562) in 5650 ms on hadoop-slave3 (executor 5) (48/51)
18/02/14 13:24:23 INFO TaskSetManager: Starting task 44.1 in stage 3.0 (TID 586, hadoop-slave5, executor 7, partition 44, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:23 INFO TaskSetManager: Finished task 58.1 in stage 3.0 (TID 564) in 5701 ms on hadoop-slave5 (executor 7) (29/200)
18/02/14 13:24:24 INFO TaskSetManager: Starting task 65.1 in stage 3.0 (TID 587, hadoop-slave3, executor 5, partition 65, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:24 INFO TaskSetManager: Finished task 59.1 in stage 3.0 (TID 565) in 5329 ms on hadoop-slave3 (executor 5) (30/200)
18/02/14 13:24:24 INFO TaskSetManager: Starting task 53.1 in stage 3.0 (TID 588, hadoop-slave3, executor 5, partition 53, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:24 INFO TaskSetManager: Finished task 14.0 in stage 0.3 (TID 541) in 14178 ms on hadoop-slave3 (executor 5) (26/26)
18/02/14 13:24:24 INFO YarnScheduler: Removed TaskSet 0.3, whose tasks have all completed, from pool 
18/02/14 13:24:24 INFO DAGScheduler: ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) finished in 19.763 s
18/02/14 13:24:24 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:24:24 INFO DAGScheduler: running: Set(ShuffleMapStage 1, ShuffleMapStage 3)
18/02/14 13:24:24 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:24:24 INFO DAGScheduler: failed: Set()
18/02/14 13:24:24 INFO TaskSetManager: Starting task 56.1 in stage 3.0 (TID 589, hadoop-slave3, executor 5, partition 56, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:24 INFO TaskSetManager: Finished task 46.0 in stage 1.1 (TID 563) in 6422 ms on hadoop-slave3 (executor 5) (49/51)
18/02/14 13:24:24 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID 590, hadoop-slave2, executor 6, partition 0, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:24 INFO TaskSetManager: Finished task 34.1 in stage 3.0 (TID 571) in 4398 ms on hadoop-slave2 (executor 6) (31/200)
18/02/14 13:24:24 INFO TaskSetManager: Starting task 36.1 in stage 3.0 (TID 591, hadoop-slave1, executor 3, partition 36, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:24 INFO TaskSetManager: Finished task 78.1 in stage 3.0 (TID 573) in 4100 ms on hadoop-slave1 (executor 3) (32/200)
18/02/14 13:24:25 INFO TaskSetManager: Starting task 72.1 in stage 3.0 (TID 592, hadoop-slave3, executor 5, partition 72, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:25 INFO TaskSetManager: Finished task 48.1 in stage 3.0 (TID 568) in 5858 ms on hadoop-slave3 (executor 5) (33/200)
18/02/14 13:24:25 INFO TaskSetManager: Starting task 30.1 in stage 3.0 (TID 593, hadoop-slave1, executor 3, partition 30, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:25 INFO TaskSetManager: Finished task 10.1 in stage 3.0 (TID 572) in 5412 ms on hadoop-slave1 (executor 3) (34/200)
18/02/14 13:24:25 INFO TaskSetManager: Starting task 76.1 in stage 3.0 (TID 594, hadoop-slave5, executor 7, partition 76, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:25 INFO TaskSetManager: Finished task 6.1 in stage 3.0 (TID 569) in 6126 ms on hadoop-slave5 (executor 7) (35/200)
18/02/14 13:24:25 INFO TaskSetManager: Starting task 31.1 in stage 3.0 (TID 595, hadoop-slave5, executor 7, partition 31, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:25 INFO TaskSetManager: Finished task 90.1 in stage 3.0 (TID 570) in 5961 ms on hadoop-slave5 (executor 7) (36/200)
18/02/14 13:24:26 INFO TaskSetManager: Starting task 27.1 in stage 3.0 (TID 596, hadoop-slave2, executor 6, partition 27, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:26 INFO TaskSetManager: Finished task 28.1 in stage 3.0 (TID 578) in 4648 ms on hadoop-slave2 (executor 6) (37/200)
18/02/14 13:24:26 INFO TaskSetManager: Starting task 12.1 in stage 3.0 (TID 597, hadoop-slave1, executor 3, partition 12, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:26 INFO TaskSetManager: Finished task 79.1 in stage 3.0 (TID 576) in 5440 ms on hadoop-slave1 (executor 3) (38/200)
18/02/14 13:24:26 INFO TaskSetManager: Starting task 50.1 in stage 3.0 (TID 598, hadoop-slave2, executor 6, partition 50, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:26 INFO TaskSetManager: Finished task 73.1 in stage 3.0 (TID 574) in 6129 ms on hadoop-slave2 (executor 6) (39/200)
18/02/14 13:24:26 INFO TaskSetManager: Starting task 82.1 in stage 3.0 (TID 599, hadoop-slave1, executor 3, partition 82, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:26 INFO TaskSetManager: Finished task 55.1 in stage 3.0 (TID 579) in 5071 ms on hadoop-slave1 (executor 3) (40/200)
18/02/14 13:24:26 INFO TaskSetManager: Starting task 54.1 in stage 3.0 (TID 600, hadoop-slave5, executor 7, partition 54, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:26 INFO TaskSetManager: Finished task 94.1 in stage 3.0 (TID 575) in 5840 ms on hadoop-slave5 (executor 7) (41/200)
18/02/14 13:24:26 INFO TaskSetManager: Starting task 5.1 in stage 3.0 (TID 601, hadoop-slave2, executor 6, partition 5, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:26 INFO TaskSetManager: Finished task 20.1 in stage 3.0 (TID 581) in 4382 ms on hadoop-slave2 (executor 6) (42/200)
18/02/14 13:24:26 INFO TaskSetManager: Starting task 83.1 in stage 3.0 (TID 602, hadoop-slave1, executor 3, partition 83, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:26 INFO TaskSetManager: Finished task 93.1 in stage 3.0 (TID 582) in 4224 ms on hadoop-slave1 (executor 3) (43/200)
18/02/14 13:24:27 INFO TaskSetManager: Starting task 2.1 in stage 3.0 (TID 603, hadoop-slave5, executor 7, partition 2, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:27 INFO TaskSetManager: Finished task 24.1 in stage 3.0 (TID 577) in 5760 ms on hadoop-slave5 (executor 7) (44/200)
18/02/14 13:24:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.19.3.36:33535 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on hadoop-slave1:46007 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on hadoop-slave2:33289 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on hadoop-slave3:33203 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on hadoop-slave5:35799 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:24:28 INFO TaskSetManager: Starting task 61.1 in stage 3.0 (TID 604, hadoop-slave3, executor 5, partition 61, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:28 INFO TaskSetManager: Finished task 65.1 in stage 3.0 (TID 587) in 3910 ms on hadoop-slave3 (executor 5) (45/200)
18/02/14 13:24:28 INFO TaskSetManager: Starting task 17.1 in stage 3.0 (TID 605, hadoop-slave3, executor 5, partition 17, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:28 INFO TaskSetManager: Finished task 41.1 in stage 3.0 (TID 584) in 4648 ms on hadoop-slave3 (executor 5) (46/200)
18/02/14 13:24:29 INFO TaskSetManager: Starting task 62.1 in stage 3.0 (TID 606, hadoop-slave3, executor 5, partition 62, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:29 INFO TaskSetManager: Finished task 56.1 in stage 3.0 (TID 589) in 4791 ms on hadoop-slave3 (executor 5) (47/200)
18/02/14 13:24:29 INFO TaskSetManager: Starting task 64.1 in stage 3.0 (TID 607, hadoop-slave5, executor 7, partition 64, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:29 INFO TaskSetManager: Finished task 43.0 in stage 1.1 (TID 558) in 12502 ms on hadoop-slave5 (executor 7) (50/51)
18/02/14 13:24:29 INFO TaskSetManager: Starting task 46.1 in stage 3.0 (TID 608, hadoop-slave3, executor 5, partition 46, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:29 INFO TaskSetManager: Finished task 72.1 in stage 3.0 (TID 592) in 4056 ms on hadoop-slave3 (executor 5) (48/200)
18/02/14 13:24:29 INFO TaskSetManager: Starting task 91.1 in stage 3.0 (TID 609, hadoop-slave1, executor 3, partition 91, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:29 INFO TaskSetManager: Finished task 30.1 in stage 3.0 (TID 593) in 4053 ms on hadoop-slave1 (executor 3) (49/200)
18/02/14 13:24:29 INFO TaskSetManager: Starting task 13.1 in stage 3.0 (TID 610, hadoop-slave2, executor 6, partition 13, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:29 INFO TaskSetManager: Finished task 71.1 in stage 3.0 (TID 580) in 7710 ms on hadoop-slave2 (executor 6) (50/200)
18/02/14 13:24:30 INFO TaskSetManager: Starting task 32.1 in stage 3.0 (TID 611, hadoop-slave2, executor 6, partition 32, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:30 INFO TaskSetManager: Finished task 8.1 in stage 3.0 (TID 583) in 6751 ms on hadoop-slave2 (executor 6) (51/200)
18/02/14 13:24:30 INFO TaskSetManager: Starting task 16.1 in stage 3.0 (TID 612, hadoop-slave2, executor 6, partition 16, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:30 INFO TaskSetManager: Finished task 0.1 in stage 3.0 (TID 590) in 6183 ms on hadoop-slave2 (executor 6) (52/200)
18/02/14 13:24:31 INFO TaskSetManager: Starting task 43.1 in stage 3.0 (TID 613, hadoop-slave1, executor 3, partition 43, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:31 INFO TaskSetManager: Finished task 83.1 in stage 3.0 (TID 602) in 4757 ms on hadoop-slave1 (executor 3) (53/200)
18/02/14 13:24:31 INFO TaskSetManager: Starting task 25.1 in stage 3.0 (TID 614, hadoop-slave5, executor 7, partition 25, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:31 INFO TaskSetManager: Finished task 31.1 in stage 3.0 (TID 595) in 6025 ms on hadoop-slave5 (executor 7) (54/200)
18/02/14 13:24:32 INFO TaskSetManager: Starting task 88.1 in stage 3.0 (TID 615, hadoop-slave5, executor 7, partition 88, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:32 INFO TaskSetManager: Finished task 44.1 in stage 3.0 (TID 586) in 8195 ms on hadoop-slave5 (executor 7) (55/200)
18/02/14 13:24:32 INFO TaskSetManager: Starting task 47.1 in stage 3.0 (TID 616, hadoop-slave3, executor 5, partition 47, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:32 INFO TaskSetManager: Finished task 53.1 in stage 3.0 (TID 588) in 7586 ms on hadoop-slave3 (executor 5) (56/200)
18/02/14 13:24:32 INFO TaskSetManager: Starting task 68.1 in stage 3.0 (TID 617, hadoop-slave3, executor 5, partition 68, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:32 INFO TaskSetManager: Finished task 61.1 in stage 3.0 (TID 604) in 4797 ms on hadoop-slave3 (executor 5) (57/200)
18/02/14 13:24:32 INFO TaskSetManager: Starting task 4.1 in stage 3.0 (TID 618, hadoop-slave1, executor 3, partition 4, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:32 INFO TaskSetManager: Finished task 36.1 in stage 3.0 (TID 591) in 8235 ms on hadoop-slave1 (executor 3) (58/200)
18/02/14 13:24:32 INFO TaskSetManager: Starting task 69.1 in stage 3.0 (TID 619, hadoop-slave2, executor 6, partition 69, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:32 INFO TaskSetManager: Finished task 50.1 in stage 3.0 (TID 598) in 6201 ms on hadoop-slave2 (executor 6) (59/200)
18/02/14 13:24:33 INFO TaskSetManager: Starting task 81.1 in stage 3.0 (TID 620, hadoop-slave3, executor 5, partition 81, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:33 INFO TaskSetManager: Finished task 62.1 in stage 3.0 (TID 606) in 3722 ms on hadoop-slave3 (executor 5) (60/200)
18/02/14 13:24:33 INFO TaskSetManager: Starting task 22.1 in stage 3.0 (TID 621, hadoop-slave2, executor 6, partition 22, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:33 INFO TaskSetManager: Finished task 27.1 in stage 3.0 (TID 596) in 6909 ms on hadoop-slave2 (executor 6) (61/200)
18/02/14 13:24:33 INFO TaskSetManager: Starting task 37.1 in stage 3.0 (TID 622, hadoop-slave1, executor 3, partition 37, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:33 INFO TaskSetManager: Finished task 91.1 in stage 3.0 (TID 609) in 3821 ms on hadoop-slave1 (executor 3) (62/200)
18/02/14 13:24:34 INFO TaskSetManager: Starting task 92.1 in stage 3.0 (TID 623, hadoop-slave3, executor 5, partition 92, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:34 INFO TaskSetManager: Finished task 23.1 in stage 3.0 (TID 585) in 10138 ms on hadoop-slave3 (executor 5) (63/200)
18/02/14 13:24:34 INFO TaskSetManager: Starting task 70.1 in stage 3.0 (TID 624, hadoop-slave2, executor 6, partition 70, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:34 INFO TaskSetManager: Finished task 5.1 in stage 3.0 (TID 601) in 7586 ms on hadoop-slave2 (executor 6) (64/200)
18/02/14 13:24:34 INFO TaskSetManager: Starting task 84.1 in stage 3.0 (TID 625, hadoop-slave5, executor 7, partition 84, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:34 INFO TaskSetManager: Finished task 2.1 in stage 3.0 (TID 603) in 7215 ms on hadoop-slave5 (executor 7) (65/200)
18/02/14 13:24:34 INFO TaskSetManager: Starting task 29.1 in stage 3.0 (TID 626, hadoop-slave1, executor 3, partition 29, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:34 INFO TaskSetManager: Finished task 40.0 in stage 1.1 (TID 554) in 20331 ms on hadoop-slave1 (executor 3) (51/51)
18/02/14 13:24:34 INFO YarnScheduler: Removed TaskSet 1.1, whose tasks have all completed, from pool 
18/02/14 13:24:34 INFO DAGScheduler: ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) finished in 159.590 s
18/02/14 13:24:34 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:24:34 INFO DAGScheduler: running: Set(ShuffleMapStage 3)
18/02/14 13:24:34 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:24:34 INFO DAGScheduler: failed: Set()
18/02/14 13:24:34 INFO DAGScheduler: Resubmitting ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) because some of its tasks had failed: 1, 5, 6, 8, 9, 10, 12, 14, 16, 17, 19, 20, 24, 26, 28, 34, 35, 36, 38, 40, 44, 45, 49, 50, 51, 53, 54, 57, 58, 61, 64, 65, 67, 69, 70, 71, 72, 73, 77, 78, 80, 87, 89, 90, 91, 93, 94, 95, 96, 100, 105, 106, 108, 113, 114, 116, 117, 120, 123, 125, 128, 130, 134, 135, 136, 138, 139, 140, 141, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 161, 165, 167, 168, 169, 170, 171, 173, 176, 177, 178, 179, 180, 182, 187, 188, 189, 190, 191, 192, 196, 199
18/02/14 13:24:34 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:24:34 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 14.7 KB, free 4.1 GB)
18/02/14 13:24:34 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.9 KB, free 4.1 GB)
18/02/14 13:24:34 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.19.3.36:33535 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:24:34 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:996
18/02/14 13:24:34 INFO DAGScheduler: Submitting 103 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74)
18/02/14 13:24:34 INFO YarnScheduler: Adding task set 1.2 with 103 tasks
18/02/14 13:24:34 INFO TaskSetManager: Starting task 0.0 in stage 1.2 (TID 627, hadoop-slave1, executor 3, partition 1, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:34 INFO TaskSetManager: Finished task 12.1 in stage 3.0 (TID 597) in 8253 ms on hadoop-slave1 (executor 3) (66/200)
18/02/14 13:24:34 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on hadoop-slave1:46007 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:24:35 INFO TaskSetManager: Starting task 2.0 in stage 1.2 (TID 628, hadoop-slave5, executor 7, partition 6, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:35 INFO TaskSetManager: Finished task 76.1 in stage 3.0 (TID 594) in 9324 ms on hadoop-slave5 (executor 7) (67/200)
18/02/14 13:24:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on hadoop-slave5:35799 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:24:35 INFO TaskSetManager: Starting task 1.0 in stage 1.2 (TID 629, hadoop-slave1, executor 3, partition 5, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:35 INFO TaskSetManager: Finished task 82.1 in stage 3.0 (TID 599) in 8571 ms on hadoop-slave1 (executor 3) (68/200)
18/02/14 13:24:35 INFO TaskSetManager: Starting task 3.0 in stage 1.2 (TID 630, hadoop-slave2, executor 6, partition 8, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:35 INFO TaskSetManager: Finished task 13.1 in stage 3.0 (TID 610) in 5620 ms on hadoop-slave2 (executor 6) (69/200)
18/02/14 13:24:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on hadoop-slave2:33289 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:24:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave2:33289 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:35 INFO TaskSetManager: Starting task 8.0 in stage 1.2 (TID 631, hadoop-slave3, executor 5, partition 16, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:35 INFO TaskSetManager: Finished task 17.1 in stage 3.0 (TID 605) in 7419 ms on hadoop-slave3 (executor 5) (70/200)
18/02/14 13:24:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on hadoop-slave3:33203 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:24:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave3:33203 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:24:36 INFO TaskSetManager: Starting task 4.0 in stage 1.2 (TID 632, hadoop-slave2, executor 6, partition 9, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:36 INFO TaskSetManager: Finished task 16.1 in stage 3.0 (TID 612) in 5311 ms on hadoop-slave2 (executor 6) (71/200)
18/02/14 13:24:36 INFO TaskSetManager: Starting task 5.0 in stage 1.2 (TID 633, hadoop-slave5, executor 7, partition 10, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:36 INFO TaskSetManager: Finished task 64.1 in stage 3.0 (TID 607) in 6648 ms on hadoop-slave5 (executor 7) (72/200)
18/02/14 13:24:36 INFO TaskSetManager: Starting task 6.0 in stage 1.2 (TID 634, hadoop-slave2, executor 6, partition 12, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:36 INFO TaskSetManager: Finished task 32.1 in stage 3.0 (TID 611) in 6021 ms on hadoop-slave2 (executor 6) (73/200)
18/02/14 13:24:36 INFO TaskSetManager: Starting task 9.0 in stage 1.2 (TID 635, hadoop-slave3, executor 5, partition 17, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:36 INFO TaskSetManager: Finished task 46.1 in stage 3.0 (TID 608) in 6920 ms on hadoop-slave3 (executor 5) (74/200)
18/02/14 13:24:36 INFO TaskSetManager: Starting task 7.0 in stage 1.2 (TID 636, hadoop-slave1, executor 3, partition 14, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:36 INFO TaskSetManager: Finished task 43.1 in stage 3.0 (TID 613) in 4813 ms on hadoop-slave1 (executor 3) (75/200)
18/02/14 13:24:36 INFO TaskSetManager: Starting task 10.0 in stage 1.2 (TID 637, hadoop-slave5, executor 7, partition 19, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:36 INFO TaskSetManager: Finished task 54.1 in stage 3.0 (TID 600) in 9675 ms on hadoop-slave5 (executor 7) (76/200)
18/02/14 13:24:36 INFO TaskSetManager: Starting task 11.0 in stage 1.2 (TID 638, hadoop-slave2, executor 6, partition 20, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:36 INFO TaskSetManager: Finished task 69.1 in stage 3.0 (TID 619) in 4055 ms on hadoop-slave2 (executor 6) (77/200)
18/02/14 13:24:37 INFO TaskSetManager: Starting task 12.0 in stage 1.2 (TID 639, hadoop-slave3, executor 5, partition 24, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:37 INFO TaskSetManager: Finished task 81.1 in stage 3.0 (TID 620) in 3996 ms on hadoop-slave3 (executor 5) (78/200)
18/02/14 13:24:37 INFO TaskSetManager: Starting task 13.0 in stage 1.2 (TID 640, hadoop-slave1, executor 3, partition 26, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:37 INFO TaskSetManager: Finished task 4.1 in stage 3.0 (TID 618) in 4344 ms on hadoop-slave1 (executor 3) (79/200)
18/02/14 13:24:37 INFO TaskSetManager: Starting task 14.0 in stage 1.2 (TID 641, hadoop-slave2, executor 6, partition 28, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:37 INFO TaskSetManager: Finished task 22.1 in stage 3.0 (TID 621) in 4272 ms on hadoop-slave2 (executor 6) (80/200)
18/02/14 13:24:37 INFO TaskSetManager: Starting task 17.0 in stage 1.2 (TID 642, hadoop-slave1, executor 3, partition 36, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:37 INFO TaskSetManager: Finished task 37.1 in stage 3.0 (TID 622) in 4256 ms on hadoop-slave1 (executor 3) (81/200)
18/02/14 13:24:38 INFO TaskSetManager: Starting task 19.0 in stage 1.2 (TID 643, hadoop-slave3, executor 5, partition 40, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:38 INFO TaskSetManager: Finished task 68.1 in stage 3.0 (TID 617) in 5272 ms on hadoop-slave3 (executor 5) (82/200)
18/02/14 13:24:38 INFO TaskSetManager: Starting task 20.0 in stage 1.2 (TID 644, hadoop-slave3, executor 5, partition 44, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:38 INFO TaskSetManager: Finished task 92.1 in stage 3.0 (TID 623) in 4522 ms on hadoop-slave3 (executor 5) (83/200)
18/02/14 13:24:38 INFO TaskSetManager: Starting task 15.0 in stage 1.2 (TID 645, hadoop-slave5, executor 7, partition 34, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:38 INFO TaskSetManager: Finished task 25.1 in stage 3.0 (TID 614) in 6677 ms on hadoop-slave5 (executor 7) (84/200)
18/02/14 13:24:38 INFO TaskSetManager: Starting task 16.0 in stage 1.2 (TID 646, hadoop-slave2, executor 6, partition 35, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:38 INFO TaskSetManager: Finished task 70.1 in stage 3.0 (TID 624) in 4310 ms on hadoop-slave2 (executor 6) (85/200)
18/02/14 13:24:39 INFO TaskSetManager: Starting task 18.0 in stage 1.2 (TID 647, hadoop-slave5, executor 7, partition 38, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:39 INFO TaskSetManager: Finished task 88.1 in stage 3.0 (TID 615) in 6964 ms on hadoop-slave5 (executor 7) (86/200)
18/02/14 13:24:40 INFO TaskSetManager: Starting task 21.0 in stage 1.2 (TID 648, hadoop-slave3, executor 5, partition 45, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:40 INFO TaskSetManager: Finished task 47.1 in stage 3.0 (TID 616) in 8393 ms on hadoop-slave3 (executor 5) (87/200)
18/02/14 13:24:41 INFO TaskSetManager: Starting task 22.0 in stage 1.2 (TID 649, hadoop-slave1, executor 3, partition 49, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:41 INFO TaskSetManager: Finished task 29.1 in stage 3.0 (TID 626) in 6394 ms on hadoop-slave1 (executor 3) (88/200)
18/02/14 13:24:41 INFO TaskSetManager: Starting task 23.0 in stage 1.2 (TID 650, hadoop-slave3, executor 5, partition 50, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:41 INFO TaskSetManager: Finished task 8.0 in stage 1.2 (TID 631) in 5367 ms on hadoop-slave3 (executor 5) (1/103)
18/02/14 13:24:42 INFO TaskSetManager: Starting task 24.0 in stage 1.2 (TID 651, hadoop-slave5, executor 7, partition 51, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:42 INFO TaskSetManager: Finished task 84.1 in stage 3.0 (TID 625) in 8098 ms on hadoop-slave5 (executor 7) (89/200)
18/02/14 13:24:43 INFO TaskSetManager: Starting task 25.0 in stage 1.2 (TID 652, hadoop-slave2, executor 6, partition 53, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:43 INFO TaskSetManager: Finished task 3.0 in stage 1.2 (TID 630) in 7530 ms on hadoop-slave2 (executor 6) (2/103)
18/02/14 13:24:43 INFO TaskSetManager: Starting task 26.0 in stage 1.2 (TID 653, hadoop-slave3, executor 5, partition 54, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:43 INFO TaskSetManager: Finished task 9.0 in stage 1.2 (TID 635) in 6960 ms on hadoop-slave3 (executor 5) (3/103)
18/02/14 13:24:44 INFO TaskSetManager: Starting task 27.0 in stage 1.2 (TID 654, hadoop-slave2, executor 6, partition 57, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:44 INFO TaskSetManager: Finished task 6.0 in stage 1.2 (TID 634) in 8344 ms on hadoop-slave2 (executor 6) (4/103)
18/02/14 13:24:46 INFO TaskSetManager: Starting task 29.0 in stage 1.2 (TID 655, hadoop-slave2, executor 6, partition 61, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:46 INFO TaskSetManager: Finished task 16.0 in stage 1.2 (TID 646) in 7555 ms on hadoop-slave2 (executor 6) (5/103)
18/02/14 13:24:46 INFO TaskSetManager: Starting task 28.0 in stage 1.2 (TID 656, hadoop-slave3, executor 5, partition 58, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:46 INFO TaskSetManager: Finished task 20.0 in stage 1.2 (TID 644) in 8050 ms on hadoop-slave3 (executor 5) (6/103)
18/02/14 13:24:46 INFO TaskSetManager: Starting task 31.0 in stage 1.2 (TID 657, hadoop-slave2, executor 6, partition 65, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:46 INFO TaskSetManager: Finished task 11.0 in stage 1.2 (TID 638) in 10018 ms on hadoop-slave2 (executor 6) (7/103)
18/02/14 13:24:47 INFO TaskSetManager: Starting task 30.0 in stage 1.2 (TID 658, hadoop-slave3, executor 5, partition 64, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:47 INFO TaskSetManager: Finished task 12.0 in stage 1.2 (TID 639) in 10413 ms on hadoop-slave3 (executor 5) (8/103)
18/02/14 13:24:47 INFO TaskSetManager: Starting task 32.0 in stage 1.2 (TID 659, hadoop-slave2, executor 6, partition 67, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:47 INFO TaskSetManager: Finished task 14.0 in stage 1.2 (TID 641) in 10191 ms on hadoop-slave2 (executor 6) (9/103)
18/02/14 13:24:49 INFO TaskSetManager: Starting task 33.0 in stage 1.2 (TID 660, hadoop-slave2, executor 6, partition 69, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:49 INFO TaskSetManager: Finished task 4.0 in stage 1.2 (TID 632) in 13017 ms on hadoop-slave2 (executor 6) (10/103)
18/02/14 13:24:50 INFO TaskSetManager: Starting task 34.0 in stage 1.2 (TID 661, hadoop-slave3, executor 5, partition 70, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:50 INFO TaskSetManager: Finished task 21.0 in stage 1.2 (TID 648) in 9441 ms on hadoop-slave3 (executor 5) (11/103)
18/02/14 13:24:52 INFO TaskSetManager: Starting task 35.0 in stage 1.2 (TID 662, hadoop-slave3, executor 5, partition 71, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:52 INFO TaskSetManager: Finished task 19.0 in stage 1.2 (TID 643) in 14611 ms on hadoop-slave3 (executor 5) (12/103)
18/02/14 13:24:53 INFO TaskSetManager: Starting task 36.0 in stage 1.2 (TID 663, hadoop-slave3, executor 5, partition 72, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:53 INFO TaskSetManager: Finished task 28.0 in stage 1.2 (TID 656) in 7157 ms on hadoop-slave3 (executor 5) (13/103)
18/02/14 13:24:53 INFO TaskSetManager: Starting task 37.0 in stage 1.2 (TID 664, hadoop-slave5, executor 7, partition 73, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:53 INFO TaskSetManager: Finished task 5.0 in stage 1.2 (TID 633) in 17677 ms on hadoop-slave5 (executor 7) (14/103)
18/02/14 13:24:54 INFO TaskSetManager: Starting task 43.0 in stage 1.2 (TID 665, hadoop-slave3, executor 5, partition 90, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:54 INFO TaskSetManager: Finished task 26.0 in stage 1.2 (TID 653) in 10770 ms on hadoop-slave3 (executor 5) (15/103)
18/02/14 13:24:54 INFO TaskSetManager: Starting task 39.0 in stage 1.2 (TID 666, hadoop-slave5, executor 7, partition 78, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:54 INFO TaskSetManager: Finished task 10.0 in stage 1.2 (TID 637) in 17986 ms on hadoop-slave5 (executor 7) (16/103)
18/02/14 13:24:55 INFO TaskSetManager: Starting task 45.0 in stage 1.2 (TID 667, hadoop-slave3, executor 5, partition 93, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:55 INFO TaskSetManager: Finished task 23.0 in stage 1.2 (TID 650) in 13787 ms on hadoop-slave3 (executor 5) (17/103)
18/02/14 13:24:56 INFO TaskSetManager: Starting task 40.0 in stage 1.2 (TID 668, hadoop-slave5, executor 7, partition 80, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:56 INFO TaskSetManager: Finished task 2.0 in stage 1.2 (TID 628) in 21354 ms on hadoop-slave5 (executor 7) (18/103)
18/02/14 13:24:57 INFO TaskSetManager: Starting task 38.0 in stage 1.2 (TID 669, hadoop-slave2, executor 6, partition 77, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:57 INFO TaskSetManager: Finished task 25.0 in stage 1.2 (TID 652) in 14305 ms on hadoop-slave2 (executor 6) (19/103)
18/02/14 13:24:57 INFO TaskSetManager: Starting task 46.0 in stage 1.2 (TID 670, hadoop-slave5, executor 7, partition 94, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:57 INFO TaskSetManager: Finished task 24.0 in stage 1.2 (TID 651) in 14766 ms on hadoop-slave5 (executor 7) (20/103)
18/02/14 13:24:57 INFO TaskSetManager: Starting task 47.0 in stage 1.2 (TID 671, hadoop-slave3, executor 5, partition 95, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:57 INFO TaskSetManager: Finished task 35.0 in stage 1.2 (TID 662) in 4740 ms on hadoop-slave3 (executor 5) (21/103)
18/02/14 13:24:57 INFO TaskSetManager: Starting task 41.0 in stage 1.2 (TID 672, hadoop-slave2, executor 6, partition 87, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:57 INFO TaskSetManager: Finished task 27.0 in stage 1.2 (TID 654) in 12849 ms on hadoop-slave2 (executor 6) (22/103)
18/02/14 13:24:58 INFO TaskSetManager: Starting task 53.0 in stage 1.2 (TID 673, hadoop-slave3, executor 5, partition 113, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:58 INFO TaskSetManager: Finished task 30.0 in stage 1.2 (TID 658) in 10478 ms on hadoop-slave3 (executor 5) (23/103)
18/02/14 13:24:58 INFO TaskSetManager: Starting task 42.0 in stage 1.2 (TID 674, hadoop-slave2, executor 6, partition 89, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:58 INFO TaskSetManager: Finished task 33.0 in stage 1.2 (TID 660) in 9228 ms on hadoop-slave2 (executor 6) (24/103)
18/02/14 13:24:58 INFO TaskSetManager: Starting task 48.0 in stage 1.2 (TID 675, hadoop-slave5, executor 7, partition 96, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:58 INFO TaskSetManager: Finished task 18.0 in stage 1.2 (TID 647) in 19484 ms on hadoop-slave5 (executor 7) (25/103)
18/02/14 13:24:58 INFO TaskSetManager: Starting task 44.0 in stage 1.2 (TID 676, hadoop-slave2, executor 6, partition 91, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:58 INFO TaskSetManager: Finished task 29.0 in stage 1.2 (TID 655) in 12367 ms on hadoop-slave2 (executor 6) (26/103)
18/02/14 13:24:59 INFO TaskSetManager: Starting task 49.0 in stage 1.2 (TID 677, hadoop-slave5, executor 7, partition 100, NODE_LOCAL, 6941 bytes)
18/02/14 13:24:59 INFO TaskSetManager: Finished task 15.0 in stage 1.2 (TID 645) in 20809 ms on hadoop-slave5 (executor 7) (27/103)
18/02/14 13:25:00 INFO TaskSetManager: Starting task 50.0 in stage 1.2 (TID 678, hadoop-slave1, executor 3, partition 105, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:00 INFO TaskSetManager: Finished task 0.0 in stage 1.2 (TID 627) in 25158 ms on hadoop-slave1 (executor 3) (28/103)
18/02/14 13:25:01 INFO TaskSetManager: Starting task 54.0 in stage 1.2 (TID 679, hadoop-slave1, executor 3, partition 114, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:01 INFO TaskSetManager: Finished task 13.0 in stage 1.2 (TID 640) in 24577 ms on hadoop-slave1 (executor 3) (29/103)
18/02/14 13:25:03 INFO TaskSetManager: Starting task 58.0 in stage 1.2 (TID 680, hadoop-slave1, executor 3, partition 123, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:03 INFO TaskSetManager: Finished task 1.0 in stage 1.2 (TID 629) in 27854 ms on hadoop-slave1 (executor 3) (30/103)
18/02/14 13:25:03 INFO TaskSetManager: Starting task 51.0 in stage 1.2 (TID 681, hadoop-slave2, executor 6, partition 106, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:03 INFO TaskSetManager: Finished task 31.0 in stage 1.2 (TID 657) in 16309 ms on hadoop-slave2 (executor 6) (31/103)
18/02/14 13:25:03 INFO TaskSetManager: Starting task 61.0 in stage 1.2 (TID 682, hadoop-slave1, executor 3, partition 130, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:03 INFO TaskSetManager: Finished task 7.0 in stage 1.2 (TID 636) in 27141 ms on hadoop-slave1 (executor 3) (32/103)
18/02/14 13:25:03 INFO TaskSetManager: Starting task 55.0 in stage 1.2 (TID 683, hadoop-slave3, executor 5, partition 116, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:03 INFO TaskSetManager: Finished task 53.0 in stage 1.2 (TID 673) in 5869 ms on hadoop-slave3 (executor 5) (33/103)
18/02/14 13:25:04 INFO TaskSetManager: Starting task 56.0 in stage 1.2 (TID 684, hadoop-slave3, executor 5, partition 117, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:04 INFO TaskSetManager: Finished task 45.0 in stage 1.2 (TID 667) in 8941 ms on hadoop-slave3 (executor 5) (34/103)
18/02/14 13:25:05 INFO TaskSetManager: Starting task 64.0 in stage 1.2 (TID 685, hadoop-slave1, executor 3, partition 136, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:05 INFO TaskSetManager: Finished task 17.0 in stage 1.2 (TID 642) in 27181 ms on hadoop-slave1 (executor 3) (35/103)
18/02/14 13:25:05 INFO TaskSetManager: Starting task 59.0 in stage 1.2 (TID 686, hadoop-slave3, executor 5, partition 125, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:05 INFO TaskSetManager: Finished task 34.0 in stage 1.2 (TID 661) in 15100 ms on hadoop-slave3 (executor 5) (36/103)
18/02/14 13:25:05 INFO TaskSetManager: Finished task 36.0 in stage 1.2 (TID 663) in 11680 ms on hadoop-slave3 (executor 5) (37/103)
18/02/14 13:25:05 INFO TaskSetManager: Starting task 66.0 in stage 1.2 (TID 687, hadoop-slave3, executor 5, partition 139, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:05 INFO TaskSetManager: Starting task 70.0 in stage 1.2 (TID 688, hadoop-slave1, executor 3, partition 145, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:05 INFO TaskSetManager: Finished task 22.0 in stage 1.2 (TID 649) in 24441 ms on hadoop-slave1 (executor 3) (38/103)
18/02/14 13:25:06 INFO TaskSetManager: Starting task 52.0 in stage 1.2 (TID 689, hadoop-slave2, executor 6, partition 108, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:06 INFO TaskSetManager: Finished task 32.0 in stage 1.2 (TID 659) in 19237 ms on hadoop-slave2 (executor 6) (39/103)
18/02/14 13:25:06 INFO TaskSetManager: Starting task 57.0 in stage 1.2 (TID 690, hadoop-slave2, executor 6, partition 120, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:06 INFO TaskSetManager: Finished task 42.0 in stage 1.2 (TID 674) in 8553 ms on hadoop-slave2 (executor 6) (40/103)
18/02/14 13:25:07 INFO TaskSetManager: Starting task 60.0 in stage 1.2 (TID 691, hadoop-slave5, executor 7, partition 128, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:07 INFO TaskSetManager: Finished task 37.0 in stage 1.2 (TID 664) in 13763 ms on hadoop-slave5 (executor 7) (41/103)
18/02/14 13:25:07 INFO TaskSetManager: Starting task 62.0 in stage 1.2 (TID 692, hadoop-slave5, executor 7, partition 134, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:07 INFO TaskSetManager: Finished task 46.0 in stage 1.2 (TID 670) in 10592 ms on hadoop-slave5 (executor 7) (42/103)
18/02/14 13:25:10 INFO TaskSetManager: Starting task 63.0 in stage 1.2 (TID 693, hadoop-slave2, executor 6, partition 135, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:10 INFO TaskSetManager: Finished task 41.0 in stage 1.2 (TID 672) in 12841 ms on hadoop-slave2 (executor 6) (43/103)
18/02/14 13:25:11 INFO TaskSetManager: Starting task 65.0 in stage 1.2 (TID 694, hadoop-slave2, executor 6, partition 138, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:11 INFO TaskSetManager: Finished task 44.0 in stage 1.2 (TID 676) in 12929 ms on hadoop-slave2 (executor 6) (44/103)
18/02/14 13:25:11 INFO TaskSetManager: Starting task 67.0 in stage 1.2 (TID 695, hadoop-slave2, executor 6, partition 140, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:11 INFO TaskSetManager: Finished task 57.0 in stage 1.2 (TID 690) in 4753 ms on hadoop-slave2 (executor 6) (45/103)
18/02/14 13:25:11 INFO TaskSetManager: Starting task 68.0 in stage 1.2 (TID 696, hadoop-slave2, executor 6, partition 141, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:11 INFO TaskSetManager: Finished task 52.0 in stage 1.2 (TID 689) in 4994 ms on hadoop-slave2 (executor 6) (46/103)
18/02/14 13:25:11 INFO TaskSetManager: Starting task 69.0 in stage 1.2 (TID 697, hadoop-slave2, executor 6, partition 144, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:11 INFO TaskSetManager: Finished task 38.0 in stage 1.2 (TID 669) in 14677 ms on hadoop-slave2 (executor 6) (47/103)
18/02/14 13:25:12 INFO TaskSetManager: Starting task 71.0 in stage 1.2 (TID 698, hadoop-slave5, executor 7, partition 146, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:12 INFO TaskSetManager: Finished task 49.0 in stage 1.2 (TID 677) in 13105 ms on hadoop-slave5 (executor 7) (48/103)
18/02/14 13:25:13 INFO TaskSetManager: Starting task 74.0 in stage 1.2 (TID 699, hadoop-slave2, executor 6, partition 150, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:13 INFO TaskSetManager: Finished task 51.0 in stage 1.2 (TID 681) in 9815 ms on hadoop-slave2 (executor 6) (49/103)
18/02/14 13:25:14 INFO TaskSetManager: Starting task 72.0 in stage 1.2 (TID 700, hadoop-slave5, executor 7, partition 148, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:14 INFO TaskSetManager: Finished task 48.0 in stage 1.2 (TID 675) in 15951 ms on hadoop-slave5 (executor 7) (50/103)
18/02/14 13:25:14 INFO TaskSetManager: Starting task 73.0 in stage 1.2 (TID 701, hadoop-slave5, executor 7, partition 149, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:14 INFO TaskSetManager: Finished task 60.0 in stage 1.2 (TID 691) in 7076 ms on hadoop-slave5 (executor 7) (51/103)
18/02/14 13:25:14 INFO TaskSetManager: Starting task 77.0 in stage 1.2 (TID 702, hadoop-slave2, executor 6, partition 153, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:14 INFO TaskSetManager: Finished task 63.0 in stage 1.2 (TID 693) in 4512 ms on hadoop-slave2 (executor 6) (52/103)
18/02/14 13:25:15 INFO TaskSetManager: Starting task 75.0 in stage 1.2 (TID 703, hadoop-slave5, executor 7, partition 151, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:15 INFO TaskSetManager: Finished task 39.0 in stage 1.2 (TID 666) in 21359 ms on hadoop-slave5 (executor 7) (53/103)
18/02/14 13:25:17 INFO TaskSetManager: Starting task 76.0 in stage 1.2 (TID 704, hadoop-slave1, executor 3, partition 152, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:17 INFO TaskSetManager: Finished task 61.0 in stage 1.2 (TID 682) in 14188 ms on hadoop-slave1 (executor 3) (54/103)
18/02/14 13:25:17 INFO TaskSetManager: Starting task 79.0 in stage 1.2 (TID 705, hadoop-slave1, executor 3, partition 155, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:17 INFO TaskSetManager: Finished task 64.0 in stage 1.2 (TID 685) in 12755 ms on hadoop-slave1 (executor 3) (55/103)
18/02/14 13:25:17 INFO TaskSetManager: Starting task 78.0 in stage 1.2 (TID 706, hadoop-slave2, executor 6, partition 154, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:17 INFO TaskSetManager: Finished task 67.0 in stage 1.2 (TID 695) in 6192 ms on hadoop-slave2 (executor 6) (56/103)
18/02/14 13:25:18 INFO TaskSetManager: Starting task 80.0 in stage 1.2 (TID 707, hadoop-slave1, executor 3, partition 156, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:18 INFO TaskSetManager: Finished task 50.0 in stage 1.2 (TID 678) in 18802 ms on hadoop-slave1 (executor 3) (57/103)
18/02/14 13:25:19 INFO TaskSetManager: Starting task 81.0 in stage 1.2 (TID 708, hadoop-slave2, executor 6, partition 161, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:19 INFO TaskSetManager: Finished task 77.0 in stage 1.2 (TID 702) in 4501 ms on hadoop-slave2 (executor 6) (58/103)
18/02/14 13:25:19 INFO TaskSetManager: Starting task 82.0 in stage 1.2 (TID 709, hadoop-slave3, executor 5, partition 165, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:19 INFO TaskSetManager: Finished task 43.0 in stage 1.2 (TID 665) in 25280 ms on hadoop-slave3 (executor 5) (59/103)
18/02/14 13:25:20 INFO TaskSetManager: Starting task 83.0 in stage 1.2 (TID 710, hadoop-slave5, executor 7, partition 167, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:20 INFO TaskSetManager: Finished task 62.0 in stage 1.2 (TID 692) in 12284 ms on hadoop-slave5 (executor 7) (60/103)
18/02/14 13:25:20 INFO TaskSetManager: Starting task 84.0 in stage 1.2 (TID 711, hadoop-slave5, executor 7, partition 168, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:20 INFO TaskSetManager: Finished task 40.0 in stage 1.2 (TID 668) in 24164 ms on hadoop-slave5 (executor 7) (61/103)
18/02/14 13:25:21 INFO TaskSetManager: Starting task 89.0 in stage 1.2 (TID 712, hadoop-slave1, executor 3, partition 176, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:21 INFO TaskSetManager: Finished task 54.0 in stage 1.2 (TID 679) in 19483 ms on hadoop-slave1 (executor 3) (62/103)
18/02/14 13:25:22 INFO TaskSetManager: Starting task 85.0 in stage 1.2 (TID 713, hadoop-slave2, executor 6, partition 169, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:22 INFO TaskSetManager: Finished task 74.0 in stage 1.2 (TID 699) in 8956 ms on hadoop-slave2 (executor 6) (63/103)
18/02/14 13:25:23 INFO TaskSetManager: Starting task 86.0 in stage 1.2 (TID 714, hadoop-slave2, executor 6, partition 170, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:23 INFO TaskSetManager: Finished task 68.0 in stage 1.2 (TID 696) in 11166 ms on hadoop-slave2 (executor 6) (64/103)
18/02/14 13:25:23 INFO TaskSetManager: Starting task 87.0 in stage 1.2 (TID 715, hadoop-slave2, executor 6, partition 171, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:23 INFO TaskSetManager: Finished task 65.0 in stage 1.2 (TID 694) in 12077 ms on hadoop-slave2 (executor 6) (65/103)
18/02/14 13:25:24 INFO TaskSetManager: Starting task 96.0 in stage 1.2 (TID 716, hadoop-slave3, executor 5, partition 188, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:24 INFO TaskSetManager: Finished task 55.0 in stage 1.2 (TID 683) in 20102 ms on hadoop-slave3 (executor 5) (66/103)
18/02/14 13:25:24 INFO TaskSetManager: Starting task 98.0 in stage 1.2 (TID 717, hadoop-slave3, executor 5, partition 190, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:24 INFO TaskSetManager: Finished task 66.0 in stage 1.2 (TID 687) in 19141 ms on hadoop-slave3 (executor 5) (67/103)
18/02/14 13:25:24 INFO TaskSetManager: Starting task 88.0 in stage 1.2 (TID 718, hadoop-slave5, executor 7, partition 173, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:24 INFO TaskSetManager: Finished task 72.0 in stage 1.2 (TID 700) in 10029 ms on hadoop-slave5 (executor 7) (68/103)
18/02/14 13:25:24 INFO TaskSetManager: Starting task 100.0 in stage 1.2 (TID 719, hadoop-slave3, executor 5, partition 192, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:24 INFO TaskSetManager: Finished task 47.0 in stage 1.2 (TID 671) in 27130 ms on hadoop-slave3 (executor 5) (69/103)
18/02/14 13:25:24 INFO TaskSetManager: Starting task 102.0 in stage 1.2 (TID 720, hadoop-slave3, executor 5, partition 199, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:24 INFO TaskSetManager: Finished task 59.0 in stage 1.2 (TID 686) in 19469 ms on hadoop-slave3 (executor 5) (70/103)
18/02/14 13:25:24 INFO TaskSetManager: Starting task 90.0 in stage 1.2 (TID 721, hadoop-slave1, executor 3, partition 177, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:24 INFO TaskSetManager: Finished task 58.0 in stage 1.2 (TID 680) in 21568 ms on hadoop-slave1 (executor 3) (71/103)
18/02/14 13:25:25 INFO TaskSetManager: Starting task 39.1 in stage 3.0 (TID 722, hadoop-slave3, executor 5, partition 39, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:25 INFO TaskSetManager: Finished task 56.0 in stage 1.2 (TID 684) in 20988 ms on hadoop-slave3 (executor 5) (72/103)
18/02/14 13:25:27 INFO TaskSetManager: Starting task 92.0 in stage 1.2 (TID 723, hadoop-slave2, executor 6, partition 179, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:27 INFO TaskSetManager: Finished task 81.0 in stage 1.2 (TID 708) in 7990 ms on hadoop-slave2 (executor 6) (73/103)
18/02/14 13:25:27 INFO TaskSetManager: Starting task 93.0 in stage 1.2 (TID 724, hadoop-slave2, executor 6, partition 180, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:27 INFO TaskSetManager: Finished task 85.0 in stage 1.2 (TID 713) in 5347 ms on hadoop-slave2 (executor 6) (74/103)
18/02/14 13:25:29 INFO TaskSetManager: Starting task 91.0 in stage 1.2 (TID 725, hadoop-slave1, executor 3, partition 178, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:29 INFO TaskSetManager: Finished task 70.0 in stage 1.2 (TID 688) in 23987 ms on hadoop-slave1 (executor 3) (75/103)
18/02/14 13:25:30 INFO TaskSetManager: Starting task 101.0 in stage 1.2 (TID 726, hadoop-slave5, executor 7, partition 196, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:30 INFO TaskSetManager: Finished task 75.0 in stage 1.2 (TID 703) in 14623 ms on hadoop-slave5 (executor 7) (76/103)
18/02/14 13:25:31 INFO TaskSetManager: Starting task 94.0 in stage 1.2 (TID 727, hadoop-slave2, executor 6, partition 182, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:31 INFO TaskSetManager: Finished task 86.0 in stage 1.2 (TID 714) in 8384 ms on hadoop-slave2 (executor 6) (77/103)
18/02/14 13:25:31 INFO TaskSetManager: Starting task 95.0 in stage 1.2 (TID 728, hadoop-slave2, executor 6, partition 187, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:31 INFO TaskSetManager: Finished task 78.0 in stage 1.2 (TID 706) in 13576 ms on hadoop-slave2 (executor 6) (78/103)
18/02/14 13:25:31 INFO TaskSetManager: Starting task 97.0 in stage 1.2 (TID 729, hadoop-slave1, executor 3, partition 189, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:31 INFO TaskSetManager: Finished task 79.0 in stage 1.2 (TID 705) in 14010 ms on hadoop-slave1 (executor 3) (79/103)
18/02/14 13:25:32 INFO TaskSetManager: Starting task 99.0 in stage 1.2 (TID 730, hadoop-slave1, executor 3, partition 191, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:32 INFO TaskSetManager: Finished task 80.0 in stage 1.2 (TID 707) in 13760 ms on hadoop-slave1 (executor 3) (80/103)
18/02/14 13:25:32 INFO TaskSetManager: Starting task 42.1 in stage 3.0 (TID 731, hadoop-slave3, executor 5, partition 42, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:32 INFO TaskSetManager: Finished task 82.0 in stage 1.2 (TID 709) in 13497 ms on hadoop-slave3 (executor 5) (81/103)
18/02/14 13:25:33 INFO TaskSetManager: Starting task 40.1 in stage 3.0 (TID 732, hadoop-slave3, executor 5, partition 40, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:33 INFO TaskSetManager: Finished task 39.1 in stage 3.0 (TID 722) in 8253 ms on hadoop-slave3 (executor 5) (90/200)
18/02/14 13:25:33 INFO TaskSetManager: Starting task 11.1 in stage 3.0 (TID 733, hadoop-slave3, executor 5, partition 11, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:33 INFO TaskSetManager: Finished task 98.0 in stage 1.2 (TID 717) in 9306 ms on hadoop-slave3 (executor 5) (82/103)
18/02/14 13:25:33 INFO TaskSetManager: Starting task 57.1 in stage 3.0 (TID 734, hadoop-slave3, executor 5, partition 57, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:33 INFO TaskSetManager: Finished task 100.0 in stage 1.2 (TID 719) in 9311 ms on hadoop-slave3 (executor 5) (83/103)
18/02/14 13:25:33 INFO TaskSetManager: Starting task 67.1 in stage 3.0 (TID 735, hadoop-slave2, executor 6, partition 67, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:33 INFO TaskSetManager: Finished task 69.0 in stage 1.2 (TID 697) in 21942 ms on hadoop-slave2 (executor 6) (84/103)
18/02/14 13:25:34 INFO TaskSetManager: Starting task 95.0 in stage 3.0 (TID 736, hadoop-slave1, executor 3, partition 95, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:34 INFO TaskSetManager: Finished task 76.0 in stage 1.2 (TID 704) in 16471 ms on hadoop-slave1 (executor 3) (85/103)
18/02/14 13:25:35 INFO TaskSetManager: Starting task 97.0 in stage 3.0 (TID 737, hadoop-slave3, executor 5, partition 97, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:35 INFO TaskSetManager: Finished task 96.0 in stage 1.2 (TID 716) in 11292 ms on hadoop-slave3 (executor 5) (86/103)
18/02/14 13:25:36 INFO TaskSetManager: Starting task 96.0 in stage 3.0 (TID 738, hadoop-slave5, executor 7, partition 96, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:36 INFO TaskSetManager: Finished task 71.0 in stage 1.2 (TID 698) in 23892 ms on hadoop-slave5 (executor 7) (87/103)
18/02/14 13:25:37 INFO TaskSetManager: Starting task 99.0 in stage 3.0 (TID 739, hadoop-slave3, executor 5, partition 99, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:37 INFO TaskSetManager: Finished task 102.0 in stage 1.2 (TID 720) in 13257 ms on hadoop-slave3 (executor 5) (88/103)
18/02/14 13:25:38 INFO TaskSetManager: Starting task 100.0 in stage 3.0 (TID 740, hadoop-slave5, executor 7, partition 100, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:38 INFO TaskSetManager: Finished task 84.0 in stage 1.2 (TID 711) in 17753 ms on hadoop-slave5 (executor 7) (89/103)
18/02/14 13:25:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-slave5:35799 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:25:38 INFO TaskSetManager: Starting task 102.0 in stage 3.0 (TID 741, hadoop-slave3, executor 5, partition 102, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:38 INFO TaskSetManager: Finished task 40.1 in stage 3.0 (TID 732) in 5672 ms on hadoop-slave3 (executor 5) (91/200)
18/02/14 13:25:38 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-slave3:33203 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:25:39 INFO TaskSetManager: Starting task 101.0 in stage 3.0 (TID 742, hadoop-slave5, executor 7, partition 101, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:39 INFO TaskSetManager: Finished task 73.0 in stage 1.2 (TID 701) in 24654 ms on hadoop-slave5 (executor 7) (90/103)
18/02/14 13:25:40 INFO TaskSetManager: Starting task 103.0 in stage 3.0 (TID 743, hadoop-slave3, executor 5, partition 103, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:40 INFO TaskSetManager: Finished task 42.1 in stage 3.0 (TID 731) in 7656 ms on hadoop-slave3 (executor 5) (92/200)
18/02/14 13:25:40 INFO TaskSetManager: Starting task 106.0 in stage 3.0 (TID 744, hadoop-slave5, executor 7, partition 106, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:40 INFO TaskSetManager: Finished task 83.0 in stage 1.2 (TID 710) in 20458 ms on hadoop-slave5 (executor 7) (91/103)
18/02/14 13:25:41 INFO TaskSetManager: Starting task 104.0 in stage 3.0 (TID 745, hadoop-slave3, executor 5, partition 104, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:41 INFO TaskSetManager: Finished task 11.1 in stage 3.0 (TID 733) in 7387 ms on hadoop-slave3 (executor 5) (93/200)
18/02/14 13:25:41 INFO TaskSetManager: Starting task 107.0 in stage 3.0 (TID 746, hadoop-slave3, executor 5, partition 107, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:41 INFO TaskSetManager: Finished task 57.1 in stage 3.0 (TID 734) in 7828 ms on hadoop-slave3 (executor 5) (94/200)
18/02/14 13:25:41 INFO TaskSetManager: Starting task 98.0 in stage 3.0 (TID 747, hadoop-slave1, executor 3, partition 98, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:41 INFO TaskSetManager: Finished task 95.0 in stage 3.0 (TID 736) in 7608 ms on hadoop-slave1 (executor 3) (95/200)
18/02/14 13:25:42 INFO TaskSetManager: Starting task 105.0 in stage 3.0 (TID 748, hadoop-slave2, executor 6, partition 105, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:42 INFO TaskSetManager: Finished task 93.0 in stage 1.2 (TID 724) in 15550 ms on hadoop-slave2 (executor 6) (92/103)
18/02/14 13:25:42 INFO TaskSetManager: Starting task 108.0 in stage 3.0 (TID 749, hadoop-slave5, executor 7, partition 108, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:42 INFO TaskSetManager: Finished task 96.0 in stage 3.0 (TID 738) in 6516 ms on hadoop-slave5 (executor 7) (96/200)
18/02/14 13:25:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-slave2:33289 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:25:43 INFO TaskSetManager: Starting task 111.0 in stage 3.0 (TID 750, hadoop-slave3, executor 5, partition 111, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:43 INFO TaskSetManager: Finished task 99.0 in stage 3.0 (TID 739) in 5547 ms on hadoop-slave3 (executor 5) (97/200)
18/02/14 13:25:43 INFO TaskSetManager: Starting task 109.0 in stage 3.0 (TID 751, hadoop-slave2, executor 6, partition 109, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:43 INFO TaskSetManager: Finished task 95.0 in stage 1.2 (TID 728) in 12008 ms on hadoop-slave2 (executor 6) (93/103)
18/02/14 13:25:43 INFO TaskSetManager: Starting task 110.0 in stage 3.0 (TID 752, hadoop-slave2, executor 6, partition 110, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:43 INFO TaskSetManager: Finished task 67.1 in stage 3.0 (TID 735) in 9559 ms on hadoop-slave2 (executor 6) (98/200)
18/02/14 13:25:43 INFO TaskSetManager: Starting task 113.0 in stage 3.0 (TID 753, hadoop-slave3, executor 5, partition 113, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:43 INFO TaskSetManager: Finished task 97.0 in stage 3.0 (TID 737) in 8593 ms on hadoop-slave3 (executor 5) (99/200)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 112.0 in stage 3.0 (TID 754, hadoop-slave1, executor 3, partition 112, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 90.0 in stage 1.2 (TID 721) in 20173 ms on hadoop-slave1 (executor 3) (94/103)
18/02/14 13:25:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-slave1:46007 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 114.0 in stage 3.0 (TID 755, hadoop-slave5, executor 7, partition 114, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 100.0 in stage 3.0 (TID 740) in 6792 ms on hadoop-slave5 (executor 7) (100/200)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 115.0 in stage 3.0 (TID 756, hadoop-slave3, executor 5, partition 115, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 102.0 in stage 3.0 (TID 741) in 6376 ms on hadoop-slave3 (executor 5) (101/200)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 116.0 in stage 3.0 (TID 757, hadoop-slave5, executor 7, partition 116, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 88.0 in stage 1.2 (TID 718) in 20877 ms on hadoop-slave5 (executor 7) (95/103)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 118.0 in stage 3.0 (TID 758, hadoop-slave1, executor 3, partition 118, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 91.0 in stage 1.2 (TID 725) in 16036 ms on hadoop-slave1 (executor 3) (96/103)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 117.0 in stage 3.0 (TID 759, hadoop-slave2, executor 6, partition 117, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 87.0 in stage 1.2 (TID 715) in 22066 ms on hadoop-slave2 (executor 6) (97/103)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 121.0 in stage 3.0 (TID 760, hadoop-slave1, executor 3, partition 121, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 89.0 in stage 1.2 (TID 712) in 24495 ms on hadoop-slave1 (executor 3) (98/103)
18/02/14 13:25:45 INFO TaskSetManager: Starting task 120.0 in stage 3.0 (TID 761, hadoop-slave2, executor 6, partition 120, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:45 INFO TaskSetManager: Finished task 94.0 in stage 1.2 (TID 727) in 14457 ms on hadoop-slave2 (executor 6) (99/103)
18/02/14 13:25:46 INFO TaskSetManager: Starting task 123.0 in stage 3.0 (TID 762, hadoop-slave1, executor 3, partition 123, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:46 INFO TaskSetManager: Finished task 97.0 in stage 1.2 (TID 729) in 14254 ms on hadoop-slave1 (executor 3) (100/103)
18/02/14 13:25:46 INFO TaskSetManager: Starting task 126.0 in stage 3.0 (TID 763, hadoop-slave1, executor 3, partition 126, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:46 INFO TaskSetManager: Finished task 99.0 in stage 1.2 (TID 730) in 13445 ms on hadoop-slave1 (executor 3) (101/103)
18/02/14 13:25:46 INFO TaskSetManager: Starting task 119.0 in stage 3.0 (TID 764, hadoop-slave3, executor 5, partition 119, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:46 INFO TaskSetManager: Finished task 103.0 in stage 3.0 (TID 743) in 6314 ms on hadoop-slave3 (executor 5) (102/200)
18/02/14 13:25:47 INFO TaskSetManager: Starting task 122.0 in stage 3.0 (TID 765, hadoop-slave3, executor 5, partition 122, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:47 INFO TaskSetManager: Finished task 104.0 in stage 3.0 (TID 745) in 6333 ms on hadoop-slave3 (executor 5) (103/200)
18/02/14 13:25:48 INFO TaskSetManager: Starting task 124.0 in stage 3.0 (TID 766, hadoop-slave5, executor 7, partition 124, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:48 INFO TaskSetManager: Finished task 101.0 in stage 3.0 (TID 742) in 8669 ms on hadoop-slave5 (executor 7) (104/200)
18/02/14 13:25:48 INFO TaskSetManager: Starting task 127.0 in stage 3.0 (TID 767, hadoop-slave2, executor 6, partition 127, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:48 INFO TaskSetManager: Finished task 92.0 in stage 1.2 (TID 723) in 20619 ms on hadoop-slave2 (executor 6) (102/103)
18/02/14 13:25:48 INFO TaskSetManager: Starting task 125.0 in stage 3.0 (TID 768, hadoop-slave5, executor 7, partition 125, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:48 INFO TaskSetManager: Finished task 106.0 in stage 3.0 (TID 744) in 7629 ms on hadoop-slave5 (executor 7) (105/200)
18/02/14 13:25:48 INFO TaskSetManager: Starting task 129.0 in stage 3.0 (TID 769, hadoop-slave3, executor 5, partition 129, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:48 INFO TaskSetManager: Finished task 111.0 in stage 3.0 (TID 750) in 5221 ms on hadoop-slave3 (executor 5) (106/200)
18/02/14 13:25:48 INFO TaskSetManager: Starting task 131.0 in stage 3.0 (TID 770, hadoop-slave3, executor 5, partition 131, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:48 INFO TaskSetManager: Finished task 113.0 in stage 3.0 (TID 753) in 5032 ms on hadoop-slave3 (executor 5) (107/200)
18/02/14 13:25:49 INFO TaskSetManager: Starting task 128.0 in stage 3.0 (TID 771, hadoop-slave2, executor 6, partition 128, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:49 INFO TaskSetManager: Finished task 105.0 in stage 3.0 (TID 748) in 6616 ms on hadoop-slave2 (executor 6) (108/200)
18/02/14 13:25:49 INFO TaskSetManager: Starting task 130.0 in stage 3.0 (TID 772, hadoop-slave1, executor 3, partition 130, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:49 INFO TaskSetManager: Finished task 98.0 in stage 3.0 (TID 747) in 7720 ms on hadoop-slave1 (executor 3) (109/200)
18/02/14 13:25:50 INFO TaskSetManager: Starting task 132.0 in stage 3.0 (TID 773, hadoop-slave3, executor 5, partition 132, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:50 INFO TaskSetManager: Finished task 107.0 in stage 3.0 (TID 746) in 8322 ms on hadoop-slave3 (executor 5) (110/200)
18/02/14 13:25:50 INFO TaskSetManager: Starting task 133.0 in stage 3.0 (TID 774, hadoop-slave2, executor 6, partition 133, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:50 INFO TaskSetManager: Finished task 109.0 in stage 3.0 (TID 751) in 6857 ms on hadoop-slave2 (executor 6) (111/200)
18/02/14 13:25:50 INFO TaskSetManager: Starting task 134.0 in stage 3.0 (TID 775, hadoop-slave5, executor 7, partition 134, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:50 INFO TaskSetManager: Finished task 108.0 in stage 3.0 (TID 749) in 7536 ms on hadoop-slave5 (executor 7) (112/200)
18/02/14 13:25:51 INFO TaskSetManager: Starting task 135.0 in stage 3.0 (TID 776, hadoop-slave2, executor 6, partition 135, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:51 INFO TaskSetManager: Finished task 110.0 in stage 3.0 (TID 752) in 8081 ms on hadoop-slave2 (executor 6) (113/200)
18/02/14 13:25:52 INFO TaskSetManager: Starting task 137.0 in stage 3.0 (TID 777, hadoop-slave3, executor 5, partition 137, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 119.0 in stage 3.0 (TID 764) in 5063 ms on hadoop-slave3 (executor 5) (114/200)
18/02/14 13:25:52 INFO TaskSetManager: Starting task 136.0 in stage 3.0 (TID 778, hadoop-slave1, executor 3, partition 136, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 118.0 in stage 3.0 (TID 758) in 6636 ms on hadoop-slave1 (executor 3) (115/200)
18/02/14 13:25:52 INFO TaskSetManager: Starting task 139.0 in stage 3.0 (TID 779, hadoop-slave3, executor 5, partition 139, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 115.0 in stage 3.0 (TID 756) in 7000 ms on hadoop-slave3 (executor 5) (116/200)
18/02/14 13:25:52 INFO TaskSetManager: Starting task 138.0 in stage 3.0 (TID 780, hadoop-slave5, executor 7, partition 138, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 101.0 in stage 1.2 (TID 726) in 21977 ms on hadoop-slave5 (executor 7) (103/103)
18/02/14 13:25:52 INFO YarnScheduler: Removed TaskSet 1.2, whose tasks have all completed, from pool 
18/02/14 13:25:52 INFO DAGScheduler: ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) finished in 77.701 s
18/02/14 13:25:52 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:25:52 INFO DAGScheduler: running: Set(ShuffleMapStage 3)
18/02/14 13:25:52 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:25:52 INFO DAGScheduler: failed: Set()
18/02/14 13:25:52 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:25:52 INFO TaskSetManager: Starting task 140.0 in stage 3.0 (TID 781, hadoop-slave2, executor 6, partition 140, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 117.0 in stage 3.0 (TID 759) in 6692 ms on hadoop-slave2 (executor 6) (117/200)
18/02/14 13:25:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 29.8 KB, free 4.1 GB)
18/02/14 13:25:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 12.0 KB, free 4.1 GB)
18/02/14 13:25:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.19.3.36:33535 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:25:52 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:996
18/02/14 13:25:52 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74)
18/02/14 13:25:52 INFO YarnScheduler: Adding task set 2.1 with 200 tasks
18/02/14 13:25:52 INFO TaskSetManager: Starting task 5.0 in stage 2.1 (TID 782, hadoop-slave1, executor 3, partition 5, NODE_LOCAL, 6201 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 123.0 in stage 3.0 (TID 762) in 6423 ms on hadoop-slave1 (executor 3) (118/200)
18/02/14 13:25:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on hadoop-slave1:46007 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:25:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.35:59084
18/02/14 13:25:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 597 bytes
18/02/14 13:25:52 INFO TaskSetManager: Starting task 9.0 in stage 2.1 (TID 783, hadoop-slave1, executor 3, partition 9, NODE_LOCAL, 6201 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 126.0 in stage 3.0 (TID 763) in 6454 ms on hadoop-slave1 (executor 3) (119/200)
18/02/14 13:25:52 INFO TaskSetManager: Starting task 0.0 in stage 2.1 (TID 784, hadoop-slave2, executor 6, partition 0, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 120.0 in stage 3.0 (TID 761) in 6736 ms on hadoop-slave2 (executor 6) (120/200)
18/02/14 13:25:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.35:59084
18/02/14 13:25:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 822 bytes
18/02/14 13:25:52 INFO TaskSetManager: Starting task 16.0 in stage 2.1 (TID 785, hadoop-slave1, executor 3, partition 16, NODE_LOCAL, 6201 bytes)
18/02/14 13:25:52 INFO TaskSetManager: Finished task 112.0 in stage 3.0 (TID 754) in 7713 ms on hadoop-slave1 (executor 3) (121/200)
18/02/14 13:25:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on hadoop-slave2:33289 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:25:53 INFO TaskSetManager: Starting task 1.0 in stage 2.1 (TID 786, hadoop-slave5, executor 7, partition 1, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:53 INFO TaskSetManager: Finished task 116.0 in stage 3.0 (TID 757) in 8440 ms on hadoop-slave5 (executor 7) (122/200)
18/02/14 13:25:54 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on hadoop-slave5:35799 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:25:54 INFO TaskSetManager: Starting task 2.0 in stage 2.1 (TID 787, hadoop-slave2, executor 6, partition 2, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:54 INFO TaskSetManager: Finished task 127.0 in stage 3.0 (TID 767) in 6169 ms on hadoop-slave2 (executor 6) (123/200)
18/02/14 13:25:54 INFO TaskSetManager: Starting task 18.0 in stage 2.1 (TID 788, hadoop-slave1, executor 3, partition 18, NODE_LOCAL, 6201 bytes)
18/02/14 13:25:54 INFO TaskSetManager: Finished task 130.0 in stage 3.0 (TID 772) in 4794 ms on hadoop-slave1 (executor 3) (124/200)
18/02/14 13:25:54 INFO TaskSetManager: Starting task 3.0 in stage 2.1 (TID 789, hadoop-slave3, executor 5, partition 3, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:54 INFO TaskSetManager: Finished task 122.0 in stage 3.0 (TID 765) in 7405 ms on hadoop-slave3 (executor 5) (125/200)
18/02/14 13:25:54 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on hadoop-slave3:33203 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:25:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.33:51972
18/02/14 13:25:55 INFO TaskSetManager: Starting task 4.0 in stage 2.1 (TID 790, hadoop-slave5, executor 7, partition 4, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:55 INFO TaskSetManager: Finished task 124.0 in stage 3.0 (TID 766) in 6972 ms on hadoop-slave5 (executor 7) (126/200)
18/02/14 13:25:55 INFO TaskSetManager: Starting task 6.0 in stage 2.1 (TID 791, hadoop-slave5, executor 7, partition 6, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:55 INFO TaskSetManager: Finished task 125.0 in stage 3.0 (TID 768) in 7061 ms on hadoop-slave5 (executor 7) (127/200)
18/02/14 13:25:55 INFO TaskSetManager: Starting task 7.0 in stage 2.1 (TID 792, hadoop-slave3, executor 5, partition 7, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:55 INFO TaskSetManager: Finished task 129.0 in stage 3.0 (TID 769) in 6753 ms on hadoop-slave3 (executor 5) (128/200)
18/02/14 13:25:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.31:55688
18/02/14 13:25:55 INFO TaskSetManager: Starting task 8.0 in stage 2.1 (TID 793, hadoop-slave3, executor 5, partition 8, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:55 INFO TaskSetManager: Finished task 131.0 in stage 3.0 (TID 770) in 6754 ms on hadoop-slave3 (executor 5) (129/200)
18/02/14 13:25:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.33:51972
18/02/14 13:25:56 INFO TaskSetManager: Starting task 10.0 in stage 2.1 (TID 794, hadoop-slave3, executor 5, partition 10, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 7.0 in stage 2.1 (TID 792) in 652 ms on hadoop-slave3 (executor 5) (1/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 11.0 in stage 2.1 (TID 795, hadoop-slave3, executor 5, partition 11, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 8.0 in stage 2.1 (TID 793) in 354 ms on hadoop-slave3 (executor 5) (2/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 12.0 in stage 2.1 (TID 796, hadoop-slave3, executor 5, partition 12, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 3.0 in stage 2.1 (TID 789) in 1050 ms on hadoop-slave3 (executor 5) (3/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 13.0 in stage 2.1 (TID 797, hadoop-slave3, executor 5, partition 13, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 10.0 in stage 2.1 (TID 794) in 11 ms on hadoop-slave3 (executor 5) (4/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 14.0 in stage 2.1 (TID 798, hadoop-slave3, executor 5, partition 14, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 11.0 in stage 2.1 (TID 795) in 9 ms on hadoop-slave3 (executor 5) (5/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 15.0 in stage 2.1 (TID 799, hadoop-slave3, executor 5, partition 15, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 12.0 in stage 2.1 (TID 796) in 8 ms on hadoop-slave3 (executor 5) (6/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 17.0 in stage 2.1 (TID 800, hadoop-slave3, executor 5, partition 17, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 13.0 in stage 2.1 (TID 797) in 7 ms on hadoop-slave3 (executor 5) (7/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 19.0 in stage 2.1 (TID 801, hadoop-slave3, executor 5, partition 19, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 14.0 in stage 2.1 (TID 798) in 7 ms on hadoop-slave3 (executor 5) (8/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 20.0 in stage 2.1 (TID 802, hadoop-slave3, executor 5, partition 20, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 15.0 in stage 2.1 (TID 799) in 7 ms on hadoop-slave3 (executor 5) (9/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 21.0 in stage 2.1 (TID 803, hadoop-slave3, executor 5, partition 21, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 19.0 in stage 2.1 (TID 801) in 7 ms on hadoop-slave3 (executor 5) (10/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 22.0 in stage 2.1 (TID 804, hadoop-slave3, executor 5, partition 22, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 17.0 in stage 2.1 (TID 800) in 8 ms on hadoop-slave3 (executor 5) (11/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 23.0 in stage 2.1 (TID 805, hadoop-slave3, executor 5, partition 23, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 20.0 in stage 2.1 (TID 802) in 7 ms on hadoop-slave3 (executor 5) (12/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 24.0 in stage 2.1 (TID 806, hadoop-slave3, executor 5, partition 24, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 21.0 in stage 2.1 (TID 803) in 7 ms on hadoop-slave3 (executor 5) (13/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 25.0 in stage 2.1 (TID 807, hadoop-slave3, executor 5, partition 25, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 22.0 in stage 2.1 (TID 804) in 7 ms on hadoop-slave3 (executor 5) (14/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 26.0 in stage 2.1 (TID 808, hadoop-slave3, executor 5, partition 26, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 23.0 in stage 2.1 (TID 805) in 7 ms on hadoop-slave3 (executor 5) (15/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 27.0 in stage 2.1 (TID 809, hadoop-slave3, executor 5, partition 27, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 24.0 in stage 2.1 (TID 806) in 9 ms on hadoop-slave3 (executor 5) (16/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 28.0 in stage 2.1 (TID 810, hadoop-slave3, executor 5, partition 28, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 26.0 in stage 2.1 (TID 808) in 7 ms on hadoop-slave3 (executor 5) (17/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 29.0 in stage 2.1 (TID 811, hadoop-slave3, executor 5, partition 29, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 25.0 in stage 2.1 (TID 807) in 12 ms on hadoop-slave3 (executor 5) (18/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 30.0 in stage 2.1 (TID 812, hadoop-slave3, executor 5, partition 30, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 27.0 in stage 2.1 (TID 809) in 8 ms on hadoop-slave3 (executor 5) (19/200)
18/02/14 13:25:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.31:55688
18/02/14 13:25:56 INFO TaskSetManager: Starting task 31.0 in stage 2.1 (TID 813, hadoop-slave3, executor 5, partition 31, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 28.0 in stage 2.1 (TID 810) in 11 ms on hadoop-slave3 (executor 5) (20/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 32.0 in stage 2.1 (TID 814, hadoop-slave3, executor 5, partition 32, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 29.0 in stage 2.1 (TID 811) in 9 ms on hadoop-slave3 (executor 5) (21/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 33.0 in stage 2.1 (TID 815, hadoop-slave3, executor 5, partition 33, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 30.0 in stage 2.1 (TID 812) in 9 ms on hadoop-slave3 (executor 5) (22/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 36.0 in stage 2.1 (TID 816, hadoop-slave3, executor 5, partition 36, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 31.0 in stage 2.1 (TID 813) in 8 ms on hadoop-slave3 (executor 5) (23/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 37.0 in stage 2.1 (TID 817, hadoop-slave3, executor 5, partition 37, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 32.0 in stage 2.1 (TID 814) in 7 ms on hadoop-slave3 (executor 5) (24/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 38.0 in stage 2.1 (TID 818, hadoop-slave3, executor 5, partition 38, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 33.0 in stage 2.1 (TID 815) in 8 ms on hadoop-slave3 (executor 5) (25/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 39.0 in stage 2.1 (TID 819, hadoop-slave3, executor 5, partition 39, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 36.0 in stage 2.1 (TID 816) in 7 ms on hadoop-slave3 (executor 5) (26/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 40.0 in stage 2.1 (TID 820, hadoop-slave3, executor 5, partition 40, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 37.0 in stage 2.1 (TID 817) in 7 ms on hadoop-slave3 (executor 5) (27/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 41.0 in stage 2.1 (TID 821, hadoop-slave3, executor 5, partition 41, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 38.0 in stage 2.1 (TID 818) in 9 ms on hadoop-slave3 (executor 5) (28/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 42.0 in stage 2.1 (TID 822, hadoop-slave3, executor 5, partition 42, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 39.0 in stage 2.1 (TID 819) in 7 ms on hadoop-slave3 (executor 5) (29/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 43.0 in stage 2.1 (TID 823, hadoop-slave3, executor 5, partition 43, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 40.0 in stage 2.1 (TID 820) in 13 ms on hadoop-slave3 (executor 5) (30/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 44.0 in stage 2.1 (TID 824, hadoop-slave3, executor 5, partition 44, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 41.0 in stage 2.1 (TID 821) in 7 ms on hadoop-slave3 (executor 5) (31/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 45.0 in stage 2.1 (TID 825, hadoop-slave3, executor 5, partition 45, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 42.0 in stage 2.1 (TID 822) in 14 ms on hadoop-slave3 (executor 5) (32/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 46.0 in stage 2.1 (TID 826, hadoop-slave3, executor 5, partition 46, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 43.0 in stage 2.1 (TID 823) in 9 ms on hadoop-slave3 (executor 5) (33/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 47.0 in stage 2.1 (TID 827, hadoop-slave3, executor 5, partition 47, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 44.0 in stage 2.1 (TID 824) in 9 ms on hadoop-slave3 (executor 5) (34/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 48.0 in stage 2.1 (TID 828, hadoop-slave3, executor 5, partition 48, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 45.0 in stage 2.1 (TID 825) in 8 ms on hadoop-slave3 (executor 5) (35/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 49.0 in stage 2.1 (TID 829, hadoop-slave3, executor 5, partition 49, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 46.0 in stage 2.1 (TID 826) in 8 ms on hadoop-slave3 (executor 5) (36/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 50.0 in stage 2.1 (TID 830, hadoop-slave3, executor 5, partition 50, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 47.0 in stage 2.1 (TID 827) in 11 ms on hadoop-slave3 (executor 5) (37/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 51.0 in stage 2.1 (TID 831, hadoop-slave3, executor 5, partition 51, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 48.0 in stage 2.1 (TID 828) in 8 ms on hadoop-slave3 (executor 5) (38/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 52.0 in stage 2.1 (TID 832, hadoop-slave3, executor 5, partition 52, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 49.0 in stage 2.1 (TID 829) in 7 ms on hadoop-slave3 (executor 5) (39/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 53.0 in stage 2.1 (TID 833, hadoop-slave3, executor 5, partition 53, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 50.0 in stage 2.1 (TID 830) in 8 ms on hadoop-slave3 (executor 5) (40/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 34.0 in stage 2.1 (TID 834, hadoop-slave1, executor 3, partition 34, NODE_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 136.0 in stage 3.0 (TID 778) in 3897 ms on hadoop-slave1 (executor 3) (130/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 54.0 in stage 2.1 (TID 835, hadoop-slave3, executor 5, partition 54, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 52.0 in stage 2.1 (TID 832) in 16 ms on hadoop-slave3 (executor 5) (41/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 55.0 in stage 2.1 (TID 836, hadoop-slave3, executor 5, partition 55, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 53.0 in stage 2.1 (TID 833) in 13 ms on hadoop-slave3 (executor 5) (42/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 56.0 in stage 2.1 (TID 837, hadoop-slave3, executor 5, partition 56, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 51.0 in stage 2.1 (TID 831) in 18 ms on hadoop-slave3 (executor 5) (43/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 58.0 in stage 2.1 (TID 838, hadoop-slave3, executor 5, partition 58, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 54.0 in stage 2.1 (TID 835) in 9 ms on hadoop-slave3 (executor 5) (44/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 59.0 in stage 2.1 (TID 839, hadoop-slave3, executor 5, partition 59, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 55.0 in stage 2.1 (TID 836) in 9 ms on hadoop-slave3 (executor 5) (45/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 60.0 in stage 2.1 (TID 840, hadoop-slave3, executor 5, partition 60, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 56.0 in stage 2.1 (TID 837) in 8 ms on hadoop-slave3 (executor 5) (46/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 61.0 in stage 2.1 (TID 841, hadoop-slave3, executor 5, partition 61, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 58.0 in stage 2.1 (TID 838) in 7 ms on hadoop-slave3 (executor 5) (47/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 62.0 in stage 2.1 (TID 842, hadoop-slave3, executor 5, partition 62, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 59.0 in stage 2.1 (TID 839) in 7 ms on hadoop-slave3 (executor 5) (48/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 63.0 in stage 2.1 (TID 843, hadoop-slave3, executor 5, partition 63, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 60.0 in stage 2.1 (TID 840) in 9 ms on hadoop-slave3 (executor 5) (49/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 64.0 in stage 2.1 (TID 844, hadoop-slave3, executor 5, partition 64, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 61.0 in stage 2.1 (TID 841) in 8 ms on hadoop-slave3 (executor 5) (50/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 65.0 in stage 2.1 (TID 845, hadoop-slave3, executor 5, partition 65, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 63.0 in stage 2.1 (TID 843) in 8 ms on hadoop-slave3 (executor 5) (51/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 67.0 in stage 2.1 (TID 846, hadoop-slave3, executor 5, partition 67, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 62.0 in stage 2.1 (TID 842) in 10 ms on hadoop-slave3 (executor 5) (52/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 68.0 in stage 2.1 (TID 847, hadoop-slave3, executor 5, partition 68, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 64.0 in stage 2.1 (TID 844) in 7 ms on hadoop-slave3 (executor 5) (53/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 69.0 in stage 2.1 (TID 848, hadoop-slave3, executor 5, partition 69, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 65.0 in stage 2.1 (TID 845) in 8 ms on hadoop-slave3 (executor 5) (54/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 70.0 in stage 2.1 (TID 849, hadoop-slave3, executor 5, partition 70, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 67.0 in stage 2.1 (TID 846) in 8 ms on hadoop-slave3 (executor 5) (55/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 71.0 in stage 2.1 (TID 850, hadoop-slave3, executor 5, partition 71, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 68.0 in stage 2.1 (TID 847) in 7 ms on hadoop-slave3 (executor 5) (56/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 72.0 in stage 2.1 (TID 851, hadoop-slave3, executor 5, partition 72, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 70.0 in stage 2.1 (TID 849) in 7 ms on hadoop-slave3 (executor 5) (57/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 73.0 in stage 2.1 (TID 852, hadoop-slave3, executor 5, partition 73, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 69.0 in stage 2.1 (TID 848) in 8 ms on hadoop-slave3 (executor 5) (58/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 74.0 in stage 2.1 (TID 853, hadoop-slave3, executor 5, partition 74, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 71.0 in stage 2.1 (TID 850) in 8 ms on hadoop-slave3 (executor 5) (59/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 75.0 in stage 2.1 (TID 854, hadoop-slave3, executor 5, partition 75, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 72.0 in stage 2.1 (TID 851) in 8 ms on hadoop-slave3 (executor 5) (60/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 76.0 in stage 2.1 (TID 855, hadoop-slave3, executor 5, partition 76, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 73.0 in stage 2.1 (TID 852) in 7 ms on hadoop-slave3 (executor 5) (61/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 78.0 in stage 2.1 (TID 856, hadoop-slave3, executor 5, partition 78, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 74.0 in stage 2.1 (TID 853) in 7 ms on hadoop-slave3 (executor 5) (62/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 79.0 in stage 2.1 (TID 857, hadoop-slave3, executor 5, partition 79, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 75.0 in stage 2.1 (TID 854) in 8 ms on hadoop-slave3 (executor 5) (63/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 80.0 in stage 2.1 (TID 858, hadoop-slave3, executor 5, partition 80, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 76.0 in stage 2.1 (TID 855) in 7 ms on hadoop-slave3 (executor 5) (64/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 83.0 in stage 2.1 (TID 859, hadoop-slave3, executor 5, partition 83, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 80.0 in stage 2.1 (TID 858) in 7 ms on hadoop-slave3 (executor 5) (65/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 84.0 in stage 2.1 (TID 860, hadoop-slave3, executor 5, partition 84, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 78.0 in stage 2.1 (TID 856) in 8 ms on hadoop-slave3 (executor 5) (66/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 85.0 in stage 2.1 (TID 861, hadoop-slave3, executor 5, partition 85, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 79.0 in stage 2.1 (TID 857) in 13 ms on hadoop-slave3 (executor 5) (67/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 87.0 in stage 2.1 (TID 862, hadoop-slave3, executor 5, partition 87, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 83.0 in stage 2.1 (TID 859) in 8 ms on hadoop-slave3 (executor 5) (68/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 88.0 in stage 2.1 (TID 863, hadoop-slave3, executor 5, partition 88, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 84.0 in stage 2.1 (TID 860) in 10 ms on hadoop-slave3 (executor 5) (69/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 90.0 in stage 2.1 (TID 864, hadoop-slave3, executor 5, partition 90, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 85.0 in stage 2.1 (TID 861) in 9 ms on hadoop-slave3 (executor 5) (70/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 91.0 in stage 2.1 (TID 865, hadoop-slave3, executor 5, partition 91, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 87.0 in stage 2.1 (TID 862) in 7 ms on hadoop-slave3 (executor 5) (71/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 93.0 in stage 2.1 (TID 866, hadoop-slave3, executor 5, partition 93, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 90.0 in stage 2.1 (TID 864) in 8 ms on hadoop-slave3 (executor 5) (72/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 94.0 in stage 2.1 (TID 867, hadoop-slave3, executor 5, partition 94, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 88.0 in stage 2.1 (TID 863) in 10 ms on hadoop-slave3 (executor 5) (73/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 96.0 in stage 2.1 (TID 868, hadoop-slave3, executor 5, partition 96, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 91.0 in stage 2.1 (TID 865) in 7 ms on hadoop-slave3 (executor 5) (74/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 97.0 in stage 2.1 (TID 869, hadoop-slave3, executor 5, partition 97, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 94.0 in stage 2.1 (TID 867) in 7 ms on hadoop-slave3 (executor 5) (75/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 98.0 in stage 2.1 (TID 870, hadoop-slave3, executor 5, partition 98, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 93.0 in stage 2.1 (TID 866) in 8 ms on hadoop-slave3 (executor 5) (76/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 99.0 in stage 2.1 (TID 871, hadoop-slave3, executor 5, partition 99, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 96.0 in stage 2.1 (TID 868) in 8 ms on hadoop-slave3 (executor 5) (77/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 102.0 in stage 2.1 (TID 872, hadoop-slave3, executor 5, partition 102, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 98.0 in stage 2.1 (TID 870) in 8 ms on hadoop-slave3 (executor 5) (78/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 103.0 in stage 2.1 (TID 873, hadoop-slave3, executor 5, partition 103, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 97.0 in stage 2.1 (TID 869) in 8 ms on hadoop-slave3 (executor 5) (79/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 104.0 in stage 2.1 (TID 874, hadoop-slave3, executor 5, partition 104, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 99.0 in stage 2.1 (TID 871) in 12 ms on hadoop-slave3 (executor 5) (80/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 105.0 in stage 2.1 (TID 875, hadoop-slave3, executor 5, partition 105, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 102.0 in stage 2.1 (TID 872) in 8 ms on hadoop-slave3 (executor 5) (81/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 106.0 in stage 2.1 (TID 876, hadoop-slave3, executor 5, partition 106, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 103.0 in stage 2.1 (TID 873) in 10 ms on hadoop-slave3 (executor 5) (82/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 107.0 in stage 2.1 (TID 877, hadoop-slave3, executor 5, partition 107, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 105.0 in stage 2.1 (TID 875) in 9 ms on hadoop-slave3 (executor 5) (83/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 108.0 in stage 2.1 (TID 878, hadoop-slave3, executor 5, partition 108, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 104.0 in stage 2.1 (TID 874) in 10 ms on hadoop-slave3 (executor 5) (84/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 109.0 in stage 2.1 (TID 879, hadoop-slave3, executor 5, partition 109, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 106.0 in stage 2.1 (TID 876) in 8 ms on hadoop-slave3 (executor 5) (85/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 110.0 in stage 2.1 (TID 880, hadoop-slave3, executor 5, partition 110, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 108.0 in stage 2.1 (TID 878) in 7 ms on hadoop-slave3 (executor 5) (86/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 111.0 in stage 2.1 (TID 881, hadoop-slave3, executor 5, partition 111, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 107.0 in stage 2.1 (TID 877) in 8 ms on hadoop-slave3 (executor 5) (87/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 112.0 in stage 2.1 (TID 882, hadoop-slave3, executor 5, partition 112, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 109.0 in stage 2.1 (TID 879) in 12 ms on hadoop-slave3 (executor 5) (88/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 113.0 in stage 2.1 (TID 883, hadoop-slave3, executor 5, partition 113, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 110.0 in stage 2.1 (TID 880) in 8 ms on hadoop-slave3 (executor 5) (89/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 114.0 in stage 2.1 (TID 884, hadoop-slave3, executor 5, partition 114, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 111.0 in stage 2.1 (TID 881) in 7 ms on hadoop-slave3 (executor 5) (90/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 115.0 in stage 2.1 (TID 885, hadoop-slave3, executor 5, partition 115, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 112.0 in stage 2.1 (TID 882) in 7 ms on hadoop-slave3 (executor 5) (91/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 116.0 in stage 2.1 (TID 886, hadoop-slave3, executor 5, partition 116, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 114.0 in stage 2.1 (TID 884) in 6 ms on hadoop-slave3 (executor 5) (92/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 117.0 in stage 2.1 (TID 887, hadoop-slave3, executor 5, partition 117, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 113.0 in stage 2.1 (TID 883) in 7 ms on hadoop-slave3 (executor 5) (93/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 118.0 in stage 2.1 (TID 888, hadoop-slave3, executor 5, partition 118, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 116.0 in stage 2.1 (TID 886) in 8 ms on hadoop-slave3 (executor 5) (94/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 119.0 in stage 2.1 (TID 889, hadoop-slave3, executor 5, partition 119, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 117.0 in stage 2.1 (TID 887) in 8 ms on hadoop-slave3 (executor 5) (95/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 120.0 in stage 2.1 (TID 890, hadoop-slave3, executor 5, partition 120, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 115.0 in stage 2.1 (TID 885) in 10 ms on hadoop-slave3 (executor 5) (96/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 121.0 in stage 2.1 (TID 891, hadoop-slave3, executor 5, partition 121, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 118.0 in stage 2.1 (TID 888) in 11 ms on hadoop-slave3 (executor 5) (97/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 122.0 in stage 2.1 (TID 892, hadoop-slave3, executor 5, partition 122, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 119.0 in stage 2.1 (TID 889) in 10 ms on hadoop-slave3 (executor 5) (98/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 123.0 in stage 2.1 (TID 893, hadoop-slave3, executor 5, partition 123, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 120.0 in stage 2.1 (TID 890) in 10 ms on hadoop-slave3 (executor 5) (99/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 124.0 in stage 2.1 (TID 894, hadoop-slave3, executor 5, partition 124, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 123.0 in stage 2.1 (TID 893) in 8 ms on hadoop-slave3 (executor 5) (100/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 125.0 in stage 2.1 (TID 895, hadoop-slave3, executor 5, partition 125, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 121.0 in stage 2.1 (TID 891) in 9 ms on hadoop-slave3 (executor 5) (101/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 127.0 in stage 2.1 (TID 896, hadoop-slave3, executor 5, partition 127, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 122.0 in stage 2.1 (TID 892) in 9 ms on hadoop-slave3 (executor 5) (102/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 128.0 in stage 2.1 (TID 897, hadoop-slave3, executor 5, partition 128, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 124.0 in stage 2.1 (TID 894) in 6 ms on hadoop-slave3 (executor 5) (103/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 130.0 in stage 2.1 (TID 898, hadoop-slave3, executor 5, partition 130, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 125.0 in stage 2.1 (TID 895) in 6 ms on hadoop-slave3 (executor 5) (104/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 131.0 in stage 2.1 (TID 899, hadoop-slave3, executor 5, partition 131, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 127.0 in stage 2.1 (TID 896) in 12 ms on hadoop-slave3 (executor 5) (105/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 132.0 in stage 2.1 (TID 900, hadoop-slave3, executor 5, partition 132, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 128.0 in stage 2.1 (TID 897) in 7 ms on hadoop-slave3 (executor 5) (106/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 133.0 in stage 2.1 (TID 901, hadoop-slave3, executor 5, partition 133, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 130.0 in stage 2.1 (TID 898) in 8 ms on hadoop-slave3 (executor 5) (107/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 134.0 in stage 2.1 (TID 902, hadoop-slave3, executor 5, partition 134, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 131.0 in stage 2.1 (TID 899) in 7 ms on hadoop-slave3 (executor 5) (108/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 135.0 in stage 2.1 (TID 903, hadoop-slave3, executor 5, partition 135, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 133.0 in stage 2.1 (TID 901) in 7 ms on hadoop-slave3 (executor 5) (109/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 136.0 in stage 2.1 (TID 904, hadoop-slave3, executor 5, partition 136, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 132.0 in stage 2.1 (TID 900) in 11 ms on hadoop-slave3 (executor 5) (110/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 137.0 in stage 2.1 (TID 905, hadoop-slave3, executor 5, partition 137, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 134.0 in stage 2.1 (TID 902) in 7 ms on hadoop-slave3 (executor 5) (111/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 138.0 in stage 2.1 (TID 906, hadoop-slave3, executor 5, partition 138, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 135.0 in stage 2.1 (TID 903) in 9 ms on hadoop-slave3 (executor 5) (112/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 139.0 in stage 2.1 (TID 907, hadoop-slave3, executor 5, partition 139, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 136.0 in stage 2.1 (TID 904) in 8 ms on hadoop-slave3 (executor 5) (113/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 140.0 in stage 2.1 (TID 908, hadoop-slave3, executor 5, partition 140, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 137.0 in stage 2.1 (TID 905) in 7 ms on hadoop-slave3 (executor 5) (114/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 141.0 in stage 2.1 (TID 909, hadoop-slave3, executor 5, partition 141, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 138.0 in stage 2.1 (TID 906) in 13 ms on hadoop-slave3 (executor 5) (115/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 143.0 in stage 2.1 (TID 910, hadoop-slave3, executor 5, partition 143, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 140.0 in stage 2.1 (TID 908) in 7 ms on hadoop-slave3 (executor 5) (116/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 144.0 in stage 2.1 (TID 911, hadoop-slave3, executor 5, partition 144, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 139.0 in stage 2.1 (TID 907) in 10 ms on hadoop-slave3 (executor 5) (117/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 145.0 in stage 2.1 (TID 912, hadoop-slave3, executor 5, partition 145, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 141.0 in stage 2.1 (TID 909) in 8 ms on hadoop-slave3 (executor 5) (118/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 146.0 in stage 2.1 (TID 913, hadoop-slave3, executor 5, partition 146, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 144.0 in stage 2.1 (TID 911) in 8 ms on hadoop-slave3 (executor 5) (119/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 147.0 in stage 2.1 (TID 914, hadoop-slave3, executor 5, partition 147, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 143.0 in stage 2.1 (TID 910) in 11 ms on hadoop-slave3 (executor 5) (120/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 148.0 in stage 2.1 (TID 915, hadoop-slave3, executor 5, partition 148, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 145.0 in stage 2.1 (TID 912) in 8 ms on hadoop-slave3 (executor 5) (121/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 149.0 in stage 2.1 (TID 916, hadoop-slave3, executor 5, partition 149, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 146.0 in stage 2.1 (TID 913) in 7 ms on hadoop-slave3 (executor 5) (122/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 150.0 in stage 2.1 (TID 917, hadoop-slave3, executor 5, partition 150, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 147.0 in stage 2.1 (TID 914) in 8 ms on hadoop-slave3 (executor 5) (123/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 151.0 in stage 2.1 (TID 918, hadoop-slave3, executor 5, partition 151, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 148.0 in stage 2.1 (TID 915) in 10 ms on hadoop-slave3 (executor 5) (124/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 152.0 in stage 2.1 (TID 919, hadoop-slave3, executor 5, partition 152, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 149.0 in stage 2.1 (TID 916) in 9 ms on hadoop-slave3 (executor 5) (125/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 155.0 in stage 2.1 (TID 920, hadoop-slave3, executor 5, partition 155, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 150.0 in stage 2.1 (TID 917) in 11 ms on hadoop-slave3 (executor 5) (126/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 156.0 in stage 2.1 (TID 921, hadoop-slave5, executor 7, partition 156, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 6.0 in stage 2.1 (TID 791) in 1012 ms on hadoop-slave5 (executor 7) (127/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 157.0 in stage 2.1 (TID 922, hadoop-slave5, executor 7, partition 157, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 4.0 in stage 2.1 (TID 790) in 1369 ms on hadoop-slave5 (executor 7) (128/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 158.0 in stage 2.1 (TID 923, hadoop-slave5, executor 7, partition 158, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 1.0 in stage 2.1 (TID 786) in 2496 ms on hadoop-slave5 (executor 7) (129/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 159.0 in stage 2.1 (TID 924, hadoop-slave3, executor 5, partition 159, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 152.0 in stage 2.1 (TID 919) in 14 ms on hadoop-slave3 (executor 5) (130/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 160.0 in stage 2.1 (TID 925, hadoop-slave3, executor 5, partition 160, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 155.0 in stage 2.1 (TID 920) in 10 ms on hadoop-slave3 (executor 5) (131/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 161.0 in stage 2.1 (TID 926, hadoop-slave3, executor 5, partition 161, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 151.0 in stage 2.1 (TID 918) in 15 ms on hadoop-slave3 (executor 5) (132/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 162.0 in stage 2.1 (TID 927, hadoop-slave5, executor 7, partition 162, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 157.0 in stage 2.1 (TID 922) in 8 ms on hadoop-slave5 (executor 7) (133/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 163.0 in stage 2.1 (TID 928, hadoop-slave5, executor 7, partition 163, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 156.0 in stage 2.1 (TID 921) in 9 ms on hadoop-slave5 (executor 7) (134/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 164.0 in stage 2.1 (TID 929, hadoop-slave5, executor 7, partition 164, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 158.0 in stage 2.1 (TID 923) in 9 ms on hadoop-slave5 (executor 7) (135/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 166.0 in stage 2.1 (TID 930, hadoop-slave5, executor 7, partition 166, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 163.0 in stage 2.1 (TID 928) in 6 ms on hadoop-slave5 (executor 7) (136/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 167.0 in stage 2.1 (TID 931, hadoop-slave5, executor 7, partition 167, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 162.0 in stage 2.1 (TID 927) in 7 ms on hadoop-slave5 (executor 7) (137/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 168.0 in stage 2.1 (TID 932, hadoop-slave5, executor 7, partition 168, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 164.0 in stage 2.1 (TID 929) in 7 ms on hadoop-slave5 (executor 7) (138/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 170.0 in stage 2.1 (TID 933, hadoop-slave3, executor 5, partition 170, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 159.0 in stage 2.1 (TID 924) in 11 ms on hadoop-slave3 (executor 5) (139/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 171.0 in stage 2.1 (TID 934, hadoop-slave3, executor 5, partition 171, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 161.0 in stage 2.1 (TID 926) in 10 ms on hadoop-slave3 (executor 5) (140/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 172.0 in stage 2.1 (TID 935, hadoop-slave3, executor 5, partition 172, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 160.0 in stage 2.1 (TID 925) in 11 ms on hadoop-slave3 (executor 5) (141/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 173.0 in stage 2.1 (TID 936, hadoop-slave5, executor 7, partition 173, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 167.0 in stage 2.1 (TID 931) in 8 ms on hadoop-slave5 (executor 7) (142/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 175.0 in stage 2.1 (TID 937, hadoop-slave5, executor 7, partition 175, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 166.0 in stage 2.1 (TID 930) in 9 ms on hadoop-slave5 (executor 7) (143/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 176.0 in stage 2.1 (TID 938, hadoop-slave5, executor 7, partition 176, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 168.0 in stage 2.1 (TID 932) in 8 ms on hadoop-slave5 (executor 7) (144/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 177.0 in stage 2.1 (TID 939, hadoop-slave3, executor 5, partition 177, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 170.0 in stage 2.1 (TID 933) in 8 ms on hadoop-slave3 (executor 5) (145/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 178.0 in stage 2.1 (TID 940, hadoop-slave3, executor 5, partition 178, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 171.0 in stage 2.1 (TID 934) in 8 ms on hadoop-slave3 (executor 5) (146/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 179.0 in stage 2.1 (TID 941, hadoop-slave3, executor 5, partition 179, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 172.0 in stage 2.1 (TID 935) in 10 ms on hadoop-slave3 (executor 5) (147/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 180.0 in stage 2.1 (TID 942, hadoop-slave5, executor 7, partition 180, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 176.0 in stage 2.1 (TID 938) in 7 ms on hadoop-slave5 (executor 7) (148/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 181.0 in stage 2.1 (TID 943, hadoop-slave5, executor 7, partition 181, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 175.0 in stage 2.1 (TID 937) in 8 ms on hadoop-slave5 (executor 7) (149/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 182.0 in stage 2.1 (TID 944, hadoop-slave5, executor 7, partition 182, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 173.0 in stage 2.1 (TID 936) in 9 ms on hadoop-slave5 (executor 7) (150/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 183.0 in stage 2.1 (TID 945, hadoop-slave3, executor 5, partition 183, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 178.0 in stage 2.1 (TID 940) in 7 ms on hadoop-slave3 (executor 5) (151/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 185.0 in stage 2.1 (TID 946, hadoop-slave3, executor 5, partition 185, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 179.0 in stage 2.1 (TID 941) in 6 ms on hadoop-slave3 (executor 5) (152/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 186.0 in stage 2.1 (TID 947, hadoop-slave3, executor 5, partition 186, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 177.0 in stage 2.1 (TID 939) in 11 ms on hadoop-slave3 (executor 5) (153/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 187.0 in stage 2.1 (TID 948, hadoop-slave5, executor 7, partition 187, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 180.0 in stage 2.1 (TID 942) in 8 ms on hadoop-slave5 (executor 7) (154/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 189.0 in stage 2.1 (TID 949, hadoop-slave5, executor 7, partition 189, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 182.0 in stage 2.1 (TID 944) in 8 ms on hadoop-slave5 (executor 7) (155/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 191.0 in stage 2.1 (TID 950, hadoop-slave5, executor 7, partition 191, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 181.0 in stage 2.1 (TID 943) in 9 ms on hadoop-slave5 (executor 7) (156/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 193.0 in stage 2.1 (TID 951, hadoop-slave3, executor 5, partition 193, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 183.0 in stage 2.1 (TID 945) in 8 ms on hadoop-slave3 (executor 5) (157/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 195.0 in stage 2.1 (TID 952, hadoop-slave3, executor 5, partition 195, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 185.0 in stage 2.1 (TID 946) in 8 ms on hadoop-slave3 (executor 5) (158/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 196.0 in stage 2.1 (TID 953, hadoop-slave3, executor 5, partition 196, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 186.0 in stage 2.1 (TID 947) in 8 ms on hadoop-slave3 (executor 5) (159/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 197.0 in stage 2.1 (TID 954, hadoop-slave5, executor 7, partition 197, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 187.0 in stage 2.1 (TID 948) in 8 ms on hadoop-slave5 (executor 7) (160/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 198.0 in stage 2.1 (TID 955, hadoop-slave5, executor 7, partition 198, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 189.0 in stage 2.1 (TID 949) in 8 ms on hadoop-slave5 (executor 7) (161/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 199.0 in stage 2.1 (TID 956, hadoop-slave5, executor 7, partition 199, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 191.0 in stage 2.1 (TID 950) in 8 ms on hadoop-slave5 (executor 7) (162/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 141.0 in stage 3.0 (TID 957, hadoop-slave3, executor 5, partition 141, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 193.0 in stage 2.1 (TID 951) in 8 ms on hadoop-slave3 (executor 5) (163/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 142.0 in stage 3.0 (TID 958, hadoop-slave3, executor 5, partition 142, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 195.0 in stage 2.1 (TID 952) in 8 ms on hadoop-slave3 (executor 5) (164/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 143.0 in stage 3.0 (TID 959, hadoop-slave3, executor 5, partition 143, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 196.0 in stage 2.1 (TID 953) in 7 ms on hadoop-slave3 (executor 5) (165/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 144.0 in stage 3.0 (TID 960, hadoop-slave5, executor 7, partition 144, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 198.0 in stage 2.1 (TID 955) in 6 ms on hadoop-slave5 (executor 7) (166/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 145.0 in stage 3.0 (TID 961, hadoop-slave5, executor 7, partition 145, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 197.0 in stage 2.1 (TID 954) in 6 ms on hadoop-slave5 (executor 7) (167/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 146.0 in stage 3.0 (TID 962, hadoop-slave5, executor 7, partition 146, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 199.0 in stage 2.1 (TID 956) in 6 ms on hadoop-slave5 (executor 7) (168/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 150.0 in stage 3.0 (TID 963, hadoop-slave2, executor 6, partition 150, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 128.0 in stage 3.0 (TID 771) in 6981 ms on hadoop-slave2 (executor 6) (131/200)
18/02/14 13:25:56 INFO TaskSetManager: Starting task 147.0 in stage 3.0 (TID 964, hadoop-slave3, executor 5, partition 147, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:56 INFO TaskSetManager: Finished task 132.0 in stage 3.0 (TID 773) in 6680 ms on hadoop-slave3 (executor 5) (132/200)
18/02/14 13:25:57 INFO TaskSetManager: Starting task 148.0 in stage 3.0 (TID 965, hadoop-slave5, executor 7, partition 148, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:57 INFO TaskSetManager: Finished task 114.0 in stage 3.0 (TID 755) in 11845 ms on hadoop-slave5 (executor 7) (133/200)
18/02/14 13:25:57 INFO TaskSetManager: Starting task 149.0 in stage 3.0 (TID 966, hadoop-slave5, executor 7, partition 149, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:57 INFO TaskSetManager: Finished task 134.0 in stage 3.0 (TID 775) in 6581 ms on hadoop-slave5 (executor 7) (134/200)
18/02/14 13:25:57 INFO TaskSetManager: Starting task 153.0 in stage 3.0 (TID 967, hadoop-slave2, executor 6, partition 153, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:57 INFO TaskSetManager: Finished task 135.0 in stage 3.0 (TID 776) in 5588 ms on hadoop-slave2 (executor 6) (135/200)
18/02/14 13:25:57 INFO TaskSetManager: Starting task 35.0 in stage 2.1 (TID 968, hadoop-slave1, executor 3, partition 35, NODE_LOCAL, 6201 bytes)
18/02/14 13:25:57 INFO TaskSetManager: Finished task 121.0 in stage 3.0 (TID 760) in 11352 ms on hadoop-slave1 (executor 3) (136/200)
18/02/14 13:25:57 INFO TaskSetManager: Starting task 151.0 in stage 3.0 (TID 969, hadoop-slave3, executor 5, partition 151, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:57 INFO TaskSetManager: Finished task 139.0 in stage 3.0 (TID 779) in 5364 ms on hadoop-slave3 (executor 5) (137/200)
18/02/14 13:25:57 INFO TaskSetManager: Starting task 154.0 in stage 3.0 (TID 970, hadoop-slave2, executor 6, partition 154, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:57 INFO TaskSetManager: Finished task 140.0 in stage 3.0 (TID 781) in 5385 ms on hadoop-slave2 (executor 6) (138/200)
18/02/14 13:25:58 INFO TaskSetManager: Starting task 152.0 in stage 3.0 (TID 971, hadoop-slave3, executor 5, partition 152, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:58 INFO TaskSetManager: Finished task 137.0 in stage 3.0 (TID 777) in 6303 ms on hadoop-slave3 (executor 5) (139/200)
18/02/14 13:25:58 INFO TaskSetManager: Starting task 158.0 in stage 3.0 (TID 972, hadoop-slave5, executor 7, partition 158, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:58 INFO TaskSetManager: Finished task 138.0 in stage 3.0 (TID 780) in 6000 ms on hadoop-slave5 (executor 7) (140/200)
18/02/14 13:25:59 INFO TaskSetManager: Starting task 155.0 in stage 3.0 (TID 973, hadoop-slave2, executor 6, partition 155, NODE_LOCAL, 6941 bytes)
18/02/14 13:25:59 INFO TaskSetManager: Finished task 133.0 in stage 3.0 (TID 774) in 8984 ms on hadoop-slave2 (executor 6) (141/200)
18/02/14 13:25:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.34:59054
18/02/14 13:26:00 INFO TaskSetManager: Starting task 57.0 in stage 2.1 (TID 974, hadoop-slave3, executor 5, partition 57, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:00 INFO TaskSetManager: Finished task 142.0 in stage 3.0 (TID 958) in 4087 ms on hadoop-slave3 (executor 5) (142/200)
18/02/14 13:26:00 INFO TaskSetManager: Starting task 66.0 in stage 2.1 (TID 975, hadoop-slave3, executor 5, partition 66, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:00 INFO TaskSetManager: Finished task 147.0 in stage 3.0 (TID 964) in 4238 ms on hadoop-slave3 (executor 5) (143/200)
18/02/14 13:26:01 INFO TaskSetManager: Starting task 77.0 in stage 2.1 (TID 976, hadoop-slave3, executor 5, partition 77, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:01 INFO TaskSetManager: Finished task 143.0 in stage 3.0 (TID 959) in 5131 ms on hadoop-slave3 (executor 5) (144/200)
18/02/14 13:26:01 INFO TaskSetManager: Starting task 81.0 in stage 2.1 (TID 977, hadoop-slave3, executor 5, partition 81, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:01 INFO TaskSetManager: Finished task 141.0 in stage 3.0 (TID 957) in 5401 ms on hadoop-slave3 (executor 5) (145/200)
18/02/14 13:26:01 INFO TaskSetManager: Starting task 82.0 in stage 2.1 (TID 978, hadoop-slave2, executor 6, partition 82, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:01 INFO TaskSetManager: Finished task 150.0 in stage 3.0 (TID 963) in 5300 ms on hadoop-slave2 (executor 6) (146/200)
18/02/14 13:26:02 INFO TaskSetManager: Starting task 86.0 in stage 2.1 (TID 979, hadoop-slave2, executor 6, partition 86, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:02 INFO TaskSetManager: Finished task 153.0 in stage 3.0 (TID 967) in 5424 ms on hadoop-slave2 (executor 6) (147/200)
18/02/14 13:26:02 INFO TaskSetManager: Starting task 89.0 in stage 2.1 (TID 980, hadoop-slave3, executor 5, partition 89, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:02 INFO TaskSetManager: Finished task 151.0 in stage 3.0 (TID 969) in 5265 ms on hadoop-slave3 (executor 5) (148/200)
18/02/14 13:26:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.34:59054
18/02/14 13:26:03 INFO TaskSetManager: Starting task 92.0 in stage 2.1 (TID 981, hadoop-slave2, executor 6, partition 92, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:03 INFO TaskSetManager: Finished task 154.0 in stage 3.0 (TID 970) in 5216 ms on hadoop-slave2 (executor 6) (149/200)
18/02/14 13:26:03 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.19.3.36:33535 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:26:03 INFO BlockManagerInfo: Removed broadcast_13_piece0 on hadoop-slave3:33203 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:26:03 INFO BlockManagerInfo: Removed broadcast_13_piece0 on hadoop-slave1:46007 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:26:03 INFO BlockManagerInfo: Removed broadcast_13_piece0 on hadoop-slave2:33289 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:26:03 INFO BlockManagerInfo: Removed broadcast_13_piece0 on hadoop-slave5:35799 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:26:03 INFO TaskSetManager: Starting task 95.0 in stage 2.1 (TID 982, hadoop-slave5, executor 7, partition 95, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:03 INFO TaskSetManager: Finished task 144.0 in stage 3.0 (TID 960) in 6860 ms on hadoop-slave5 (executor 7) (150/200)
18/02/14 13:26:03 INFO TaskSetManager: Starting task 100.0 in stage 2.1 (TID 983, hadoop-slave5, executor 7, partition 100, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:03 INFO TaskSetManager: Finished task 146.0 in stage 3.0 (TID 962) in 7222 ms on hadoop-slave5 (executor 7) (151/200)
18/02/14 13:26:04 INFO TaskSetManager: Starting task 101.0 in stage 2.1 (TID 984, hadoop-slave2, executor 6, partition 101, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:04 INFO TaskSetManager: Finished task 0.0 in stage 2.1 (TID 784) in 11434 ms on hadoop-slave2 (executor 6) (169/200)
18/02/14 13:26:04 INFO TaskSetManager: Starting task 126.0 in stage 2.1 (TID 985, hadoop-slave2, executor 6, partition 126, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:04 INFO TaskSetManager: Finished task 2.0 in stage 2.1 (TID 787) in 9826 ms on hadoop-slave2 (executor 6) (170/200)
18/02/14 13:26:04 INFO TaskSetManager: Starting task 129.0 in stage 2.1 (TID 986, hadoop-slave5, executor 7, partition 129, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:04 INFO TaskSetManager: Finished task 149.0 in stage 3.0 (TID 966) in 7460 ms on hadoop-slave5 (executor 7) (152/200)
18/02/14 13:26:04 INFO TaskSetManager: Starting task 142.0 in stage 2.1 (TID 987, hadoop-slave5, executor 7, partition 142, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:04 INFO TaskSetManager: Finished task 145.0 in stage 3.0 (TID 961) in 8200 ms on hadoop-slave5 (executor 7) (153/200)
18/02/14 13:26:04 INFO TaskSetManager: Starting task 153.0 in stage 2.1 (TID 988, hadoop-slave5, executor 7, partition 153, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:04 INFO TaskSetManager: Finished task 148.0 in stage 3.0 (TID 965) in 7588 ms on hadoop-slave5 (executor 7) (154/200)
18/02/14 13:26:04 INFO TaskSetManager: Starting task 154.0 in stage 2.1 (TID 989, hadoop-slave5, executor 7, partition 154, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:04 INFO TaskSetManager: Finished task 158.0 in stage 3.0 (TID 972) in 6379 ms on hadoop-slave5 (executor 7) (155/200)
18/02/14 13:26:04 INFO TaskSetManager: Starting task 165.0 in stage 2.1 (TID 990, hadoop-slave3, executor 5, partition 165, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:04 INFO TaskSetManager: Finished task 152.0 in stage 3.0 (TID 971) in 6629 ms on hadoop-slave3 (executor 5) (156/200)
18/02/14 13:26:06 INFO TaskSetManager: Starting task 169.0 in stage 2.1 (TID 991, hadoop-slave2, executor 6, partition 169, RACK_LOCAL, 6201 bytes)
18/02/14 13:26:06 INFO TaskSetManager: Finished task 155.0 in stage 3.0 (TID 973) in 6928 ms on hadoop-slave2 (executor 6) (157/200)
18/02/14 13:35:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 3.
18/02/14 13:35:33 INFO DAGScheduler: Executor lost: 3 (epoch 55)
18/02/14 13:35:33 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
18/02/14 13:35:33 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, hadoop-slave1, 46007, None)
18/02/14 13:35:33 INFO BlockManagerMaster: Removed 3 successfully in removeExecutor
18/02/14 13:35:33 INFO DAGScheduler: Shuffle files lost for executor: 3 (epoch 55)
18/02/14 13:35:33 INFO ShuffleMapStage: ShuffleMapStage 1 is now unavailable on executor 3 (106/200, false)
18/02/14 13:35:33 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 3 (110/200, false)
18/02/14 13:35:33 INFO ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 3 (25/100, false)
18/02/14 13:35:33 ERROR YarnScheduler: Lost executor 3 on hadoop-slave1: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 98), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 45), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 52), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 91), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 123), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 4), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 86), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 89), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 77), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 49), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 79), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 26), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 1), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 12), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 55), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 14), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 78), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 93), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 36), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 75), so marking it as still running
18/02/14 13:35:33 WARN TaskSetManager: Lost task 9.0 in stage 2.1 (TID 783, hadoop-slave1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 60), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 66), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 21), so marking it as still running
18/02/14 13:35:33 WARN TaskSetManager: Lost task 35.0 in stage 2.1 (TID 968, hadoop-slave1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 118), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 29), so marking it as still running
18/02/14 13:35:33 WARN TaskSetManager: Lost task 18.0 in stage 2.1 (TID 788, hadoop-slave1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 85), so marking it as still running
18/02/14 13:35:33 WARN TaskSetManager: Lost task 5.0 in stage 2.1 (TID 782, hadoop-slave1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 82), so marking it as still running
18/02/14 13:35:33 WARN TaskSetManager: Lost task 16.0 in stage 2.1 (TID 785, hadoop-slave1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 10), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 9), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 33), so marking it as still running
18/02/14 13:35:33 WARN TaskSetManager: Lost task 34.0 in stage 2.1 (TID 834, hadoop-slave1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 74), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 30), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 83), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 136), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 121), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 130), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 87), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 126), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 112), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 18), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 95), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 63), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 37), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 43), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 7), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 51), so marking it as still running
18/02/14 13:35:33 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 15), so marking it as still running
18/02/14 13:35:33 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_e41_1518606550421_0002_01_000005 on host: hadoop-slave1. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

18/02/14 13:35:33 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
18/02/14 13:35:33 INFO BlockManagerMaster: Removal of executor 3 requested
18/02/14 13:35:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 3
18/02/14 13:35:41 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.35:60446) with ID 8
18/02/14 13:35:41 INFO TaskSetManager: Starting task 34.1 in stage 2.1 (TID 992, hadoop-slave1, executor 8, partition 34, NODE_LOCAL, 6201 bytes)
18/02/14 13:35:41 INFO TaskSetManager: Starting task 16.1 in stage 2.1 (TID 993, hadoop-slave1, executor 8, partition 16, NODE_LOCAL, 6201 bytes)
18/02/14 13:35:41 INFO TaskSetManager: Starting task 5.1 in stage 2.1 (TID 994, hadoop-slave1, executor 8, partition 5, NODE_LOCAL, 6201 bytes)
18/02/14 13:35:41 INFO TaskSetManager: Starting task 18.1 in stage 2.1 (TID 995, hadoop-slave1, executor 8, partition 18, NODE_LOCAL, 6201 bytes)
18/02/14 13:35:41 INFO TaskSetManager: Starting task 35.1 in stage 2.1 (TID 996, hadoop-slave1, executor 8, partition 35, NODE_LOCAL, 6201 bytes)
18/02/14 13:35:41 INFO TaskSetManager: Starting task 9.1 in stage 2.1 (TID 997, hadoop-slave1, executor 8, partition 9, NODE_LOCAL, 6201 bytes)
18/02/14 13:35:41 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave1:41387 with 4.1 GB RAM, BlockManagerId(8, hadoop-slave1, 41387, None)
18/02/14 13:35:41 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on hadoop-slave1:41387 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:35:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.35:60446
18/02/14 13:35:42 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 378 bytes
18/02/14 13:35:42 INFO TaskSetManager: Starting task 174.0 in stage 2.1 (TID 998, hadoop-slave1, executor 8, partition 174, NODE_LOCAL, 6201 bytes)
18/02/14 13:35:42 WARN TaskSetManager: Lost task 34.1 in stage 2.1 (TID 992, hadoop-slave1, executor 8): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=34, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:35:42 INFO TaskSetManager: Task 34.1 in stage 2.1 (TID 992) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:42 INFO DAGScheduler: Marking ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) as failed due to a fetch failure from ShuffleMapStage 0 (show at JaccardCoefficient.scala:74)
18/02/14 13:35:42 INFO DAGScheduler: ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) failed in 590.102 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

18/02/14 13:35:42 INFO DAGScheduler: Resubmitting ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:35:42 INFO TaskSetManager: Starting task 15.2 in stage 3.0 (TID 999, hadoop-slave1, executor 8, partition 15, NODE_LOCAL, 6941 bytes)
18/02/14 13:35:42 WARN TaskSetManager: Lost task 16.1 in stage 2.1 (TID 993, hadoop-slave1, executor 8): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=16, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:35:42 INFO TaskSetManager: Task 16.1 in stage 2.1 (TID 993) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:42 INFO TaskSetManager: Starting task 51.2 in stage 3.0 (TID 1000, hadoop-slave1, executor 8, partition 51, NODE_LOCAL, 6941 bytes)
18/02/14 13:35:42 WARN TaskSetManager: Lost task 35.1 in stage 2.1 (TID 996, hadoop-slave1, executor 8): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=35, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:35:42 INFO TaskSetManager: Task 35.1 in stage 2.1 (TID 996) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:42 INFO TaskSetManager: Starting task 7.1 in stage 3.0 (TID 1001, hadoop-slave1, executor 8, partition 7, NODE_LOCAL, 6941 bytes)
18/02/14 13:35:42 WARN TaskSetManager: Lost task 9.1 in stage 2.1 (TID 997, hadoop-slave1, executor 8): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:35:42 INFO TaskSetManager: Task 9.1 in stage 2.1 (TID 997) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:42 INFO TaskSetManager: Starting task 43.2 in stage 3.0 (TID 1002, hadoop-slave1, executor 8, partition 43, NODE_LOCAL, 6941 bytes)
18/02/14 13:35:42 WARN TaskSetManager: Lost task 5.1 in stage 2.1 (TID 994, hadoop-slave1, executor 8): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:35:42 INFO TaskSetManager: Task 5.1 in stage 2.1 (TID 994) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:42 INFO TaskSetManager: Starting task 37.2 in stage 3.0 (TID 1003, hadoop-slave1, executor 8, partition 37, NODE_LOCAL, 6941 bytes)
18/02/14 13:35:42 WARN TaskSetManager: Lost task 18.1 in stage 2.1 (TID 995, hadoop-slave1, executor 8): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=18, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:35:42 INFO TaskSetManager: Task 18.1 in stage 2.1 (TID 995) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:42 INFO TaskSetManager: Starting task 63.1 in stage 3.0 (TID 1004, hadoop-slave1, executor 8, partition 63, NODE_LOCAL, 6941 bytes)
18/02/14 13:35:42 WARN TaskSetManager: Lost task 174.0 in stage 2.1 (TID 998, hadoop-slave1, executor 8): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=174, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:35:42 INFO TaskSetManager: Task 174.0 in stage 2.1 (TID 998) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave1:41387 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:35:42 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:35:42 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:35:42 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 10.3 KB, free 4.1 GB)
18/02/14 13:35:42 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.1 KB, free 4.1 GB)
18/02/14 13:35:42 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.19.3.36:33535 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:35:42 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:996
18/02/14 13:35:42 INFO DAGScheduler: Submitting 75 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74)
18/02/14 13:35:42 INFO YarnScheduler: Adding task set 0.4 with 75 tasks
18/02/14 13:35:42 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:35:42 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 14.7 KB, free 4.1 GB)
18/02/14 13:35:42 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.9 KB, free 4.1 GB)
18/02/14 13:35:42 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.19.3.36:33535 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:35:42 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:996
18/02/14 13:35:42 INFO DAGScheduler: Submitting 94 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74)
18/02/14 13:35:42 INFO YarnScheduler: Adding task set 1.3 with 94 tasks
18/02/14 13:35:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave1:41387 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:35:48 INFO TaskSetManager: Starting task 2.0 in stage 0.4 (TID 1005, hadoop-slave3, executor 5, partition 2, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 89.0 in stage 2.1 (TID 980, hadoop-slave3, executor 5): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=111, reduceId=89, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 89.0 in stage 2.1 (TID 980) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO DAGScheduler: Resubmitting ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:35:48 INFO TaskSetManager: Starting task 3.0 in stage 0.4 (TID 1006, hadoop-slave3, executor 5, partition 4, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 81.0 in stage 2.1 (TID 977, hadoop-slave3, executor 5): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=166, reduceId=81, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 81.0 in stage 2.1 (TID 977) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on hadoop-slave3:33203 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:35:48 INFO TaskSetManager: Starting task 5.0 in stage 0.4 (TID 1007, hadoop-slave3, executor 5, partition 7, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 77.0 in stage 2.1 (TID 976, hadoop-slave3, executor 5): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=114, reduceId=77, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 77.0 in stage 2.1 (TID 976) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 8.0 in stage 0.4 (TID 1008, hadoop-slave3, executor 5, partition 11, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 165.0 in stage 2.1 (TID 990, hadoop-slave3, executor 5): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=155, reduceId=165, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 165.0 in stage 2.1 (TID 990) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 10.0 in stage 0.4 (TID 1009, hadoop-slave3, executor 5, partition 13, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 66.0 in stage 2.1 (TID 975, hadoop-slave3, executor 5): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=37, reduceId=66, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 66.0 in stage 2.1 (TID 975) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 0.0 in stage 0.4 (TID 1010, hadoop-slave2, executor 6, partition 0, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 169.0 in stage 2.1 (TID 991, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=177, reduceId=169, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 169.0 in stage 2.1 (TID 991) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 1.0 in stage 0.4 (TID 1011, hadoop-slave2, executor 6, partition 1, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 92.0 in stage 2.1 (TID 981, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=52, reduceId=92, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 92.0 in stage 2.1 (TID 981) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 4.0 in stage 0.4 (TID 1012, hadoop-slave2, executor 6, partition 5, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 101.0 in stage 2.1 (TID 984, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=59, reduceId=101, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 101.0 in stage 2.1 (TID 984) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 6.0 in stage 0.4 (TID 1013, hadoop-slave2, executor 6, partition 9, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 126.0 in stage 2.1 (TID 985, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=5, reduceId=126, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 126.0 in stage 2.1 (TID 985) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 7.0 in stage 0.4 (TID 1014, hadoop-slave2, executor 6, partition 10, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 82.0 in stage 2.1 (TID 978, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=105, reduceId=82, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 82.0 in stage 2.1 (TID 978) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 12.0 in stage 0.4 (TID 1015, hadoop-slave3, executor 5, partition 15, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 57.0 in stage 2.1 (TID 974, hadoop-slave3, executor 5): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=0, reduceId=57, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 57.0 in stage 2.1 (TID 974) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 11.0 in stage 0.4 (TID 1016, hadoop-slave5, executor 7, partition 14, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 129.0 in stage 2.1 (TID 986, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=127, reduceId=129, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 129.0 in stage 2.1 (TID 986) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 13.0 in stage 0.4 (TID 1017, hadoop-slave5, executor 7, partition 16, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 153.0 in stage 2.1 (TID 988, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=43, reduceId=153, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 153.0 in stage 2.1 (TID 988) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 15.0 in stage 0.4 (TID 1018, hadoop-slave5, executor 7, partition 21, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 154.0 in stage 2.1 (TID 989, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=109, reduceId=154, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 154.0 in stage 2.1 (TID 989) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 17.0 in stage 0.4 (TID 1019, hadoop-slave5, executor 7, partition 25, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 95.0 in stage 2.1 (TID 982, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=145, reduceId=95, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 95.0 in stage 2.1 (TID 982) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 18.0 in stage 0.4 (TID 1020, hadoop-slave5, executor 7, partition 26, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 100.0 in stage 2.1 (TID 983, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=198, reduceId=100, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 100.0 in stage 2.1 (TID 983) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO TaskSetManager: Starting task 22.0 in stage 0.4 (TID 1021, hadoop-slave5, executor 7, partition 31, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:48 WARN TaskSetManager: Lost task 142.0 in stage 2.1 (TID 987, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(3, hadoop-slave1, 46007, None), shuffleId=1, mapId=145, reduceId=142, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave1/172.19.3.35:46007
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave1/172.19.3.35:46007
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:35:48 INFO TaskSetManager: Task 142.0 in stage 2.1 (TID 987) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:35:48 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:35:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on hadoop-slave2:33289 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:35:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on hadoop-slave5:35799 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:35:51 INFO TaskSetManager: Starting task 9.0 in stage 0.4 (TID 1022, hadoop-slave1, executor 8, partition 12, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:51 INFO TaskSetManager: Finished task 51.2 in stage 3.0 (TID 1000) in 8756 ms on hadoop-slave1 (executor 8) (111/200)
18/02/14 13:35:51 INFO TaskSetManager: Starting task 14.0 in stage 0.4 (TID 1023, hadoop-slave1, executor 8, partition 18, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:51 INFO TaskSetManager: Finished task 15.2 in stage 3.0 (TID 999) in 8758 ms on hadoop-slave1 (executor 8) (112/200)
18/02/14 13:35:51 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on hadoop-slave1:41387 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:35:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave1:41387 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:35:51 INFO TaskSetManager: Starting task 16.0 in stage 0.4 (TID 1024, hadoop-slave1, executor 8, partition 23, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:51 INFO TaskSetManager: Finished task 37.2 in stage 3.0 (TID 1003) in 9027 ms on hadoop-slave1 (executor 8) (113/200)
18/02/14 13:35:51 INFO TaskSetManager: Starting task 19.0 in stage 0.4 (TID 1025, hadoop-slave1, executor 8, partition 27, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:51 INFO TaskSetManager: Finished task 43.2 in stage 3.0 (TID 1002) in 9262 ms on hadoop-slave1 (executor 8) (114/200)
18/02/14 13:35:52 INFO TaskSetManager: Starting task 20.0 in stage 0.4 (TID 1026, hadoop-slave3, executor 5, partition 29, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:52 INFO TaskSetManager: Finished task 8.0 in stage 0.4 (TID 1008) in 3619 ms on hadoop-slave3 (executor 5) (1/75)
18/02/14 13:35:52 INFO TaskSetManager: Starting task 21.0 in stage 0.4 (TID 1027, hadoop-slave1, executor 8, partition 30, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:52 INFO TaskSetManager: Finished task 7.1 in stage 3.0 (TID 1001) in 9631 ms on hadoop-slave1 (executor 8) (115/200)
18/02/14 13:35:52 INFO TaskSetManager: Starting task 23.0 in stage 0.4 (TID 1028, hadoop-slave3, executor 5, partition 32, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:52 INFO TaskSetManager: Finished task 10.0 in stage 0.4 (TID 1009) in 3830 ms on hadoop-slave3 (executor 5) (2/75)
18/02/14 13:35:52 INFO TaskSetManager: Starting task 24.0 in stage 0.4 (TID 1029, hadoop-slave1, executor 8, partition 33, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:52 INFO TaskSetManager: Finished task 63.1 in stage 3.0 (TID 1004) in 9729 ms on hadoop-slave1 (executor 8) (116/200)
18/02/14 13:35:52 INFO TaskSetManager: Starting task 26.0 in stage 0.4 (TID 1030, hadoop-slave3, executor 5, partition 37, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:52 INFO TaskSetManager: Finished task 5.0 in stage 0.4 (TID 1007) in 4105 ms on hadoop-slave3 (executor 5) (3/75)
18/02/14 13:35:52 INFO TaskSetManager: Starting task 25.0 in stage 0.4 (TID 1031, hadoop-slave2, executor 6, partition 36, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:52 INFO TaskSetManager: Finished task 7.0 in stage 0.4 (TID 1014) in 4397 ms on hadoop-slave2 (executor 6) (4/75)
18/02/14 13:35:53 INFO TaskSetManager: Starting task 28.0 in stage 0.4 (TID 1032, hadoop-slave2, executor 6, partition 41, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:53 INFO TaskSetManager: Finished task 6.0 in stage 0.4 (TID 1013) in 4836 ms on hadoop-slave2 (executor 6) (5/75)
18/02/14 13:35:53 INFO TaskSetManager: Starting task 27.0 in stage 0.4 (TID 1033, hadoop-slave3, executor 5, partition 39, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:53 INFO TaskSetManager: Finished task 2.0 in stage 0.4 (TID 1005) in 5015 ms on hadoop-slave3 (executor 5) (6/75)
18/02/14 13:35:53 INFO TaskSetManager: Starting task 34.0 in stage 0.4 (TID 1034, hadoop-slave2, executor 6, partition 50, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:53 INFO TaskSetManager: Finished task 4.0 in stage 0.4 (TID 1012) in 5434 ms on hadoop-slave2 (executor 6) (7/75)
18/02/14 13:35:54 INFO TaskSetManager: Starting task 29.0 in stage 0.4 (TID 1035, hadoop-slave3, executor 5, partition 42, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:54 INFO TaskSetManager: Finished task 12.0 in stage 0.4 (TID 1015) in 5855 ms on hadoop-slave3 (executor 5) (8/75)
18/02/14 13:35:54 INFO TaskSetManager: Starting task 37.0 in stage 0.4 (TID 1036, hadoop-slave2, executor 6, partition 53, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:54 INFO TaskSetManager: Finished task 0.0 in stage 0.4 (TID 1010) in 5968 ms on hadoop-slave2 (executor 6) (9/75)
18/02/14 13:35:54 INFO TaskSetManager: Starting task 31.0 in stage 0.4 (TID 1037, hadoop-slave5, executor 7, partition 46, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:54 INFO TaskSetManager: Finished task 22.0 in stage 0.4 (TID 1021) in 5967 ms on hadoop-slave5 (executor 7) (10/75)
18/02/14 13:35:54 INFO TaskSetManager: Starting task 30.0 in stage 0.4 (TID 1038, hadoop-slave3, executor 5, partition 43, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:54 INFO TaskSetManager: Finished task 3.0 in stage 0.4 (TID 1006) in 6294 ms on hadoop-slave3 (executor 5) (11/75)
18/02/14 13:35:54 INFO TaskSetManager: Starting task 33.0 in stage 0.4 (TID 1039, hadoop-slave5, executor 7, partition 49, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:54 INFO TaskSetManager: Finished task 18.0 in stage 0.4 (TID 1020) in 6241 ms on hadoop-slave5 (executor 7) (12/75)
18/02/14 13:35:54 INFO TaskSetManager: Starting task 38.0 in stage 0.4 (TID 1040, hadoop-slave2, executor 6, partition 54, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:54 INFO TaskSetManager: Finished task 1.0 in stage 0.4 (TID 1011) in 6437 ms on hadoop-slave2 (executor 6) (13/75)
18/02/14 13:35:54 INFO TaskSetManager: Starting task 35.0 in stage 0.4 (TID 1041, hadoop-slave5, executor 7, partition 51, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:54 INFO TaskSetManager: Finished task 17.0 in stage 0.4 (TID 1019) in 6329 ms on hadoop-slave5 (executor 7) (14/75)
18/02/14 13:35:55 INFO TaskSetManager: Starting task 36.0 in stage 0.4 (TID 1042, hadoop-slave5, executor 7, partition 52, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:55 INFO TaskSetManager: Finished task 13.0 in stage 0.4 (TID 1017) in 6581 ms on hadoop-slave5 (executor 7) (15/75)
18/02/14 13:35:55 INFO TaskSetManager: Starting task 32.0 in stage 0.4 (TID 1043, hadoop-slave1, executor 8, partition 47, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:55 INFO TaskSetManager: Finished task 9.0 in stage 0.4 (TID 1022) in 4264 ms on hadoop-slave1 (executor 8) (16/75)
18/02/14 13:35:55 INFO TaskSetManager: Starting task 42.0 in stage 0.4 (TID 1044, hadoop-slave5, executor 7, partition 58, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:55 INFO TaskSetManager: Finished task 15.0 in stage 0.4 (TID 1018) in 7083 ms on hadoop-slave5 (executor 7) (17/75)
18/02/14 13:35:55 INFO TaskSetManager: Starting task 43.0 in stage 0.4 (TID 1045, hadoop-slave5, executor 7, partition 59, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:55 INFO TaskSetManager: Finished task 11.0 in stage 0.4 (TID 1016) in 7092 ms on hadoop-slave5 (executor 7) (18/75)
18/02/14 13:35:55 INFO TaskSetManager: Starting task 39.0 in stage 0.4 (TID 1046, hadoop-slave1, executor 8, partition 55, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:55 INFO TaskSetManager: Finished task 14.0 in stage 0.4 (TID 1023) in 4411 ms on hadoop-slave1 (executor 8) (19/75)
18/02/14 13:35:56 INFO TaskSetManager: Starting task 40.0 in stage 0.4 (TID 1047, hadoop-slave3, executor 5, partition 56, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:56 INFO TaskSetManager: Finished task 20.0 in stage 0.4 (TID 1026) in 4202 ms on hadoop-slave3 (executor 5) (20/75)
18/02/14 13:35:56 INFO TaskSetManager: Starting task 41.0 in stage 0.4 (TID 1048, hadoop-slave1, executor 8, partition 57, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:56 INFO TaskSetManager: Finished task 21.0 in stage 0.4 (TID 1027) in 4222 ms on hadoop-slave1 (executor 8) (21/75)
18/02/14 13:35:56 INFO TaskSetManager: Starting task 44.0 in stage 0.4 (TID 1049, hadoop-slave1, executor 8, partition 60, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:56 INFO TaskSetManager: Finished task 16.0 in stage 0.4 (TID 1024) in 4842 ms on hadoop-slave1 (executor 8) (22/75)
18/02/14 13:35:56 INFO TaskSetManager: Starting task 47.0 in stage 0.4 (TID 1050, hadoop-slave1, executor 8, partition 63, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:56 INFO TaskSetManager: Finished task 19.0 in stage 0.4 (TID 1025) in 4610 ms on hadoop-slave1 (executor 8) (23/75)
18/02/14 13:35:57 INFO TaskSetManager: Starting task 48.0 in stage 0.4 (TID 1051, hadoop-slave1, executor 8, partition 66, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:57 INFO TaskSetManager: Finished task 24.0 in stage 0.4 (TID 1029) in 4990 ms on hadoop-slave1 (executor 8) (24/75)
18/02/14 13:35:57 INFO TaskSetManager: Starting task 45.0 in stage 0.4 (TID 1052, hadoop-slave3, executor 5, partition 61, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:57 INFO TaskSetManager: Finished task 26.0 in stage 0.4 (TID 1030) in 5251 ms on hadoop-slave3 (executor 5) (25/75)
18/02/14 13:35:57 INFO TaskSetManager: Starting task 46.0 in stage 0.4 (TID 1053, hadoop-slave3, executor 5, partition 62, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:57 INFO TaskSetManager: Finished task 27.0 in stage 0.4 (TID 1033) in 4361 ms on hadoop-slave3 (executor 5) (26/75)
18/02/14 13:35:57 INFO TaskSetManager: Starting task 49.0 in stage 0.4 (TID 1054, hadoop-slave2, executor 6, partition 70, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:57 INFO TaskSetManager: Finished task 25.0 in stage 0.4 (TID 1031) in 5162 ms on hadoop-slave2 (executor 6) (27/75)
18/02/14 13:35:57 INFO TaskSetManager: Starting task 50.0 in stage 0.4 (TID 1055, hadoop-slave3, executor 5, partition 71, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:57 INFO TaskSetManager: Finished task 29.0 in stage 0.4 (TID 1035) in 3728 ms on hadoop-slave3 (executor 5) (28/75)
18/02/14 13:35:58 INFO TaskSetManager: Starting task 51.0 in stage 0.4 (TID 1056, hadoop-slave2, executor 6, partition 73, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:58 INFO TaskSetManager: Finished task 37.0 in stage 0.4 (TID 1036) in 3939 ms on hadoop-slave2 (executor 6) (29/75)
18/02/14 13:35:58 INFO TaskSetManager: Starting task 52.0 in stage 0.4 (TID 1057, hadoop-slave5, executor 7, partition 74, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:58 INFO TaskSetManager: Finished task 31.0 in stage 0.4 (TID 1037) in 3816 ms on hadoop-slave5 (executor 7) (30/75)
18/02/14 13:35:58 INFO TaskSetManager: Starting task 54.0 in stage 0.4 (TID 1058, hadoop-slave2, executor 6, partition 76, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:58 INFO TaskSetManager: Finished task 28.0 in stage 0.4 (TID 1032) in 5324 ms on hadoop-slave2 (executor 6) (31/75)
18/02/14 13:35:58 INFO TaskSetManager: Starting task 53.0 in stage 0.4 (TID 1059, hadoop-slave3, executor 5, partition 75, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:58 INFO TaskSetManager: Finished task 23.0 in stage 0.4 (TID 1028) in 6454 ms on hadoop-slave3 (executor 5) (32/75)
18/02/14 13:35:58 INFO TaskSetManager: Starting task 55.0 in stage 0.4 (TID 1060, hadoop-slave2, executor 6, partition 77, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:58 INFO TaskSetManager: Finished task 34.0 in stage 0.4 (TID 1034) in 4892 ms on hadoop-slave2 (executor 6) (33/75)
18/02/14 13:35:58 INFO TaskSetManager: Starting task 59.0 in stage 0.4 (TID 1061, hadoop-slave3, executor 5, partition 81, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:58 INFO TaskSetManager: Finished task 30.0 in stage 0.4 (TID 1038) in 4102 ms on hadoop-slave3 (executor 5) (34/75)
18/02/14 13:35:59 INFO TaskSetManager: Starting task 56.0 in stage 0.4 (TID 1062, hadoop-slave5, executor 7, partition 78, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:59 INFO TaskSetManager: Finished task 35.0 in stage 0.4 (TID 1041) in 4151 ms on hadoop-slave5 (executor 7) (35/75)
18/02/14 13:35:59 INFO TaskSetManager: Starting task 57.0 in stage 0.4 (TID 1063, hadoop-slave2, executor 6, partition 79, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:59 INFO TaskSetManager: Finished task 38.0 in stage 0.4 (TID 1040) in 4725 ms on hadoop-slave2 (executor 6) (36/75)
18/02/14 13:35:59 INFO TaskSetManager: Starting task 58.0 in stage 0.4 (TID 1064, hadoop-slave5, executor 7, partition 80, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:59 INFO TaskSetManager: Finished task 43.0 in stage 0.4 (TID 1045) in 4310 ms on hadoop-slave5 (executor 7) (37/75)
18/02/14 13:35:59 INFO TaskSetManager: Starting task 62.0 in stage 0.4 (TID 1065, hadoop-slave5, executor 7, partition 84, NODE_LOCAL, 6832 bytes)
18/02/14 13:35:59 INFO TaskSetManager: Finished task 36.0 in stage 0.4 (TID 1042) in 4846 ms on hadoop-slave5 (executor 7) (38/75)
18/02/14 13:36:00 INFO TaskSetManager: Starting task 60.0 in stage 0.4 (TID 1066, hadoop-slave1, executor 8, partition 82, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:00 INFO TaskSetManager: Finished task 39.0 in stage 0.4 (TID 1046) in 4495 ms on hadoop-slave1 (executor 8) (39/75)
18/02/14 13:36:00 INFO TaskSetManager: Starting task 61.0 in stage 0.4 (TID 1067, hadoop-slave1, executor 8, partition 83, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:00 INFO TaskSetManager: Finished task 44.0 in stage 0.4 (TID 1049) in 3936 ms on hadoop-slave1 (executor 8) (40/75)
18/02/14 13:36:00 INFO TaskSetManager: Starting task 63.0 in stage 0.4 (TID 1068, hadoop-slave1, executor 8, partition 85, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:00 INFO TaskSetManager: Finished task 47.0 in stage 0.4 (TID 1050) in 4010 ms on hadoop-slave1 (executor 8) (41/75)
18/02/14 13:36:00 INFO TaskSetManager: Starting task 64.0 in stage 0.4 (TID 1069, hadoop-slave1, executor 8, partition 86, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:00 INFO TaskSetManager: Finished task 32.0 in stage 0.4 (TID 1043) in 4887 ms on hadoop-slave1 (executor 8) (42/75)
18/02/14 13:36:01 INFO TaskSetManager: Starting task 65.0 in stage 0.4 (TID 1070, hadoop-slave1, executor 8, partition 87, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:01 INFO TaskSetManager: Finished task 41.0 in stage 0.4 (TID 1048) in 4646 ms on hadoop-slave1 (executor 8) (43/75)
18/02/14 13:36:01 INFO TaskSetManager: Starting task 66.0 in stage 0.4 (TID 1071, hadoop-slave1, executor 8, partition 89, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:01 INFO TaskSetManager: Finished task 48.0 in stage 0.4 (TID 1051) in 4396 ms on hadoop-slave1 (executor 8) (44/75)
18/02/14 13:36:01 INFO TaskSetManager: Starting task 67.0 in stage 0.4 (TID 1072, hadoop-slave3, executor 5, partition 90, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:01 INFO TaskSetManager: Finished task 40.0 in stage 0.4 (TID 1047) in 5674 ms on hadoop-slave3 (executor 5) (45/75)
18/02/14 13:36:01 INFO TaskSetManager: Starting task 70.0 in stage 0.4 (TID 1073, hadoop-slave5, executor 7, partition 94, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:01 INFO TaskSetManager: Finished task 33.0 in stage 0.4 (TID 1039) in 7093 ms on hadoop-slave5 (executor 7) (46/75)
18/02/14 13:36:01 INFO TaskSetManager: Starting task 68.0 in stage 0.4 (TID 1074, hadoop-slave2, executor 6, partition 91, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:01 INFO TaskSetManager: Finished task 49.0 in stage 0.4 (TID 1054) in 4041 ms on hadoop-slave2 (executor 6) (47/75)
18/02/14 13:36:02 INFO TaskSetManager: Starting task 69.0 in stage 0.4 (TID 1075, hadoop-slave3, executor 5, partition 93, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:02 INFO TaskSetManager: Finished task 46.0 in stage 0.4 (TID 1053) in 4717 ms on hadoop-slave3 (executor 5) (48/75)
18/02/14 13:36:03 INFO TaskSetManager: Starting task 73.0 in stage 0.4 (TID 1076, hadoop-slave2, executor 6, partition 98, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:03 INFO TaskSetManager: Finished task 51.0 in stage 0.4 (TID 1056) in 4830 ms on hadoop-slave2 (executor 6) (49/75)
18/02/14 13:36:03 INFO TaskSetManager: Starting task 71.0 in stage 0.4 (TID 1077, hadoop-slave3, executor 5, partition 95, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:03 INFO TaskSetManager: Finished task 45.0 in stage 0.4 (TID 1052) in 5740 ms on hadoop-slave3 (executor 5) (50/75)
18/02/14 13:36:03 INFO TaskSetManager: Starting task 74.0 in stage 0.4 (TID 1078, hadoop-slave2, executor 6, partition 99, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:03 INFO TaskSetManager: Finished task 54.0 in stage 0.4 (TID 1058) in 5003 ms on hadoop-slave2 (executor 6) (51/75)
18/02/14 13:36:03 INFO TaskSetManager: Starting task 72.0 in stage 0.4 (TID 1079, hadoop-slave3, executor 5, partition 97, NODE_LOCAL, 6832 bytes)
18/02/14 13:36:03 INFO TaskSetManager: Finished task 50.0 in stage 0.4 (TID 1055) in 5756 ms on hadoop-slave3 (executor 5) (52/75)
18/02/14 13:36:04 INFO TaskSetManager: Starting task 0.0 in stage 1.3 (TID 1080, hadoop-slave2, executor 6, partition 0, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:04 INFO TaskSetManager: Finished task 55.0 in stage 0.4 (TID 1060) in 5391 ms on hadoop-slave2 (executor 6) (53/75)
18/02/14 13:36:04 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on hadoop-slave2:33289 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:36:04 INFO TaskSetManager: Starting task 1.0 in stage 1.3 (TID 1081, hadoop-slave5, executor 7, partition 1, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:04 INFO TaskSetManager: Finished task 52.0 in stage 0.4 (TID 1057) in 6239 ms on hadoop-slave5 (executor 7) (54/75)
18/02/14 13:36:04 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on hadoop-slave5:35799 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:36:04 INFO TaskSetManager: Starting task 4.0 in stage 1.3 (TID 1082, hadoop-slave2, executor 6, partition 5, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:04 INFO TaskSetManager: Finished task 57.0 in stage 0.4 (TID 1063) in 5241 ms on hadoop-slave2 (executor 6) (55/75)
18/02/14 13:36:05 INFO TaskSetManager: Starting task 2.0 in stage 1.3 (TID 1083, hadoop-slave5, executor 7, partition 2, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:05 INFO TaskSetManager: Finished task 42.0 in stage 0.4 (TID 1044) in 9793 ms on hadoop-slave5 (executor 7) (56/75)
18/02/14 13:36:05 INFO TaskSetManager: Starting task 3.0 in stage 1.3 (TID 1084, hadoop-slave1, executor 8, partition 4, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:05 INFO TaskSetManager: Finished task 64.0 in stage 0.4 (TID 1069) in 5082 ms on hadoop-slave1 (executor 8) (57/75)
18/02/14 13:36:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on hadoop-slave1:41387 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:36:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave1:41387 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:36:05 INFO TaskSetManager: Starting task 5.0 in stage 1.3 (TID 1085, hadoop-slave1, executor 8, partition 7, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:05 INFO TaskSetManager: Finished task 61.0 in stage 0.4 (TID 1067) in 5398 ms on hadoop-slave1 (executor 8) (58/75)
18/02/14 13:36:05 INFO TaskSetManager: Starting task 8.0 in stage 1.3 (TID 1086, hadoop-slave5, executor 7, partition 14, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:05 INFO TaskSetManager: Finished task 58.0 in stage 0.4 (TID 1064) in 5894 ms on hadoop-slave5 (executor 7) (59/75)
18/02/14 13:36:05 INFO TaskSetManager: Starting task 6.0 in stage 1.3 (TID 1087, hadoop-slave3, executor 5, partition 11, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:05 INFO TaskSetManager: Finished task 67.0 in stage 0.4 (TID 1072) in 4001 ms on hadoop-slave3 (executor 5) (60/75)
18/02/14 13:36:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on hadoop-slave3:33203 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:36:07 INFO TaskSetManager: Starting task 9.0 in stage 1.3 (TID 1088, hadoop-slave1, executor 8, partition 15, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:07 INFO TaskSetManager: Finished task 60.0 in stage 0.4 (TID 1066) in 7026 ms on hadoop-slave1 (executor 8) (61/75)
18/02/14 13:36:07 INFO TaskSetManager: Finished task 69.0 in stage 0.4 (TID 1075) in 4823 ms on hadoop-slave3 (executor 5) (62/75)
18/02/14 13:36:07 INFO TaskSetManager: Starting task 7.0 in stage 1.3 (TID 1089, hadoop-slave3, executor 5, partition 13, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:07 INFO TaskSetManager: Starting task 11.0 in stage 1.3 (TID 1090, hadoop-slave5, executor 7, partition 21, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:07 INFO TaskSetManager: Finished task 70.0 in stage 0.4 (TID 1073) in 5801 ms on hadoop-slave5 (executor 7) (63/75)
18/02/14 13:36:07 INFO TaskSetManager: Starting task 13.0 in stage 1.3 (TID 1091, hadoop-slave5, executor 7, partition 26, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:07 INFO TaskSetManager: Finished task 62.0 in stage 0.4 (TID 1065) in 7847 ms on hadoop-slave5 (executor 7) (64/75)
18/02/14 13:36:07 INFO TaskSetManager: Starting task 12.0 in stage 1.3 (TID 1092, hadoop-slave2, executor 6, partition 23, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:07 INFO TaskSetManager: Finished task 68.0 in stage 0.4 (TID 1074) in 5944 ms on hadoop-slave2 (executor 6) (65/75)
18/02/14 13:36:07 INFO TaskSetManager: Starting task 21.0 in stage 1.3 (TID 1093, hadoop-slave5, executor 7, partition 39, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:07 INFO TaskSetManager: Finished task 56.0 in stage 0.4 (TID 1062) in 8963 ms on hadoop-slave5 (executor 7) (66/75)
18/02/14 13:36:08 INFO TaskSetManager: Starting task 10.0 in stage 1.3 (TID 1094, hadoop-slave3, executor 5, partition 18, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:08 INFO TaskSetManager: Finished task 71.0 in stage 0.4 (TID 1077) in 5076 ms on hadoop-slave3 (executor 5) (67/75)
18/02/14 13:36:08 INFO TaskSetManager: Starting task 14.0 in stage 1.3 (TID 1095, hadoop-slave2, executor 6, partition 27, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:08 INFO TaskSetManager: Finished task 73.0 in stage 0.4 (TID 1076) in 5747 ms on hadoop-slave2 (executor 6) (68/75)
18/02/14 13:36:08 INFO TaskSetManager: Starting task 16.0 in stage 1.3 (TID 1096, hadoop-slave2, executor 6, partition 30, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:08 INFO TaskSetManager: Finished task 74.0 in stage 0.4 (TID 1078) in 5352 ms on hadoop-slave2 (executor 6) (69/75)
18/02/14 13:36:09 INFO TaskSetManager: Starting task 15.0 in stage 1.3 (TID 1097, hadoop-slave1, executor 8, partition 29, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:09 INFO TaskSetManager: Finished task 65.0 in stage 0.4 (TID 1070) in 8000 ms on hadoop-slave1 (executor 8) (70/75)
18/02/14 13:36:09 INFO TaskSetManager: Starting task 17.0 in stage 1.3 (TID 1098, hadoop-slave1, executor 8, partition 32, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:09 INFO TaskSetManager: Finished task 66.0 in stage 0.4 (TID 1071) in 7857 ms on hadoop-slave1 (executor 8) (71/75)
18/02/14 13:36:09 INFO TaskSetManager: Starting task 18.0 in stage 1.3 (TID 1099, hadoop-slave1, executor 8, partition 33, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:09 INFO TaskSetManager: Finished task 63.0 in stage 0.4 (TID 1068) in 9313 ms on hadoop-slave1 (executor 8) (72/75)
18/02/14 13:36:09 INFO TaskSetManager: Starting task 20.0 in stage 1.3 (TID 1100, hadoop-slave3, executor 5, partition 37, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:09 INFO TaskSetManager: Finished task 59.0 in stage 0.4 (TID 1061) in 11087 ms on hadoop-slave3 (executor 5) (73/75)
18/02/14 13:36:10 INFO TaskSetManager: Starting task 22.0 in stage 1.3 (TID 1101, hadoop-slave3, executor 5, partition 42, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:10 INFO TaskSetManager: Finished task 53.0 in stage 0.4 (TID 1059) in 11734 ms on hadoop-slave3 (executor 5) (74/75)
18/02/14 13:36:10 INFO TaskSetManager: Starting task 19.0 in stage 1.3 (TID 1102, hadoop-slave2, executor 6, partition 36, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:10 INFO TaskSetManager: Finished task 0.0 in stage 1.3 (TID 1080) in 6518 ms on hadoop-slave2 (executor 6) (1/94)
18/02/14 13:36:11 INFO TaskSetManager: Starting task 23.0 in stage 1.3 (TID 1103, hadoop-slave3, executor 5, partition 43, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:11 INFO TaskSetManager: Finished task 72.0 in stage 0.4 (TID 1079) in 7284 ms on hadoop-slave3 (executor 5) (75/75)
18/02/14 13:36:11 INFO YarnScheduler: Removed TaskSet 0.4, whose tasks have all completed, from pool 
18/02/14 13:36:11 INFO DAGScheduler: ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) finished in 28.241 s
18/02/14 13:36:11 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:36:11 INFO DAGScheduler: running: Set(ShuffleMapStage 1, ShuffleMapStage 3)
18/02/14 13:36:11 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:36:11 INFO DAGScheduler: failed: Set()
18/02/14 13:36:11 INFO TaskSetManager: Starting task 28.0 in stage 1.3 (TID 1104, hadoop-slave2, executor 6, partition 55, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:11 INFO TaskSetManager: Finished task 4.0 in stage 1.3 (TID 1082) in 6597 ms on hadoop-slave2 (executor 6) (2/94)
18/02/14 13:36:11 INFO TaskSetManager: Starting task 24.0 in stage 1.3 (TID 1105, hadoop-slave3, executor 5, partition 46, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:11 INFO TaskSetManager: Finished task 7.0 in stage 1.3 (TID 1089) in 4363 ms on hadoop-slave3 (executor 5) (3/94)
18/02/14 13:36:16 INFO TaskSetManager: Starting task 25.0 in stage 1.3 (TID 1106, hadoop-slave1, executor 8, partition 47, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:16 INFO TaskSetManager: Finished task 3.0 in stage 1.3 (TID 1084) in 11265 ms on hadoop-slave1 (executor 8) (4/94)
18/02/14 13:36:17 INFO TaskSetManager: Starting task 26.0 in stage 1.3 (TID 1107, hadoop-slave5, executor 7, partition 49, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:17 INFO TaskSetManager: Finished task 8.0 in stage 1.3 (TID 1086) in 11277 ms on hadoop-slave5 (executor 7) (5/94)
18/02/14 13:36:17 INFO TaskSetManager: Starting task 27.0 in stage 1.3 (TID 1108, hadoop-slave1, executor 8, partition 52, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:17 INFO TaskSetManager: Finished task 5.0 in stage 1.3 (TID 1085) in 11735 ms on hadoop-slave1 (executor 8) (6/94)
18/02/14 13:36:18 INFO TaskSetManager: Starting task 29.0 in stage 1.3 (TID 1109, hadoop-slave2, executor 6, partition 56, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:18 INFO TaskSetManager: Finished task 12.0 in stage 1.3 (TID 1092) in 10698 ms on hadoop-slave2 (executor 6) (7/94)
18/02/14 13:36:19 INFO TaskSetManager: Starting task 32.0 in stage 1.3 (TID 1110, hadoop-slave2, executor 6, partition 62, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:19 INFO TaskSetManager: Finished task 19.0 in stage 1.3 (TID 1102) in 9190 ms on hadoop-slave2 (executor 6) (8/94)
18/02/14 13:36:20 INFO TaskSetManager: Starting task 33.0 in stage 1.3 (TID 1111, hadoop-slave2, executor 6, partition 63, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:20 INFO TaskSetManager: Finished task 16.0 in stage 1.3 (TID 1096) in 11246 ms on hadoop-slave2 (executor 6) (9/94)
18/02/14 13:36:20 INFO TaskSetManager: Starting task 31.0 in stage 1.3 (TID 1112, hadoop-slave1, executor 8, partition 60, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:20 INFO TaskSetManager: Finished task 17.0 in stage 1.3 (TID 1098) in 11110 ms on hadoop-slave1 (executor 8) (10/94)
18/02/14 13:36:21 INFO TaskSetManager: Starting task 34.0 in stage 1.3 (TID 1113, hadoop-slave1, executor 8, partition 66, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:21 INFO TaskSetManager: Finished task 15.0 in stage 1.3 (TID 1097) in 11958 ms on hadoop-slave1 (executor 8) (11/94)
18/02/14 13:36:22 INFO TaskSetManager: Starting task 30.0 in stage 1.3 (TID 1114, hadoop-slave3, executor 5, partition 59, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:22 INFO TaskSetManager: Finished task 20.0 in stage 1.3 (TID 1100) in 12238 ms on hadoop-slave3 (executor 5) (12/94)
18/02/14 13:36:22 INFO TaskSetManager: Starting task 36.0 in stage 1.3 (TID 1115, hadoop-slave1, executor 8, partition 74, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:22 INFO TaskSetManager: Finished task 18.0 in stage 1.3 (TID 1099) in 12506 ms on hadoop-slave1 (executor 8) (13/94)
18/02/14 13:36:22 INFO TaskSetManager: Starting task 35.0 in stage 1.3 (TID 1116, hadoop-slave3, executor 5, partition 68, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:22 INFO TaskSetManager: Finished task 22.0 in stage 1.3 (TID 1101) in 11919 ms on hadoop-slave3 (executor 5) (14/94)
18/02/14 13:36:23 INFO TaskSetManager: Starting task 37.0 in stage 1.3 (TID 1117, hadoop-slave3, executor 5, partition 75, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:23 INFO TaskSetManager: Finished task 6.0 in stage 1.3 (TID 1087) in 17242 ms on hadoop-slave3 (executor 5) (15/94)
18/02/14 13:36:23 INFO TaskSetManager: Starting task 42.0 in stage 1.3 (TID 1118, hadoop-slave3, executor 5, partition 85, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:23 INFO TaskSetManager: Finished task 23.0 in stage 1.3 (TID 1103) in 12720 ms on hadoop-slave3 (executor 5) (16/94)
18/02/14 13:36:24 INFO TaskSetManager: Starting task 38.0 in stage 1.3 (TID 1119, hadoop-slave1, executor 8, partition 76, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:24 INFO TaskSetManager: Finished task 9.0 in stage 1.3 (TID 1088) in 16868 ms on hadoop-slave1 (executor 8) (17/94)
18/02/14 13:36:24 INFO TaskSetManager: Starting task 46.0 in stage 1.3 (TID 1120, hadoop-slave5, executor 7, partition 101, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:24 INFO TaskSetManager: Finished task 11.0 in stage 1.3 (TID 1090) in 16587 ms on hadoop-slave5 (executor 7) (18/94)
18/02/14 13:36:24 INFO TaskSetManager: Starting task 39.0 in stage 1.3 (TID 1121, hadoop-slave2, executor 6, partition 79, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:24 INFO TaskSetManager: Finished task 28.0 in stage 1.3 (TID 1104) in 13030 ms on hadoop-slave2 (executor 6) (19/94)
18/02/14 13:36:24 INFO TaskSetManager: Starting task 40.0 in stage 1.3 (TID 1122, hadoop-slave2, executor 6, partition 82, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:24 INFO TaskSetManager: Finished task 14.0 in stage 1.3 (TID 1095) in 15998 ms on hadoop-slave2 (executor 6) (20/94)
18/02/14 13:36:24 INFO TaskSetManager: Starting task 50.0 in stage 1.3 (TID 1123, hadoop-slave5, executor 7, partition 109, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:24 INFO TaskSetManager: Finished task 21.0 in stage 1.3 (TID 1093) in 16920 ms on hadoop-slave5 (executor 7) (21/94)
18/02/14 13:36:25 INFO TaskSetManager: Starting task 43.0 in stage 1.3 (TID 1124, hadoop-slave3, executor 5, partition 86, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:25 INFO TaskSetManager: Finished task 24.0 in stage 1.3 (TID 1105) in 14346 ms on hadoop-slave3 (executor 5) (22/94)
18/02/14 13:36:26 INFO TaskSetManager: Starting task 41.0 in stage 1.3 (TID 1125, hadoop-slave1, executor 8, partition 83, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:26 INFO TaskSetManager: Finished task 27.0 in stage 1.3 (TID 1108) in 9358 ms on hadoop-slave1 (executor 8) (23/94)
18/02/14 13:36:26 INFO TaskSetManager: Starting task 45.0 in stage 1.3 (TID 1126, hadoop-slave2, executor 6, partition 98, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:26 INFO TaskSetManager: Finished task 32.0 in stage 1.3 (TID 1110) in 7069 ms on hadoop-slave2 (executor 6) (24/94)
18/02/14 13:36:26 INFO TaskSetManager: Starting task 51.0 in stage 1.3 (TID 1127, hadoop-slave5, executor 7, partition 110, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:26 INFO TaskSetManager: Finished task 13.0 in stage 1.3 (TID 1091) in 19069 ms on hadoop-slave5 (executor 7) (25/94)
18/02/14 13:36:28 INFO TaskSetManager: Starting task 54.0 in stage 1.3 (TID 1128, hadoop-slave5, executor 7, partition 114, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:28 INFO TaskSetManager: Finished task 1.0 in stage 1.3 (TID 1081) in 23835 ms on hadoop-slave5 (executor 7) (26/94)
18/02/14 13:36:28 INFO TaskSetManager: Starting task 44.0 in stage 1.3 (TID 1129, hadoop-slave3, executor 5, partition 97, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:28 INFO TaskSetManager: Finished task 10.0 in stage 1.3 (TID 1094) in 19973 ms on hadoop-slave3 (executor 5) (27/94)
18/02/14 13:36:29 INFO TaskSetManager: Starting task 47.0 in stage 1.3 (TID 1130, hadoop-slave1, executor 8, partition 104, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:29 INFO TaskSetManager: Finished task 31.0 in stage 1.3 (TID 1112) in 8364 ms on hadoop-slave1 (executor 8) (28/94)
18/02/14 13:36:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave1:41387 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:36:29 INFO TaskSetManager: Starting task 55.0 in stage 1.3 (TID 1131, hadoop-slave5, executor 7, partition 115, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:29 INFO TaskSetManager: Finished task 2.0 in stage 1.3 (TID 1083) in 23693 ms on hadoop-slave5 (executor 7) (29/94)
18/02/14 13:36:29 INFO TaskSetManager: Starting task 57.0 in stage 1.3 (TID 1132, hadoop-slave5, executor 7, partition 121, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:29 INFO TaskSetManager: Finished task 46.0 in stage 1.3 (TID 1120) in 4932 ms on hadoop-slave5 (executor 7) (30/94)
18/02/14 13:36:29 INFO TaskSetManager: Starting task 48.0 in stage 1.3 (TID 1133, hadoop-slave1, executor 8, partition 105, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:29 INFO TaskSetManager: Finished task 25.0 in stage 1.3 (TID 1106) in 12417 ms on hadoop-slave1 (executor 8) (31/94)
18/02/14 13:36:30 INFO TaskSetManager: Starting task 49.0 in stage 1.3 (TID 1134, hadoop-slave2, executor 6, partition 107, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:30 INFO TaskSetManager: Finished task 29.0 in stage 1.3 (TID 1109) in 11715 ms on hadoop-slave2 (executor 6) (32/94)
18/02/14 13:36:31 INFO TaskSetManager: Starting task 53.0 in stage 1.3 (TID 1135, hadoop-slave2, executor 6, partition 112, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:31 INFO TaskSetManager: Finished task 33.0 in stage 1.3 (TID 1111) in 11562 ms on hadoop-slave2 (executor 6) (33/94)
18/02/14 13:36:32 INFO TaskSetManager: Starting task 58.0 in stage 1.3 (TID 1136, hadoop-slave2, executor 6, partition 123, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:32 INFO TaskSetManager: Finished task 39.0 in stage 1.3 (TID 1121) in 8380 ms on hadoop-slave2 (executor 6) (34/94)
18/02/14 13:36:35 INFO TaskSetManager: Starting task 52.0 in stage 1.3 (TID 1137, hadoop-slave1, executor 8, partition 111, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:35 INFO TaskSetManager: Finished task 34.0 in stage 1.3 (TID 1113) in 14046 ms on hadoop-slave1 (executor 8) (35/94)
18/02/14 13:36:35 INFO TaskSetManager: Starting task 56.0 in stage 1.3 (TID 1138, hadoop-slave1, executor 8, partition 118, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:35 INFO TaskSetManager: Finished task 36.0 in stage 1.3 (TID 1115) in 13273 ms on hadoop-slave1 (executor 8) (36/94)
18/02/14 13:36:36 INFO TaskSetManager: Starting task 61.0 in stage 1.3 (TID 1139, hadoop-slave3, executor 5, partition 129, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:36 INFO TaskSetManager: Finished task 43.0 in stage 1.3 (TID 1124) in 10168 ms on hadoop-slave3 (executor 5) (37/94)
18/02/14 13:36:37 INFO TaskSetManager: Starting task 60.0 in stage 1.3 (TID 1140, hadoop-slave2, executor 6, partition 127, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:37 INFO TaskSetManager: Finished task 58.0 in stage 1.3 (TID 1136) in 4426 ms on hadoop-slave2 (executor 6) (38/94)
18/02/14 13:36:37 INFO TaskSetManager: Starting task 62.0 in stage 1.3 (TID 1141, hadoop-slave2, executor 6, partition 130, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:37 INFO TaskSetManager: Finished task 40.0 in stage 1.3 (TID 1122) in 12376 ms on hadoop-slave2 (executor 6) (39/94)
18/02/14 13:36:38 INFO TaskSetManager: Starting task 59.0 in stage 1.3 (TID 1142, hadoop-slave1, executor 8, partition 126, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:38 INFO TaskSetManager: Finished task 47.0 in stage 1.3 (TID 1130) in 9270 ms on hadoop-slave1 (executor 8) (40/94)
18/02/14 13:36:38 INFO TaskSetManager: Starting task 63.0 in stage 1.3 (TID 1143, hadoop-slave3, executor 5, partition 132, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:38 INFO TaskSetManager: Finished task 42.0 in stage 1.3 (TID 1118) in 14687 ms on hadoop-slave3 (executor 5) (41/94)
18/02/14 13:36:38 INFO TaskSetManager: Starting task 64.0 in stage 1.3 (TID 1144, hadoop-slave3, executor 5, partition 133, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:38 INFO TaskSetManager: Finished task 37.0 in stage 1.3 (TID 1117) in 15542 ms on hadoop-slave3 (executor 5) (42/94)
18/02/14 13:36:41 INFO TaskSetManager: Starting task 65.0 in stage 1.3 (TID 1145, hadoop-slave2, executor 6, partition 136, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:41 INFO TaskSetManager: Finished task 45.0 in stage 1.3 (TID 1126) in 14137 ms on hadoop-slave2 (executor 6) (43/94)
18/02/14 13:36:42 INFO TaskSetManager: Starting task 72.0 in stage 1.3 (TID 1146, hadoop-slave2, executor 6, partition 155, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:42 INFO TaskSetManager: Finished task 49.0 in stage 1.3 (TID 1134) in 11733 ms on hadoop-slave2 (executor 6) (44/94)
18/02/14 13:36:43 INFO TaskSetManager: Starting task 73.0 in stage 1.3 (TID 1147, hadoop-slave2, executor 6, partition 156, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:43 INFO TaskSetManager: Finished task 62.0 in stage 1.3 (TID 1141) in 5999 ms on hadoop-slave2 (executor 6) (45/94)
18/02/14 13:36:44 INFO TaskSetManager: Starting task 66.0 in stage 1.3 (TID 1148, hadoop-slave1, executor 8, partition 137, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:44 INFO TaskSetManager: Finished task 56.0 in stage 1.3 (TID 1138) in 8504 ms on hadoop-slave1 (executor 8) (46/94)
18/02/14 13:36:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.19.3.36:33535 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:36:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on hadoop-slave2:33289 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:36:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on hadoop-slave5:35799 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:36:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on hadoop-slave3:33203 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:36:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on hadoop-slave1:41387 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:36:44 INFO TaskSetManager: Starting task 67.0 in stage 1.3 (TID 1149, hadoop-slave1, executor 8, partition 142, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:44 INFO TaskSetManager: Finished task 52.0 in stage 1.3 (TID 1137) in 9005 ms on hadoop-slave1 (executor 8) (47/94)
18/02/14 13:36:44 INFO TaskSetManager: Starting task 68.0 in stage 1.3 (TID 1150, hadoop-slave3, executor 5, partition 143, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:44 INFO TaskSetManager: Finished task 30.0 in stage 1.3 (TID 1114) in 22740 ms on hadoop-slave3 (executor 5) (48/94)
18/02/14 13:36:45 INFO TaskSetManager: Starting task 69.0 in stage 1.3 (TID 1151, hadoop-slave3, executor 5, partition 145, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:45 INFO TaskSetManager: Finished task 61.0 in stage 1.3 (TID 1139) in 8855 ms on hadoop-slave3 (executor 5) (49/94)
18/02/14 13:36:45 INFO TaskSetManager: Starting task 71.0 in stage 1.3 (TID 1152, hadoop-slave5, executor 7, partition 152, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:45 INFO TaskSetManager: Finished task 26.0 in stage 1.3 (TID 1107) in 28310 ms on hadoop-slave5 (executor 7) (50/94)
18/02/14 13:36:45 INFO TaskSetManager: Starting task 70.0 in stage 1.3 (TID 1153, hadoop-slave1, executor 8, partition 147, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:45 INFO TaskSetManager: Finished task 48.0 in stage 1.3 (TID 1133) in 16619 ms on hadoop-slave1 (executor 8) (51/94)
18/02/14 13:36:46 INFO TaskSetManager: Starting task 74.0 in stage 1.3 (TID 1154, hadoop-slave2, executor 6, partition 157, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:46 INFO TaskSetManager: Finished task 53.0 in stage 1.3 (TID 1135) in 14907 ms on hadoop-slave2 (executor 6) (52/94)
18/02/14 13:36:46 INFO TaskSetManager: Starting task 75.0 in stage 1.3 (TID 1155, hadoop-slave3, executor 5, partition 158, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:46 INFO TaskSetManager: Finished task 64.0 in stage 1.3 (TID 1144) in 8185 ms on hadoop-slave3 (executor 5) (53/94)
18/02/14 13:36:46 INFO TaskSetManager: Starting task 76.0 in stage 1.3 (TID 1156, hadoop-slave3, executor 5, partition 160, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:46 INFO TaskSetManager: Finished task 63.0 in stage 1.3 (TID 1143) in 8475 ms on hadoop-slave3 (executor 5) (54/94)
18/02/14 13:36:48 INFO TaskSetManager: Starting task 77.0 in stage 1.3 (TID 1157, hadoop-slave2, executor 6, partition 163, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:48 INFO TaskSetManager: Finished task 65.0 in stage 1.3 (TID 1145) in 6982 ms on hadoop-slave2 (executor 6) (55/94)
18/02/14 13:36:48 INFO TaskSetManager: Starting task 78.0 in stage 1.3 (TID 1158, hadoop-slave3, executor 5, partition 166, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:48 INFO TaskSetManager: Finished task 35.0 in stage 1.3 (TID 1116) in 26092 ms on hadoop-slave3 (executor 5) (56/94)
18/02/14 13:36:49 INFO TaskSetManager: Starting task 81.0 in stage 1.3 (TID 1159, hadoop-slave2, executor 6, partition 176, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:49 INFO TaskSetManager: Finished task 60.0 in stage 1.3 (TID 1140) in 12032 ms on hadoop-slave2 (executor 6) (57/94)
18/02/14 13:36:50 INFO TaskSetManager: Starting task 79.0 in stage 1.3 (TID 1160, hadoop-slave5, executor 7, partition 174, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:50 INFO TaskSetManager: Finished task 55.0 in stage 1.3 (TID 1131) in 20998 ms on hadoop-slave5 (executor 7) (58/94)
18/02/14 13:36:50 INFO TaskSetManager: Starting task 80.0 in stage 1.3 (TID 1161, hadoop-slave3, executor 5, partition 175, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:50 INFO TaskSetManager: Finished task 44.0 in stage 1.3 (TID 1129) in 22108 ms on hadoop-slave3 (executor 5) (59/94)
18/02/14 13:36:50 INFO TaskSetManager: Starting task 85.0 in stage 1.3 (TID 1162, hadoop-slave3, executor 5, partition 185, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:50 INFO TaskSetManager: Finished task 68.0 in stage 1.3 (TID 1150) in 5885 ms on hadoop-slave3 (executor 5) (60/94)
18/02/14 13:36:51 INFO TaskSetManager: Starting task 82.0 in stage 1.3 (TID 1163, hadoop-slave1, executor 8, partition 177, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:51 INFO TaskSetManager: Finished task 59.0 in stage 1.3 (TID 1142) in 12800 ms on hadoop-slave1 (executor 8) (61/94)
18/02/14 13:36:52 INFO TaskSetManager: Starting task 83.0 in stage 1.3 (TID 1164, hadoop-slave1, executor 8, partition 178, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:52 INFO TaskSetManager: Finished task 70.0 in stage 1.3 (TID 1153) in 6629 ms on hadoop-slave1 (executor 8) (62/94)
18/02/14 13:36:52 INFO TaskSetManager: Starting task 90.0 in stage 1.3 (TID 1165, hadoop-slave5, executor 7, partition 194, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:52 INFO TaskSetManager: Finished task 54.0 in stage 1.3 (TID 1128) in 24321 ms on hadoop-slave5 (executor 7) (63/94)
18/02/14 13:36:53 INFO TaskSetManager: Starting task 91.0 in stage 1.3 (TID 1166, hadoop-slave5, executor 7, partition 195, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:53 INFO TaskSetManager: Finished task 57.0 in stage 1.3 (TID 1132) in 23969 ms on hadoop-slave5 (executor 7) (64/94)
18/02/14 13:36:53 INFO TaskSetManager: Starting task 84.0 in stage 1.3 (TID 1167, hadoop-slave1, executor 8, partition 183, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:53 INFO TaskSetManager: Finished task 38.0 in stage 1.3 (TID 1119) in 29330 ms on hadoop-slave1 (executor 8) (65/94)
18/02/14 13:36:54 INFO TaskSetManager: Starting task 95.1 in stage 3.0 (TID 1168, hadoop-slave5, executor 7, partition 95, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:54 INFO TaskSetManager: Finished task 51.0 in stage 1.3 (TID 1127) in 27276 ms on hadoop-slave5 (executor 7) (66/94)
18/02/14 13:36:54 INFO TaskSetManager: Starting task 87.0 in stage 1.3 (TID 1169, hadoop-slave2, executor 6, partition 189, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:54 INFO TaskSetManager: Finished task 72.0 in stage 1.3 (TID 1146) in 12733 ms on hadoop-slave2 (executor 6) (67/94)
18/02/14 13:36:56 INFO TaskSetManager: Starting task 88.0 in stage 1.3 (TID 1170, hadoop-slave2, executor 6, partition 191, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:56 INFO TaskSetManager: Finished task 77.0 in stage 1.3 (TID 1157) in 8522 ms on hadoop-slave2 (executor 6) (68/94)
18/02/14 13:36:58 INFO TaskSetManager: Starting task 89.0 in stage 1.3 (TID 1171, hadoop-slave2, executor 6, partition 193, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:58 INFO TaskSetManager: Finished task 74.0 in stage 1.3 (TID 1154) in 11469 ms on hadoop-slave2 (executor 6) (69/94)
18/02/14 13:36:58 INFO TaskSetManager: Starting task 126.1 in stage 3.0 (TID 1172, hadoop-slave5, executor 7, partition 126, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:58 INFO TaskSetManager: Finished task 79.0 in stage 1.3 (TID 1160) in 8746 ms on hadoop-slave5 (executor 7) (70/94)
18/02/14 13:36:59 INFO TaskSetManager: Starting task 121.1 in stage 3.0 (TID 1173, hadoop-slave5, executor 7, partition 121, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:59 INFO TaskSetManager: Finished task 50.0 in stage 1.3 (TID 1123) in 34091 ms on hadoop-slave5 (executor 7) (71/94)
18/02/14 13:36:59 INFO TaskSetManager: Starting task 93.0 in stage 1.3 (TID 1174, hadoop-slave2, executor 6, partition 198, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:59 INFO TaskSetManager: Finished task 81.0 in stage 1.3 (TID 1159) in 10149 ms on hadoop-slave2 (executor 6) (72/94)
18/02/14 13:36:59 INFO TaskSetManager: Starting task 86.0 in stage 1.3 (TID 1175, hadoop-slave1, executor 8, partition 186, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:59 INFO TaskSetManager: Finished task 41.0 in stage 1.3 (TID 1125) in 32947 ms on hadoop-slave1 (executor 8) (73/94)
18/02/14 13:36:59 INFO TaskSetManager: Starting task 112.1 in stage 3.0 (TID 1176, hadoop-slave2, executor 6, partition 112, NODE_LOCAL, 6941 bytes)
18/02/14 13:36:59 INFO TaskSetManager: Finished task 73.0 in stage 1.3 (TID 1147) in 16576 ms on hadoop-slave2 (executor 6) (74/94)
18/02/14 13:37:01 INFO TaskSetManager: Starting task 74.2 in stage 3.0 (TID 1177, hadoop-slave5, executor 7, partition 74, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:01 INFO TaskSetManager: Finished task 71.0 in stage 1.3 (TID 1152) in 15959 ms on hadoop-slave5 (executor 7) (75/94)
18/02/14 13:37:04 INFO TaskSetManager: Starting task 92.0 in stage 1.3 (TID 1178, hadoop-slave5, executor 7, partition 197, RACK_LOCAL, 6941 bytes)
18/02/14 13:37:04 INFO TaskSetManager: Finished task 126.1 in stage 3.0 (TID 1172) in 5157 ms on hadoop-slave5 (executor 7) (117/200)
18/02/14 13:37:04 INFO TaskSetManager: Starting task 9.1 in stage 3.0 (TID 1179, hadoop-slave5, executor 7, partition 9, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:04 INFO TaskSetManager: Finished task 95.1 in stage 3.0 (TID 1168) in 9988 ms on hadoop-slave5 (executor 7) (118/200)
18/02/14 13:37:04 INFO TaskSetManager: Starting task 10.2 in stage 3.0 (TID 1180, hadoop-slave5, executor 7, partition 10, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:04 INFO TaskSetManager: Finished task 121.1 in stage 3.0 (TID 1173) in 5616 ms on hadoop-slave5 (executor 7) (119/200)
18/02/14 13:37:05 INFO TaskSetManager: Starting task 21.1 in stage 3.0 (TID 1181, hadoop-slave5, executor 7, partition 21, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:05 INFO TaskSetManager: Finished task 90.0 in stage 1.3 (TID 1165) in 12368 ms on hadoop-slave5 (executor 7) (76/94)
18/02/14 13:37:06 INFO TaskSetManager: Starting task 60.1 in stage 3.0 (TID 1182, hadoop-slave5, executor 7, partition 60, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:06 INFO TaskSetManager: Finished task 74.2 in stage 3.0 (TID 1177) in 4675 ms on hadoop-slave5 (executor 7) (120/200)
18/02/14 13:37:06 INFO TaskSetManager: Starting task 87.1 in stage 3.0 (TID 1183, hadoop-slave2, executor 6, partition 87, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:06 INFO TaskSetManager: Starting task 130.1 in stage 3.0 (TID 1184, hadoop-slave2, executor 6, partition 130, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:06 INFO TaskSetManager: Finished task 112.1 in stage 3.0 (TID 1176) in 6533 ms on hadoop-slave2 (executor 6) (121/200)
18/02/14 13:37:06 INFO TaskSetManager: Finished task 93.0 in stage 1.3 (TID 1174) in 6948 ms on hadoop-slave2 (executor 6) (77/94)
18/02/14 13:37:06 INFO TaskSetManager: Starting task 78.2 in stage 3.0 (TID 1185, hadoop-slave5, executor 7, partition 78, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:06 INFO TaskSetManager: Finished task 91.0 in stage 1.3 (TID 1166) in 13328 ms on hadoop-slave5 (executor 7) (78/94)
18/02/14 13:37:06 INFO TaskSetManager: Starting task 136.1 in stage 3.0 (TID 1186, hadoop-slave2, executor 6, partition 136, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:06 INFO TaskSetManager: Finished task 87.0 in stage 1.3 (TID 1169) in 11768 ms on hadoop-slave2 (executor 6) (79/94)
18/02/14 13:37:06 INFO TaskSetManager: Starting task 18.1 in stage 3.0 (TID 1187, hadoop-slave3, executor 5, partition 18, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:06 INFO TaskSetManager: Finished task 85.0 in stage 1.3 (TID 1162) in 15984 ms on hadoop-slave3 (executor 5) (80/94)
18/02/14 13:37:06 INFO TaskSetManager: Starting task 83.2 in stage 3.0 (TID 1188, hadoop-slave2, executor 6, partition 83, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:06 INFO TaskSetManager: Finished task 88.0 in stage 1.3 (TID 1170) in 10435 ms on hadoop-slave2 (executor 6) (81/94)
18/02/14 13:37:07 INFO TaskSetManager: Starting task 30.2 in stage 3.0 (TID 1189, hadoop-slave1, executor 8, partition 30, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:07 INFO TaskSetManager: Finished task 67.0 in stage 1.3 (TID 1149) in 23459 ms on hadoop-slave1 (executor 8) (82/94)
18/02/14 13:37:07 INFO TaskSetManager: Starting task 33.1 in stage 3.0 (TID 1190, hadoop-slave2, executor 6, partition 33, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:07 INFO TaskSetManager: Finished task 89.0 in stage 1.3 (TID 1171) in 9696 ms on hadoop-slave2 (executor 6) (83/94)
18/02/14 13:37:07 INFO TaskSetManager: Starting task 82.2 in stage 3.0 (TID 1191, hadoop-slave1, executor 8, partition 82, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:07 INFO TaskSetManager: Finished task 66.0 in stage 1.3 (TID 1148) in 23822 ms on hadoop-slave1 (executor 8) (84/94)
18/02/14 13:37:08 INFO TaskSetManager: Starting task 85.1 in stage 3.0 (TID 1192, hadoop-slave3, executor 5, partition 85, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:08 INFO TaskSetManager: Finished task 78.0 in stage 1.3 (TID 1158) in 19904 ms on hadoop-slave3 (executor 5) (85/94)
18/02/14 13:37:08 INFO TaskSetManager: Starting task 29.2 in stage 3.0 (TID 1193, hadoop-slave3, executor 5, partition 29, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:08 INFO TaskSetManager: Finished task 69.0 in stage 1.3 (TID 1151) in 23925 ms on hadoop-slave3 (executor 5) (86/94)
18/02/14 13:37:09 INFO TaskSetManager: Starting task 14.1 in stage 3.0 (TID 1194, hadoop-slave5, executor 7, partition 14, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:09 INFO TaskSetManager: Finished task 10.2 in stage 3.0 (TID 1180) in 4878 ms on hadoop-slave5 (executor 7) (122/200)
18/02/14 13:37:09 INFO TaskSetManager: Starting task 118.1 in stage 3.0 (TID 1195, hadoop-slave3, executor 5, partition 118, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:09 INFO TaskSetManager: Finished task 80.0 in stage 1.3 (TID 1161) in 18973 ms on hadoop-slave3 (executor 5) (87/94)
18/02/14 13:37:09 INFO TaskSetManager: Starting task 1.1 in stage 3.0 (TID 1196, hadoop-slave5, executor 7, partition 1, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:09 INFO TaskSetManager: Finished task 21.1 in stage 3.0 (TID 1181) in 4772 ms on hadoop-slave5 (executor 7) (123/200)
18/02/14 13:37:10 INFO TaskSetManager: Starting task 66.1 in stage 3.0 (TID 1197, hadoop-slave3, executor 5, partition 66, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:10 INFO TaskSetManager: Finished task 75.0 in stage 1.3 (TID 1155) in 23863 ms on hadoop-slave3 (executor 5) (88/94)
18/02/14 13:37:10 INFO TaskSetManager: Starting task 36.2 in stage 3.0 (TID 1198, hadoop-slave2, executor 6, partition 36, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:10 INFO TaskSetManager: Finished task 130.1 in stage 3.0 (TID 1184) in 4380 ms on hadoop-slave2 (executor 6) (124/200)
18/02/14 13:37:10 INFO TaskSetManager: Starting task 75.1 in stage 3.0 (TID 1199, hadoop-slave3, executor 5, partition 75, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:10 INFO TaskSetManager: Finished task 76.0 in stage 1.3 (TID 1156) in 23885 ms on hadoop-slave3 (executor 5) (89/94)
18/02/14 13:37:11 INFO TaskSetManager: Starting task 26.1 in stage 3.0 (TID 1200, hadoop-slave5, executor 7, partition 26, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:11 INFO TaskSetManager: Finished task 9.1 in stage 3.0 (TID 1179) in 6873 ms on hadoop-slave5 (executor 7) (125/200)
18/02/14 13:37:11 INFO TaskSetManager: Starting task 93.2 in stage 3.0 (TID 1201, hadoop-slave1, executor 8, partition 93, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:11 INFO TaskSetManager: Finished task 84.0 in stage 1.3 (TID 1167) in 17752 ms on hadoop-slave1 (executor 8) (90/94)
18/02/14 13:37:12 INFO TaskSetManager: Starting task 55.2 in stage 3.0 (TID 1202, hadoop-slave2, executor 6, partition 55, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:12 INFO TaskSetManager: Finished task 136.1 in stage 3.0 (TID 1186) in 5617 ms on hadoop-slave2 (executor 6) (126/200)
18/02/14 13:37:12 INFO TaskSetManager: Starting task 49.1 in stage 3.0 (TID 1203, hadoop-slave5, executor 7, partition 49, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:12 INFO TaskSetManager: Finished task 60.1 in stage 3.0 (TID 1182) in 6359 ms on hadoop-slave5 (executor 7) (127/200)
18/02/14 13:37:12 INFO TaskSetManager: Starting task 12.2 in stage 3.0 (TID 1204, hadoop-slave2, executor 6, partition 12, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:12 INFO TaskSetManager: Finished task 33.1 in stage 3.0 (TID 1190) in 4896 ms on hadoop-slave2 (executor 6) (128/200)
18/02/14 13:37:13 INFO TaskSetManager: Starting task 79.2 in stage 3.0 (TID 1205, hadoop-slave2, executor 6, partition 79, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:13 INFO TaskSetManager: Finished task 87.1 in stage 3.0 (TID 1183) in 7253 ms on hadoop-slave2 (executor 6) (129/200)
18/02/14 13:37:13 INFO TaskSetManager: Starting task 77.1 in stage 3.0 (TID 1206, hadoop-slave2, executor 6, partition 77, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:13 INFO TaskSetManager: Finished task 83.2 in stage 3.0 (TID 1188) in 6712 ms on hadoop-slave2 (executor 6) (130/200)
18/02/14 13:37:13 INFO TaskSetManager: Starting task 52.1 in stage 3.0 (TID 1207, hadoop-slave5, executor 7, partition 52, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:13 INFO TaskSetManager: Finished task 78.2 in stage 3.0 (TID 1185) in 7270 ms on hadoop-slave5 (executor 7) (131/200)
18/02/14 13:37:13 INFO TaskSetManager: Starting task 45.1 in stage 3.0 (TID 1208, hadoop-slave5, executor 7, partition 45, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:13 INFO TaskSetManager: Finished task 1.1 in stage 3.0 (TID 1196) in 3962 ms on hadoop-slave5 (executor 7) (132/200)
18/02/14 13:37:14 INFO TaskSetManager: Starting task 159.0 in stage 3.0 (TID 1209, hadoop-slave5, executor 7, partition 159, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:14 INFO TaskSetManager: Finished task 14.1 in stage 3.0 (TID 1194) in 4664 ms on hadoop-slave5 (executor 7) (133/200)
18/02/14 13:37:14 INFO TaskSetManager: Starting task 86.1 in stage 3.0 (TID 1210, hadoop-slave3, executor 5, partition 86, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:14 INFO TaskSetManager: Finished task 18.1 in stage 3.0 (TID 1187) in 7881 ms on hadoop-slave3 (executor 5) (134/200)
18/02/14 13:37:14 INFO TaskSetManager: Starting task 160.0 in stage 3.0 (TID 1211, hadoop-slave5, executor 7, partition 160, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:14 INFO TaskSetManager: Finished task 92.0 in stage 1.3 (TID 1178) in 10598 ms on hadoop-slave5 (executor 7) (91/94)
18/02/14 13:37:15 INFO TaskSetManager: Starting task 163.0 in stage 3.0 (TID 1212, hadoop-slave5, executor 7, partition 163, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:15 INFO TaskSetManager: Finished task 26.1 in stage 3.0 (TID 1200) in 4639 ms on hadoop-slave5 (executor 7) (135/200)
18/02/14 13:37:15 INFO TaskSetManager: Starting task 4.2 in stage 3.0 (TID 1213, hadoop-slave3, executor 5, partition 4, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:15 INFO TaskSetManager: Finished task 85.1 in stage 3.0 (TID 1192) in 7521 ms on hadoop-slave3 (executor 5) (136/200)
18/02/14 13:37:16 INFO TaskSetManager: Starting task 89.1 in stage 3.0 (TID 1214, hadoop-slave2, executor 6, partition 89, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:16 INFO TaskSetManager: Finished task 36.2 in stage 3.0 (TID 1198) in 5287 ms on hadoop-slave2 (executor 6) (137/200)
18/02/14 13:37:16 INFO TaskSetManager: Starting task 123.1 in stage 3.0 (TID 1215, hadoop-slave3, executor 5, partition 123, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:16 INFO TaskSetManager: Finished task 29.2 in stage 3.0 (TID 1193) in 7141 ms on hadoop-slave3 (executor 5) (138/200)
18/02/14 13:37:16 INFO TaskSetManager: Starting task 91.2 in stage 3.0 (TID 1216, hadoop-slave1, executor 8, partition 91, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:16 INFO TaskSetManager: Finished task 30.2 in stage 3.0 (TID 1189) in 9013 ms on hadoop-slave1 (executor 8) (139/200)
18/02/14 13:37:16 INFO TaskSetManager: Starting task 156.0 in stage 3.0 (TID 1217, hadoop-slave3, executor 5, partition 156, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:16 INFO TaskSetManager: Finished task 118.1 in stage 3.0 (TID 1195) in 6979 ms on hadoop-slave3 (executor 5) (140/200)
18/02/14 13:37:16 INFO TaskSetManager: Starting task 98.1 in stage 3.0 (TID 1218, hadoop-slave1, executor 8, partition 98, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:16 INFO TaskSetManager: Finished task 82.0 in stage 1.3 (TID 1163) in 25577 ms on hadoop-slave1 (executor 8) (92/94)
18/02/14 13:37:16 INFO TaskSetManager: Starting task 157.0 in stage 3.0 (TID 1219, hadoop-slave1, executor 8, partition 157, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:16 INFO TaskSetManager: Finished task 82.2 in stage 3.0 (TID 1191) in 8801 ms on hadoop-slave1 (executor 8) (141/200)
18/02/14 13:37:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-slave1:41387 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:37:17 INFO TaskSetManager: Starting task 161.0 in stage 3.0 (TID 1220, hadoop-slave3, executor 5, partition 161, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:17 INFO TaskSetManager: Finished task 75.1 in stage 3.0 (TID 1199) in 6405 ms on hadoop-slave3 (executor 5) (142/200)
18/02/14 13:37:17 INFO TaskSetManager: Starting task 164.0 in stage 3.0 (TID 1221, hadoop-slave5, executor 7, partition 164, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:17 INFO TaskSetManager: Finished task 49.1 in stage 3.0 (TID 1203) in 4928 ms on hadoop-slave5 (executor 7) (143/200)
18/02/14 13:37:17 INFO TaskSetManager: Starting task 162.0 in stage 3.0 (TID 1222, hadoop-slave2, executor 6, partition 162, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:17 INFO TaskSetManager: Finished task 12.2 in stage 3.0 (TID 1204) in 4768 ms on hadoop-slave2 (executor 6) (144/200)
18/02/14 13:37:17 INFO TaskSetManager: Starting task 166.0 in stage 3.0 (TID 1223, hadoop-slave1, executor 8, partition 166, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:17 INFO TaskSetManager: Finished task 83.0 in stage 1.3 (TID 1164) in 25021 ms on hadoop-slave1 (executor 8) (93/94)
18/02/14 13:37:18 INFO TaskSetManager: Starting task 165.0 in stage 3.0 (TID 1224, hadoop-slave2, executor 6, partition 165, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:18 INFO TaskSetManager: Finished task 55.2 in stage 3.0 (TID 1202) in 5879 ms on hadoop-slave2 (executor 6) (145/200)
18/02/14 13:37:18 INFO TaskSetManager: Starting task 167.0 in stage 3.0 (TID 1225, hadoop-slave3, executor 5, partition 167, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:18 INFO TaskSetManager: Finished task 66.1 in stage 3.0 (TID 1197) in 7688 ms on hadoop-slave3 (executor 5) (146/200)
18/02/14 13:37:18 INFO TaskSetManager: Starting task 174.0 in stage 3.0 (TID 1226, hadoop-slave1, executor 8, partition 174, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:18 INFO TaskSetManager: Finished task 93.2 in stage 3.0 (TID 1201) in 7193 ms on hadoop-slave1 (executor 8) (147/200)
18/02/14 13:37:18 INFO TaskSetManager: Starting task 169.0 in stage 3.0 (TID 1227, hadoop-slave2, executor 6, partition 169, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:18 INFO TaskSetManager: Finished task 77.1 in stage 3.0 (TID 1206) in 5064 ms on hadoop-slave2 (executor 6) (148/200)
18/02/14 13:37:18 INFO TaskSetManager: Starting task 168.0 in stage 3.0 (TID 1228, hadoop-slave5, executor 7, partition 168, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:18 INFO TaskSetManager: Finished task 52.1 in stage 3.0 (TID 1207) in 5072 ms on hadoop-slave5 (executor 7) (149/200)
18/02/14 13:37:18 INFO TaskSetManager: Starting task 170.0 in stage 3.0 (TID 1229, hadoop-slave2, executor 6, partition 170, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:18 INFO TaskSetManager: Finished task 79.2 in stage 3.0 (TID 1205) in 5395 ms on hadoop-slave2 (executor 6) (150/200)
18/02/14 13:37:19 INFO TaskSetManager: Starting task 171.0 in stage 3.0 (TID 1230, hadoop-slave5, executor 7, partition 171, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:19 INFO TaskSetManager: Finished task 159.0 in stage 3.0 (TID 1209) in 4929 ms on hadoop-slave5 (executor 7) (151/200)
18/02/14 13:37:19 INFO TaskSetManager: Starting task 175.0 in stage 3.0 (TID 1231, hadoop-slave1, executor 8, partition 175, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:19 INFO TaskSetManager: Finished task 86.0 in stage 1.3 (TID 1175) in 19382 ms on hadoop-slave1 (executor 8) (94/94)
18/02/14 13:37:19 INFO YarnScheduler: Removed TaskSet 1.3, whose tasks have all completed, from pool 
18/02/14 13:37:19 INFO DAGScheduler: ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) finished in 96.410 s
18/02/14 13:37:19 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:37:19 INFO DAGScheduler: running: Set(ShuffleMapStage 3)
18/02/14 13:37:19 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:37:19 INFO DAGScheduler: failed: Set()
18/02/14 13:37:19 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:37:19 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 29.8 KB, free 4.1 GB)
18/02/14 13:37:19 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.0 KB, free 4.1 GB)
18/02/14 13:37:19 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.19.3.36:33535 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:37:19 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:996
18/02/14 13:37:19 INFO DAGScheduler: Submitting 30 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74)
18/02/14 13:37:19 INFO YarnScheduler: Adding task set 2.2 with 30 tasks
18/02/14 13:37:20 INFO TaskSetManager: Starting task 0.0 in stage 2.2 (TID 1232, hadoop-slave3, executor 5, partition 5, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:20 INFO TaskSetManager: Finished task 86.1 in stage 3.0 (TID 1210) in 5530 ms on hadoop-slave3 (executor 5) (152/200)
18/02/14 13:37:20 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on hadoop-slave3:33203 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:37:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.33:51972
18/02/14 13:37:20 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 630 bytes
18/02/14 13:37:20 INFO TaskSetManager: Starting task 1.0 in stage 2.2 (TID 1233, hadoop-slave5, executor 7, partition 9, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:20 INFO TaskSetManager: Finished task 160.0 in stage 3.0 (TID 1211) in 5617 ms on hadoop-slave5 (executor 7) (153/200)
18/02/14 13:37:20 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on hadoop-slave5:35799 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:37:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.31:55688
18/02/14 13:37:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.33:51972
18/02/14 13:37:20 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 833 bytes
18/02/14 13:37:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.31:55688
18/02/14 13:37:21 INFO TaskSetManager: Starting task 2.0 in stage 2.2 (TID 1234, hadoop-slave2, executor 6, partition 16, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:21 INFO TaskSetManager: Finished task 89.1 in stage 3.0 (TID 1214) in 5638 ms on hadoop-slave2 (executor 6) (154/200)
18/02/14 13:37:21 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on hadoop-slave2:33289 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:37:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.34:59054
18/02/14 13:37:22 INFO TaskSetManager: Starting task 3.0 in stage 2.2 (TID 1235, hadoop-slave3, executor 5, partition 18, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:22 INFO TaskSetManager: Finished task 4.2 in stage 3.0 (TID 1213) in 6374 ms on hadoop-slave3 (executor 5) (155/200)
18/02/14 13:37:22 INFO TaskSetManager: Starting task 176.0 in stage 3.0 (TID 1236, hadoop-slave1, executor 8, partition 176, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:22 INFO TaskSetManager: Finished task 157.0 in stage 3.0 (TID 1219) in 5905 ms on hadoop-slave1 (executor 8) (156/200)
18/02/14 13:37:22 INFO TaskSetManager: Starting task 4.0 in stage 2.2 (TID 1237, hadoop-slave3, executor 5, partition 34, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:22 INFO TaskSetManager: Finished task 156.0 in stage 3.0 (TID 1217) in 6150 ms on hadoop-slave3 (executor 5) (157/200)
18/02/14 13:37:22 INFO TaskSetManager: Starting task 5.0 in stage 2.2 (TID 1238, hadoop-slave3, executor 5, partition 35, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:22 INFO TaskSetManager: Finished task 123.1 in stage 3.0 (TID 1215) in 6828 ms on hadoop-slave3 (executor 5) (158/200)
18/02/14 13:37:23 INFO TaskSetManager: Starting task 177.0 in stage 3.0 (TID 1239, hadoop-slave1, executor 8, partition 177, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:23 INFO TaskSetManager: Finished task 166.0 in stage 3.0 (TID 1223) in 5496 ms on hadoop-slave1 (executor 8) (159/200)
18/02/14 13:37:23 INFO TaskSetManager: Starting task 6.0 in stage 2.2 (TID 1240, hadoop-slave5, executor 7, partition 57, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:23 INFO TaskSetManager: Finished task 163.0 in stage 3.0 (TID 1212) in 7418 ms on hadoop-slave5 (executor 7) (160/200)
18/02/14 13:37:23 INFO TaskSetManager: Starting task 7.0 in stage 2.2 (TID 1241, hadoop-slave3, executor 5, partition 66, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:23 INFO TaskSetManager: Finished task 161.0 in stage 3.0 (TID 1220) in 5981 ms on hadoop-slave3 (executor 5) (161/200)
18/02/14 13:37:23 INFO TaskSetManager: Starting task 8.0 in stage 2.2 (TID 1242, hadoop-slave2, executor 6, partition 77, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:23 INFO TaskSetManager: Finished task 170.0 in stage 3.0 (TID 1229) in 4235 ms on hadoop-slave2 (executor 6) (162/200)
18/02/14 13:37:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.34:59054
18/02/14 13:37:23 INFO TaskSetManager: Starting task 9.0 in stage 2.2 (TID 1243, hadoop-slave5, executor 7, partition 81, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:23 INFO TaskSetManager: Finished task 45.1 in stage 3.0 (TID 1208) in 9544 ms on hadoop-slave5 (executor 7) (163/200)
18/02/14 13:37:23 INFO TaskSetManager: Starting task 10.0 in stage 2.2 (TID 1244, hadoop-slave2, executor 6, partition 82, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:23 INFO TaskSetManager: Finished task 162.0 in stage 3.0 (TID 1222) in 6077 ms on hadoop-slave2 (executor 6) (164/200)
18/02/14 13:37:23 INFO TaskSetManager: Starting task 11.0 in stage 2.2 (TID 1245, hadoop-slave2, executor 6, partition 86, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:23 INFO TaskSetManager: Finished task 169.0 in stage 3.0 (TID 1227) in 5266 ms on hadoop-slave2 (executor 6) (165/200)
18/02/14 13:37:24 INFO TaskSetManager: Starting task 12.0 in stage 2.2 (TID 1246, hadoop-slave5, executor 7, partition 89, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:24 INFO TaskSetManager: Finished task 168.0 in stage 3.0 (TID 1228) in 5349 ms on hadoop-slave5 (executor 7) (166/200)
18/02/14 13:37:24 INFO TaskSetManager: Starting task 178.0 in stage 3.0 (TID 1247, hadoop-slave1, executor 8, partition 178, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:24 INFO TaskSetManager: Finished task 91.2 in stage 3.0 (TID 1216) in 7751 ms on hadoop-slave1 (executor 8) (167/200)
18/02/14 13:37:25 INFO TaskSetManager: Starting task 13.0 in stage 2.2 (TID 1248, hadoop-slave2, executor 6, partition 92, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:25 INFO TaskSetManager: Finished task 165.0 in stage 3.0 (TID 1224) in 6983 ms on hadoop-slave2 (executor 6) (168/200)
18/02/14 13:37:25 INFO TaskSetManager: Starting task 179.0 in stage 3.0 (TID 1249, hadoop-slave1, executor 8, partition 179, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:25 INFO TaskSetManager: Finished task 174.0 in stage 3.0 (TID 1226) in 6655 ms on hadoop-slave1 (executor 8) (169/200)
18/02/14 13:37:25 INFO TaskSetManager: Starting task 14.0 in stage 2.2 (TID 1250, hadoop-slave3, executor 5, partition 95, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:25 INFO TaskSetManager: Finished task 167.0 in stage 3.0 (TID 1225) in 6833 ms on hadoop-slave3 (executor 5) (170/200)
18/02/14 13:37:25 INFO TaskSetManager: Starting task 182.0 in stage 3.0 (TID 1251, hadoop-slave1, executor 8, partition 182, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:25 INFO TaskSetManager: Finished task 98.1 in stage 3.0 (TID 1218) in 9180 ms on hadoop-slave1 (executor 8) (171/200)
18/02/14 13:37:26 INFO TaskSetManager: Starting task 15.0 in stage 2.2 (TID 1252, hadoop-slave5, executor 7, partition 100, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:26 INFO TaskSetManager: Finished task 164.0 in stage 3.0 (TID 1221) in 9451 ms on hadoop-slave5 (executor 7) (172/200)
18/02/14 13:37:27 INFO TaskSetManager: Starting task 16.0 in stage 2.2 (TID 1253, hadoop-slave5, executor 7, partition 101, NODE_LOCAL, 6201 bytes)
18/02/14 13:37:27 INFO TaskSetManager: Finished task 171.0 in stage 3.0 (TID 1230) in 7995 ms on hadoop-slave5 (executor 7) (173/200)
18/02/14 13:37:27 INFO TaskSetManager: Starting task 183.0 in stage 3.0 (TID 1254, hadoop-slave1, executor 8, partition 183, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:27 INFO TaskSetManager: Finished task 175.0 in stage 3.0 (TID 1231) in 8351 ms on hadoop-slave1 (executor 8) (174/200)
18/02/14 13:37:28 INFO TaskSetManager: Starting task 185.0 in stage 3.0 (TID 1255, hadoop-slave1, executor 8, partition 185, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:28 INFO TaskSetManager: Finished task 176.0 in stage 3.0 (TID 1236) in 5968 ms on hadoop-slave1 (executor 8) (175/200)
18/02/14 13:37:28 INFO TaskSetManager: Starting task 186.0 in stage 3.0 (TID 1256, hadoop-slave1, executor 8, partition 186, NODE_LOCAL, 6941 bytes)
18/02/14 13:37:28 INFO TaskSetManager: Finished task 177.0 in stage 3.0 (TID 1239) in 5951 ms on hadoop-slave1 (executor 8) (176/200)
18/02/14 13:37:30 INFO TaskSetManager: Starting task 17.0 in stage 2.2 (TID 1257, hadoop-slave1, executor 8, partition 126, RACK_LOCAL, 6201 bytes)
18/02/14 13:37:30 INFO TaskSetManager: Finished task 178.0 in stage 3.0 (TID 1247) in 5915 ms on hadoop-slave1 (executor 8) (177/200)
18/02/14 13:37:30 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on hadoop-slave1:41387 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:37:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.35:60446
18/02/14 13:37:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.35:60446
18/02/14 13:37:32 INFO TaskSetManager: Starting task 18.0 in stage 2.2 (TID 1258, hadoop-slave1, executor 8, partition 129, RACK_LOCAL, 6201 bytes)
18/02/14 13:37:32 INFO TaskSetManager: Finished task 182.0 in stage 3.0 (TID 1251) in 6397 ms on hadoop-slave1 (executor 8) (178/200)
18/02/14 13:37:33 INFO TaskSetManager: Starting task 19.0 in stage 2.2 (TID 1259, hadoop-slave1, executor 8, partition 142, RACK_LOCAL, 6201 bytes)
18/02/14 13:37:33 INFO TaskSetManager: Finished task 183.0 in stage 3.0 (TID 1254) in 6225 ms on hadoop-slave1 (executor 8) (179/200)
18/02/14 13:37:34 INFO TaskSetManager: Starting task 20.0 in stage 2.2 (TID 1260, hadoop-slave1, executor 8, partition 153, RACK_LOCAL, 6201 bytes)
18/02/14 13:37:34 INFO TaskSetManager: Finished task 179.0 in stage 3.0 (TID 1249) in 9200 ms on hadoop-slave1 (executor 8) (180/200)
18/02/14 13:37:35 INFO TaskSetManager: Starting task 21.0 in stage 2.2 (TID 1261, hadoop-slave1, executor 8, partition 154, RACK_LOCAL, 6201 bytes)
18/02/14 13:37:35 INFO TaskSetManager: Finished task 185.0 in stage 3.0 (TID 1255) in 6958 ms on hadoop-slave1 (executor 8) (181/200)
18/02/14 13:37:35 INFO TaskSetManager: Starting task 22.0 in stage 2.2 (TID 1262, hadoop-slave1, executor 8, partition 165, RACK_LOCAL, 6201 bytes)
18/02/14 13:37:35 INFO TaskSetManager: Finished task 186.0 in stage 3.0 (TID 1256) in 6996 ms on hadoop-slave1 (executor 8) (182/200)
18/02/14 13:42:35 WARN HeartbeatReceiver: Removing executor 5 with no recent heartbeats: 168871 ms exceeds timeout 120000 ms
18/02/14 13:42:35 ERROR YarnScheduler: Lost executor 5 on hadoop-slave3: Executor heartbeat timed out after 168871 ms
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 48), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 122), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 18), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 115), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 137), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 102), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 40), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 111), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 113), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 143), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 19), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 23), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 4), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 118), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 152), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 62), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 53), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 167), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 86), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 131), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 139), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 85), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 103), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 119), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 97), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 107), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 132), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 92), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 39), so marking it as still running
18/02/14 13:42:35 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 147), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 42), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 57), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 46), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 142), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 68), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 41), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 81), so marking it as still running
18/02/14 13:42:36 WARN TaskSetManager: Lost task 4.0 in stage 2.2 (TID 1237, hadoop-slave3, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 168871 ms
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 65), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 66), so marking it as still running
18/02/14 13:42:36 WARN TaskSetManager: Lost task 14.0 in stage 2.2 (TID 1250, hadoop-slave3, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 168871 ms
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 123), so marking it as still running
18/02/14 13:42:36 WARN TaskSetManager: Lost task 7.0 in stage 2.2 (TID 1241, hadoop-slave3, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 168871 ms
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 17), so marking it as still running
18/02/14 13:42:36 WARN TaskSetManager: Lost task 0.0 in stage 2.2 (TID 1232, hadoop-slave3, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 168871 ms
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 11), so marking it as still running
18/02/14 13:42:36 WARN TaskSetManager: Lost task 3.0 in stage 2.2 (TID 1235, hadoop-slave3, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 168871 ms
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 129), so marking it as still running
18/02/14 13:42:36 WARN TaskSetManager: Lost task 5.0 in stage 2.2 (TID 1238, hadoop-slave3, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 168871 ms
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 104), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 151), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 61), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 99), so marking it as still running
18/02/14 13:42:36 INFO YarnClientSchedulerBackend: Requesting to kill executor(s) 5
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 75), so marking it as still running
18/02/14 13:42:36 INFO YarnClientSchedulerBackend: Actual list of executor(s) to be killed is 5
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 56), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 156), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 141), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 47), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 72), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 59), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 29), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(3, 161), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 90), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 121), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 67), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 76), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 48), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 56), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 28), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 39), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 7), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 19), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 103), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 112), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 51), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 22), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 31), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 11), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 42), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 179), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 81), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 140), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 172), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 131), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 161), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 89), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 102), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 111), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 120), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 196), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 88), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 150), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 79), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 50), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 93), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 186), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 30), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 69), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 41), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 123), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 57), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 21), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 59), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 155), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 105), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 134), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 114), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 144), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 33), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 44), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 127), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 137), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 108), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 72), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 84), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 117), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 97), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 53), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 62), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 13), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 24), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 147), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 27), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 17), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 75), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 55), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 65), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 38), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 47), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 185), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 10), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 178), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 77), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 136), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 116), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 146), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 125), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 107), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 87), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 99), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 37), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 74), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 119), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 64), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 160), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 195), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 171), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 46), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 54), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 130), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 110), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 139), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 149), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 40), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 49), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 133), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 143), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 20), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 29), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 113), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 122), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 58), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 68), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 78), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 8), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 91), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 104), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 152), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 83), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 71), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 12), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 23), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 32), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 96), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 61), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 43), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 52), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 26), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 15), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 141), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 165), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 132), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 183), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 151), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 94), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 135), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 80), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 177), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 115), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 124), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 66), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 106), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 70), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 60), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 145), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 73), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 45), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 85), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 138), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 170), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 25), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 36), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 118), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 128), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 14), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 63), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 148), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 159), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 3), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 98), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 109), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Resubmitted ShuffleMapTask(2, 193), so marking it as still running
18/02/14 13:42:36 INFO DAGScheduler: Executor lost: 5 (epoch 80)
18/02/14 13:42:36 INFO BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
18/02/14 13:42:36 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(5, hadoop-slave3, 33203, None)
18/02/14 13:42:36 INFO BlockManagerMaster: Removed 5 successfully in removeExecutor
18/02/14 13:42:36 INFO DAGScheduler: Shuffle files lost for executor: 5 (epoch 80)
18/02/14 13:42:36 INFO ShuffleMapStage: ShuffleMapStage 2 is now unavailable on executor 5 (26/200, false)
18/02/14 13:42:36 INFO ShuffleMapStage: ShuffleMapStage 1 is now unavailable on executor 5 (149/200, false)
18/02/14 13:42:36 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 5 (126/200, false)
18/02/14 13:42:36 INFO ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 5 (72/100, false)
18/02/14 13:42:36 INFO DAGScheduler: Host added was in lost list earlier: hadoop-slave3
18/02/14 13:42:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 5.
18/02/14 13:42:38 INFO DAGScheduler: Executor lost: 5 (epoch 86)
18/02/14 13:42:38 INFO BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
18/02/14 13:42:38 INFO BlockManagerMaster: Removed 5 successfully in removeExecutor
18/02/14 13:42:38 INFO DAGScheduler: Shuffle files lost for executor: 5 (epoch 86)
18/02/14 13:42:38 ERROR YarnScheduler: Lost executor 5 on hadoop-slave3: Container container_e41_1518606550421_0002_01_000007 exited from explicit termination request.
18/02/14 13:42:45 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (172.19.3.33:52708) with ID 9
18/02/14 13:42:45 INFO TaskSetManager: Starting task 5.1 in stage 2.2 (TID 1263, hadoop-slave3, executor 9, partition 35, NODE_LOCAL, 6201 bytes)
18/02/14 13:42:45 INFO TaskSetManager: Starting task 3.1 in stage 2.2 (TID 1264, hadoop-slave3, executor 9, partition 18, NODE_LOCAL, 6201 bytes)
18/02/14 13:42:45 INFO TaskSetManager: Starting task 0.1 in stage 2.2 (TID 1265, hadoop-slave3, executor 9, partition 5, NODE_LOCAL, 6201 bytes)
18/02/14 13:42:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.19.3.36:33535 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:45 INFO TaskSetManager: Starting task 7.1 in stage 2.2 (TID 1266, hadoop-slave3, executor 9, partition 66, NODE_LOCAL, 6201 bytes)
18/02/14 13:42:45 INFO TaskSetManager: Starting task 14.1 in stage 2.2 (TID 1267, hadoop-slave3, executor 9, partition 95, NODE_LOCAL, 6201 bytes)
18/02/14 13:42:45 INFO TaskSetManager: Starting task 4.1 in stage 2.2 (TID 1268, hadoop-slave3, executor 9, partition 34, NODE_LOCAL, 6201 bytes)
18/02/14 13:42:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on hadoop-slave2:33289 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on hadoop-slave5:35799 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on hadoop-slave1:41387 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:45 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-slave3:34425 with 4.1 GB RAM, BlockManagerId(9, hadoop-slave3, 34425, None)
18/02/14 13:42:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on hadoop-slave3:34425 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:42:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.33:52708
18/02/14 13:42:46 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 549 bytes
18/02/14 13:42:46 INFO TaskSetManager: Starting task 23.0 in stage 2.2 (TID 1269, hadoop-slave3, executor 9, partition 169, NODE_LOCAL, 6201 bytes)
18/02/14 13:42:46 WARN TaskSetManager: Lost task 5.1 in stage 2.2 (TID 1263, hadoop-slave3, executor 9): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=35, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:42:46 INFO TaskSetManager: Task 5.1 in stage 2.2 (TID 1263) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:46 INFO DAGScheduler: Marking ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) as failed due to a fetch failure from ShuffleMapStage 0 (show at JaccardCoefficient.scala:74)
18/02/14 13:42:46 INFO DAGScheduler: ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) failed in 327.415 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

18/02/14 13:42:46 INFO DAGScheduler: Resubmitting ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:42:46 INFO TaskSetManager: Starting task 161.1 in stage 3.0 (TID 1270, hadoop-slave3, executor 9, partition 161, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:46 WARN TaskSetManager: Lost task 14.1 in stage 2.2 (TID 1267, hadoop-slave3, executor 9): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=95, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:42:46 INFO TaskSetManager: Task 14.1 in stage 2.2 (TID 1267) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:46 INFO TaskSetManager: Starting task 29.3 in stage 3.0 (TID 1271, hadoop-slave3, executor 9, partition 29, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:46 WARN TaskSetManager: Lost task 4.1 in stage 2.2 (TID 1268, hadoop-slave3, executor 9): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=34, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:42:46 INFO TaskSetManager: Task 4.1 in stage 2.2 (TID 1268) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:46 INFO TaskSetManager: Starting task 59.2 in stage 3.0 (TID 1272, hadoop-slave3, executor 9, partition 59, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:46 WARN TaskSetManager: Lost task 7.1 in stage 2.2 (TID 1266, hadoop-slave3, executor 9): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=66, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:42:46 INFO TaskSetManager: Task 7.1 in stage 2.2 (TID 1266) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:46 INFO TaskSetManager: Starting task 72.2 in stage 3.0 (TID 1273, hadoop-slave3, executor 9, partition 72, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:46 WARN TaskSetManager: Lost task 0.1 in stage 2.2 (TID 1265, hadoop-slave3, executor 9): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:42:46 INFO TaskSetManager: Task 0.1 in stage 2.2 (TID 1265) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:46 INFO TaskSetManager: Starting task 47.2 in stage 3.0 (TID 1274, hadoop-slave3, executor 9, partition 47, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:46 WARN TaskSetManager: Lost task 3.1 in stage 2.2 (TID 1264, hadoop-slave3, executor 9): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=18, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:42:46 INFO TaskSetManager: Task 3.1 in stage 2.2 (TID 1264) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:46 INFO TaskSetManager: Starting task 141.1 in stage 3.0 (TID 1275, hadoop-slave3, executor 9, partition 141, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:46 WARN TaskSetManager: Lost task 23.0 in stage 2.2 (TID 1269, hadoop-slave3, executor 9): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=169, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

)
18/02/14 13:42:46 INFO TaskSetManager: Task 23.0 in stage 2.2 (TID 1269) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop-slave3:34425 (size: 5.7 KB, free: 4.1 GB)
18/02/14 13:42:46 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:42:46 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:42:46 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 10.3 KB, free 4.1 GB)
18/02/14 13:42:46 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.1 KB, free 4.1 GB)
18/02/14 13:42:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.19.3.36:33535 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:42:46 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:996
18/02/14 13:42:46 INFO DAGScheduler: Submitting 28 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at JaccardCoefficient.scala:74)
18/02/14 13:42:46 INFO YarnScheduler: Adding task set 0.5 with 28 tasks
18/02/14 13:42:46 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:42:46 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.7 KB, free 4.1 GB)
18/02/14 13:42:46 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.9 KB, free 4.1 GB)
18/02/14 13:42:46 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.19.3.36:33535 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:46 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:996
18/02/14 13:42:46 INFO DAGScheduler: Submitting 51 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at show at JaccardCoefficient.scala:74)
18/02/14 13:42:46 INFO YarnScheduler: Adding task set 1.4 with 51 tasks
18/02/14 13:42:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-slave3:34425 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:42:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-slave3:34425 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:42:48 INFO TaskSetManager: Starting task 0.0 in stage 0.5 (TID 1276, hadoop-slave5, executor 7, partition 2, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:48 WARN TaskSetManager: Lost task 1.0 in stage 2.2 (TID 1233, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=46, reduceId=9, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:48 INFO TaskSetManager: Task 1.0 in stage 2.2 (TID 1233) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:48 INFO DAGScheduler: Resubmitting ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:42:48 INFO TaskSetManager: Starting task 5.0 in stage 0.5 (TID 1277, hadoop-slave5, executor 7, partition 15, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:48 WARN TaskSetManager: Lost task 16.0 in stage 2.2 (TID 1253, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=158, reduceId=101, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:48 INFO TaskSetManager: Task 16.0 in stage 2.2 (TID 1253) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:48 INFO TaskSetManager: Starting task 9.0 in stage 0.5 (TID 1278, hadoop-slave5, executor 7, partition 39, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:48 WARN TaskSetManager: Lost task 6.0 in stage 2.2 (TID 1240, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=125, reduceId=57, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:48 INFO TaskSetManager: Task 6.0 in stage 2.2 (TID 1240) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:48 INFO TaskSetManager: Starting task 12.0 in stage 0.5 (TID 1279, hadoop-slave5, executor 7, partition 44, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:48 WARN TaskSetManager: Lost task 12.0 in stage 2.2 (TID 1246, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=185, reduceId=89, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:48 INFO TaskSetManager: Task 12.0 in stage 2.2 (TID 1246) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:48 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on hadoop-slave5:35799 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:42:48 INFO TaskSetManager: Starting task 13.0 in stage 0.5 (TID 1280, hadoop-slave5, executor 7, partition 45, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:48 WARN TaskSetManager: Lost task 9.0 in stage 2.2 (TID 1243, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=44, reduceId=81, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:48 INFO TaskSetManager: Task 9.0 in stage 2.2 (TID 1243) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:48 INFO TaskSetManager: Starting task 14.0 in stage 0.5 (TID 1281, hadoop-slave5, executor 7, partition 48, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:48 WARN TaskSetManager: Lost task 15.0 in stage 2.2 (TID 1252, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=46, reduceId=100, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:48 INFO TaskSetManager: Task 15.0 in stage 2.2 (TID 1252) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:48 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:42:53 INFO TaskSetManager: Starting task 2.0 in stage 0.5 (TID 1282, hadoop-slave2, executor 6, partition 7, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 8.0 in stage 2.2 (TID 1242, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=37, reduceId=77, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 8.0 in stage 2.2 (TID 1242) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO DAGScheduler: Resubmitting ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) and ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) due to fetch failure
18/02/14 13:42:53 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on hadoop-slave2:33289 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:42:53 INFO TaskSetManager: Starting task 4.0 in stage 0.5 (TID 1283, hadoop-slave2, executor 6, partition 13, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 10.0 in stage 2.2 (TID 1244, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=58, reduceId=82, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 10.0 in stage 2.2 (TID 1244) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO TaskSetManager: Starting task 7.0 in stage 0.5 (TID 1284, hadoop-slave2, executor 6, partition 32, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 13.0 in stage 2.2 (TID 1248, hadoop-slave2, executor 6): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=86, reduceId=92, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 13.0 in stage 2.2 (TID 1248) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO TaskSetManager: Starting task 1.0 in stage 0.5 (TID 1285, hadoop-slave1, executor 8, partition 4, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 20.0 in stage 2.2 (TID 1260, hadoop-slave1, executor 8): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=188, reduceId=153, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 20.0 in stage 2.2 (TID 1260) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO TaskSetManager: Starting task 3.0 in stage 0.5 (TID 1286, hadoop-slave1, executor 8, partition 11, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 19.0 in stage 2.2 (TID 1259, hadoop-slave1, executor 8): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=68, reduceId=142, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 19.0 in stage 2.2 (TID 1259) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO TaskSetManager: Starting task 6.0 in stage 0.5 (TID 1287, hadoop-slave1, executor 8, partition 29, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 22.0 in stage 2.2 (TID 1262, hadoop-slave1, executor 8): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=160, reduceId=165, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 22.0 in stage 2.2 (TID 1262) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO TaskSetManager: Starting task 8.0 in stage 0.5 (TID 1288, hadoop-slave1, executor 8, partition 37, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 21.0 in stage 2.2 (TID 1261, hadoop-slave1, executor 8): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=166, reduceId=154, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 21.0 in stage 2.2 (TID 1261) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO TaskSetManager: Starting task 10.0 in stage 0.5 (TID 1289, hadoop-slave1, executor 8, partition 42, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 18.0 in stage 2.2 (TID 1258, hadoop-slave1, executor 8): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=42, reduceId=129, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 18.0 in stage 2.2 (TID 1258) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on hadoop-slave1:41387 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:42:53 INFO TaskSetManager: Starting task 11.0 in stage 0.5 (TID 1290, hadoop-slave1, executor 8, partition 43, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 WARN TaskSetManager: Lost task 17.0 in stage 2.2 (TID 1257, hadoop-slave1, executor 8): FetchFailed(BlockManagerId(5, hadoop-slave3, 33203, None), shuffleId=1, mapId=42, reduceId=126, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to hadoop-slave3/172.19.3.33:33203
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: hadoop-slave3/172.19.3.33:33203
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	... 2 more

)
18/02/14 13:42:53 INFO TaskSetManager: Task 17.0 in stage 2.2 (TID 1257) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:42:53 INFO DAGScheduler: Resubmitting failed stages
18/02/14 13:42:53 INFO TaskSetManager: Starting task 18.0 in stage 0.5 (TID 1291, hadoop-slave5, executor 7, partition 64, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:53 INFO TaskSetManager: Finished task 14.0 in stage 0.5 (TID 1281) in 5205 ms on hadoop-slave5 (executor 7) (1/28)
18/02/14 13:42:54 INFO TaskSetManager: Starting task 19.0 in stage 0.5 (TID 1292, hadoop-slave5, executor 7, partition 65, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:54 INFO TaskSetManager: Finished task 13.0 in stage 0.5 (TID 1280) in 5812 ms on hadoop-slave5 (executor 7) (2/28)
18/02/14 13:42:54 INFO TaskSetManager: Starting task 15.0 in stage 0.5 (TID 1293, hadoop-slave3, executor 9, partition 56, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:54 INFO TaskSetManager: Finished task 161.1 in stage 3.0 (TID 1270) in 7636 ms on hadoop-slave3 (executor 9) (127/200)
18/02/14 13:42:54 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on hadoop-slave3:34425 (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:42:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-slave3:34425 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:42:54 INFO TaskSetManager: Starting task 20.0 in stage 0.5 (TID 1294, hadoop-slave5, executor 7, partition 67, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:54 INFO TaskSetManager: Finished task 12.0 in stage 0.5 (TID 1279) in 6102 ms on hadoop-slave5 (executor 7) (3/28)
18/02/14 13:42:54 INFO TaskSetManager: Starting task 21.0 in stage 0.5 (TID 1295, hadoop-slave5, executor 7, partition 71, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:54 INFO TaskSetManager: Finished task 5.0 in stage 0.5 (TID 1277) in 6327 ms on hadoop-slave5 (executor 7) (4/28)
18/02/14 13:42:54 INFO TaskSetManager: Starting task 16.0 in stage 0.5 (TID 1296, hadoop-slave3, executor 9, partition 61, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:54 INFO TaskSetManager: Finished task 29.3 in stage 3.0 (TID 1271) in 8232 ms on hadoop-slave3 (executor 9) (128/200)
18/02/14 13:42:55 INFO TaskSetManager: Starting task 17.0 in stage 0.5 (TID 1297, hadoop-slave3, executor 9, partition 62, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:55 INFO TaskSetManager: Finished task 141.1 in stage 3.0 (TID 1275) in 8509 ms on hadoop-slave3 (executor 9) (129/200)
18/02/14 13:42:55 INFO TaskSetManager: Starting task 23.0 in stage 0.5 (TID 1298, hadoop-slave5, executor 7, partition 81, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:55 INFO TaskSetManager: Finished task 0.0 in stage 0.5 (TID 1276) in 6959 ms on hadoop-slave5 (executor 7) (5/28)
18/02/14 13:42:55 INFO TaskSetManager: Starting task 22.0 in stage 0.5 (TID 1299, hadoop-slave3, executor 9, partition 75, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:55 INFO TaskSetManager: Finished task 72.2 in stage 3.0 (TID 1273) in 8808 ms on hadoop-slave3 (executor 9) (130/200)
18/02/14 13:42:55 INFO TaskSetManager: Starting task 24.0 in stage 0.5 (TID 1300, hadoop-slave5, executor 7, partition 90, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:55 INFO TaskSetManager: Finished task 9.0 in stage 0.5 (TID 1278) in 7410 ms on hadoop-slave5 (executor 7) (6/28)
18/02/14 13:42:55 INFO TaskSetManager: Starting task 25.0 in stage 0.5 (TID 1301, hadoop-slave3, executor 9, partition 93, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:55 INFO TaskSetManager: Finished task 47.2 in stage 3.0 (TID 1274) in 9305 ms on hadoop-slave3 (executor 9) (131/200)
18/02/14 13:42:56 INFO TaskSetManager: Starting task 26.0 in stage 0.5 (TID 1302, hadoop-slave3, executor 9, partition 95, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:56 INFO TaskSetManager: Finished task 59.2 in stage 3.0 (TID 1272) in 9414 ms on hadoop-slave3 (executor 9) (132/200)
18/02/14 13:42:56 INFO TaskSetManager: Starting task 27.0 in stage 0.5 (TID 1303, hadoop-slave1, executor 8, partition 97, NODE_LOCAL, 6832 bytes)
18/02/14 13:42:56 INFO TaskSetManager: Finished task 1.0 in stage 0.5 (TID 1285) in 3292 ms on hadoop-slave1 (executor 8) (7/28)
18/02/14 13:42:57 INFO TaskSetManager: Starting task 0.0 in stage 1.4 (TID 1304, hadoop-slave1, executor 8, partition 11, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:57 INFO TaskSetManager: Finished task 8.0 in stage 0.5 (TID 1288) in 3944 ms on hadoop-slave1 (executor 8) (8/28)
18/02/14 13:42:57 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on hadoop-slave1:41387 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:58 INFO TaskSetManager: Starting task 4.0 in stage 1.4 (TID 1305, hadoop-slave1, executor 8, partition 18, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:58 INFO TaskSetManager: Finished task 10.0 in stage 0.5 (TID 1289) in 4733 ms on hadoop-slave1 (executor 8) (9/28)
18/02/14 13:42:58 INFO TaskSetManager: Starting task 6.0 in stage 1.4 (TID 1306, hadoop-slave1, executor 8, partition 37, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:58 INFO TaskSetManager: Finished task 3.0 in stage 0.5 (TID 1286) in 4841 ms on hadoop-slave1 (executor 8) (10/28)
18/02/14 13:42:58 INFO TaskSetManager: Starting task 1.0 in stage 1.4 (TID 1307, hadoop-slave3, executor 9, partition 13, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:58 INFO TaskSetManager: Finished task 15.0 in stage 0.5 (TID 1293) in 4313 ms on hadoop-slave3 (executor 9) (11/28)
18/02/14 13:42:58 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on hadoop-slave3:34425 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:58 INFO TaskSetManager: Starting task 2.0 in stage 1.4 (TID 1308, hadoop-slave5, executor 7, partition 16, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:58 INFO TaskSetManager: Finished task 23.0 in stage 0.5 (TID 1298) in 3592 ms on hadoop-slave5 (executor 7) (12/28)
18/02/14 13:42:58 INFO TaskSetManager: Starting task 5.0 in stage 1.4 (TID 1309, hadoop-slave5, executor 7, partition 24, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:58 INFO TaskSetManager: Finished task 19.0 in stage 0.5 (TID 1292) in 4690 ms on hadoop-slave5 (executor 7) (13/28)
18/02/14 13:42:58 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on hadoop-slave5:35799 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 3.0 in stage 1.4 (TID 1310, hadoop-slave3, executor 9, partition 17, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 16.0 in stage 0.5 (TID 1296) in 4159 ms on hadoop-slave3 (executor 9) (14/28)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 8.0 in stage 1.4 (TID 1311, hadoop-slave1, executor 8, partition 42, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 6.0 in stage 0.5 (TID 1287) in 5710 ms on hadoop-slave1 (executor 8) (15/28)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 9.0 in stage 1.4 (TID 1312, hadoop-slave1, executor 8, partition 43, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 11.0 in stage 0.5 (TID 1290) in 5820 ms on hadoop-slave1 (executor 8) (16/28)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 7.0 in stage 1.4 (TID 1313, hadoop-slave5, executor 7, partition 40, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 21.0 in stage 0.5 (TID 1295) in 4750 ms on hadoop-slave5 (executor 7) (17/28)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 10.0 in stage 1.4 (TID 1314, hadoop-slave5, executor 7, partition 44, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 20.0 in stage 0.5 (TID 1294) in 5242 ms on hadoop-slave5 (executor 7) (18/28)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 11.0 in stage 1.4 (TID 1315, hadoop-slave3, executor 9, partition 45, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 17.0 in stage 0.5 (TID 1297) in 4526 ms on hadoop-slave3 (executor 9) (19/28)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 12.0 in stage 1.4 (TID 1316, hadoop-slave3, executor 9, partition 46, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 22.0 in stage 0.5 (TID 1299) in 4403 ms on hadoop-slave3 (executor 9) (20/28)
18/02/14 13:42:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-slave3:34425 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:42:59 INFO TaskSetManager: Starting task 13.0 in stage 1.4 (TID 1317, hadoop-slave3, executor 9, partition 50, NODE_LOCAL, 6941 bytes)
18/02/14 13:42:59 INFO TaskSetManager: Finished task 25.0 in stage 0.5 (TID 1301) in 4047 ms on hadoop-slave3 (executor 9) (21/28)
18/02/14 13:43:00 INFO TaskSetManager: Starting task 14.0 in stage 1.4 (TID 1318, hadoop-slave5, executor 7, partition 54, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:00 INFO TaskSetManager: Finished task 24.0 in stage 0.5 (TID 1300) in 4340 ms on hadoop-slave5 (executor 7) (22/28)
18/02/14 13:43:00 INFO TaskSetManager: Starting task 15.0 in stage 1.4 (TID 1319, hadoop-slave3, executor 9, partition 58, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:00 INFO TaskSetManager: Finished task 26.0 in stage 0.5 (TID 1302) in 4365 ms on hadoop-slave3 (executor 9) (23/28)
18/02/14 13:43:02 INFO TaskSetManager: Starting task 16.0 in stage 1.4 (TID 1320, hadoop-slave5, executor 7, partition 59, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:02 INFO TaskSetManager: Finished task 18.0 in stage 0.5 (TID 1291) in 8603 ms on hadoop-slave5 (executor 7) (24/28)
18/02/14 13:43:02 INFO TaskSetManager: Starting task 19.0 in stage 1.4 (TID 1321, hadoop-slave2, executor 6, partition 70, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:02 INFO TaskSetManager: Finished task 2.0 in stage 0.5 (TID 1282) in 9158 ms on hadoop-slave2 (executor 6) (25/28)
18/02/14 13:43:02 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on hadoop-slave2:33289 (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:43:03 INFO TaskSetManager: Starting task 22.0 in stage 1.4 (TID 1322, hadoop-slave1, executor 8, partition 75, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:03 INFO TaskSetManager: Finished task 27.0 in stage 0.5 (TID 1303) in 7164 ms on hadoop-slave1 (executor 8) (26/28)
18/02/14 13:43:04 INFO TaskSetManager: Starting task 23.0 in stage 1.4 (TID 1323, hadoop-slave1, executor 8, partition 85, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:04 INFO TaskSetManager: Finished task 9.0 in stage 1.4 (TID 1312) in 4813 ms on hadoop-slave1 (executor 8) (1/51)
18/02/14 13:43:04 INFO TaskSetManager: Starting task 20.0 in stage 1.4 (TID 1324, hadoop-slave2, executor 6, partition 71, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:04 INFO TaskSetManager: Finished task 4.0 in stage 0.5 (TID 1283) in 11045 ms on hadoop-slave2 (executor 6) (27/28)
18/02/14 13:43:04 INFO TaskSetManager: Starting task 24.0 in stage 1.4 (TID 1325, hadoop-slave1, executor 8, partition 86, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:04 INFO TaskSetManager: Finished task 6.0 in stage 1.4 (TID 1306) in 6768 ms on hadoop-slave1 (executor 8) (2/51)
18/02/14 13:43:05 INFO TaskSetManager: Starting task 21.0 in stage 1.4 (TID 1326, hadoop-slave2, executor 6, partition 72, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:05 INFO TaskSetManager: Finished task 7.0 in stage 0.5 (TID 1284) in 12004 ms on hadoop-slave2 (executor 6) (28/28)
18/02/14 13:43:05 INFO YarnScheduler: Removed TaskSet 0.5, whose tasks have all completed, from pool 
18/02/14 13:43:05 INFO DAGScheduler: ShuffleMapStage 0 (show at JaccardCoefficient.scala:74) finished in 18.479 s
18/02/14 13:43:05 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:43:05 INFO DAGScheduler: running: Set(ShuffleMapStage 1, ShuffleMapStage 3)
18/02/14 13:43:05 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:43:05 INFO DAGScheduler: failed: Set()
18/02/14 13:43:05 INFO TaskSetManager: Starting task 25.0 in stage 1.4 (TID 1327, hadoop-slave1, executor 8, partition 90, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:05 INFO TaskSetManager: Finished task 4.0 in stage 1.4 (TID 1305) in 7550 ms on hadoop-slave1 (executor 8) (3/51)
18/02/14 13:43:06 INFO TaskSetManager: Finished task 15.0 in stage 1.4 (TID 1319) in 5957 ms on hadoop-slave3 (executor 9) (4/51)
18/02/14 13:43:06 INFO TaskSetManager: Starting task 17.0 in stage 1.4 (TID 1328, hadoop-slave3, executor 9, partition 64, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:06 INFO TaskSetManager: Starting task 26.0 in stage 1.4 (TID 1329, hadoop-slave1, executor 8, partition 93, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:06 INFO TaskSetManager: Finished task 0.0 in stage 1.4 (TID 1304) in 9538 ms on hadoop-slave1 (executor 8) (5/51)
18/02/14 13:43:06 INFO TaskSetManager: Starting task 18.0 in stage 1.4 (TID 1330, hadoop-slave5, executor 7, partition 68, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:06 INFO TaskSetManager: Finished task 10.0 in stage 1.4 (TID 1314) in 7223 ms on hadoop-slave5 (executor 7) (6/51)
18/02/14 13:43:07 INFO TaskSetManager: Starting task 27.0 in stage 1.4 (TID 1331, hadoop-slave3, executor 9, partition 95, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:07 INFO TaskSetManager: Finished task 12.0 in stage 1.4 (TID 1316) in 7185 ms on hadoop-slave3 (executor 9) (7/51)
18/02/14 13:43:07 INFO TaskSetManager: Starting task 28.0 in stage 1.4 (TID 1332, hadoop-slave3, executor 9, partition 97, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:07 INFO TaskSetManager: Finished task 1.0 in stage 1.4 (TID 1307) in 8963 ms on hadoop-slave3 (executor 9) (8/51)
18/02/14 13:43:07 INFO TaskSetManager: Starting task 29.0 in stage 1.4 (TID 1333, hadoop-slave3, executor 9, partition 113, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:07 INFO TaskSetManager: Finished task 11.0 in stage 1.4 (TID 1315) in 7999 ms on hadoop-slave3 (executor 9) (9/51)
18/02/14 13:43:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-slave3:34425 (size: 32.5 KB, free: 4.1 GB)
18/02/14 13:43:09 INFO TaskSetManager: Starting task 30.0 in stage 1.4 (TID 1334, hadoop-slave3, executor 9, partition 116, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:09 INFO TaskSetManager: Finished task 13.0 in stage 1.4 (TID 1317) in 9910 ms on hadoop-slave3 (executor 9) (10/51)
18/02/14 13:43:10 INFO TaskSetManager: Starting task 31.0 in stage 1.4 (TID 1335, hadoop-slave2, executor 6, partition 117, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:10 INFO TaskSetManager: Finished task 19.0 in stage 1.4 (TID 1321) in 7751 ms on hadoop-slave2 (executor 6) (11/51)
18/02/14 13:43:10 INFO TaskSetManager: Starting task 33.0 in stage 1.4 (TID 1336, hadoop-slave1, executor 8, partition 129, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:10 INFO TaskSetManager: Finished task 8.0 in stage 1.4 (TID 1311) in 11195 ms on hadoop-slave1 (executor 8) (12/51)
18/02/14 13:43:11 INFO TaskSetManager: Starting task 32.0 in stage 1.4 (TID 1337, hadoop-slave5, executor 7, partition 125, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:11 INFO TaskSetManager: Finished task 16.0 in stage 1.4 (TID 1320) in 9512 ms on hadoop-slave5 (executor 7) (13/51)
18/02/14 13:43:12 INFO TaskSetManager: Starting task 34.0 in stage 1.4 (TID 1338, hadoop-slave3, executor 9, partition 132, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:12 INFO TaskSetManager: Finished task 29.0 in stage 1.4 (TID 1333) in 4485 ms on hadoop-slave3 (executor 9) (14/51)
18/02/14 13:43:13 INFO TaskSetManager: Starting task 36.0 in stage 1.4 (TID 1339, hadoop-slave5, executor 7, partition 139, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:13 INFO TaskSetManager: Finished task 2.0 in stage 1.4 (TID 1308) in 14224 ms on hadoop-slave5 (executor 7) (15/51)
18/02/14 13:43:13 INFO TaskSetManager: Starting task 35.0 in stage 1.4 (TID 1340, hadoop-slave3, executor 9, partition 133, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:13 INFO TaskSetManager: Finished task 28.0 in stage 1.4 (TID 1332) in 6368 ms on hadoop-slave3 (executor 9) (16/51)
18/02/14 13:43:14 INFO TaskSetManager: Starting task 38.0 in stage 1.4 (TID 1341, hadoop-slave5, executor 7, partition 145, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:14 INFO TaskSetManager: Finished task 5.0 in stage 1.4 (TID 1309) in 15567 ms on hadoop-slave5 (executor 7) (17/51)
18/02/14 13:43:15 INFO TaskSetManager: Starting task 37.0 in stage 1.4 (TID 1342, hadoop-slave3, executor 9, partition 143, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:15 INFO TaskSetManager: Finished task 3.0 in stage 1.4 (TID 1310) in 16329 ms on hadoop-slave3 (executor 9) (18/51)
18/02/14 13:43:15 INFO TaskSetManager: Starting task 41.0 in stage 1.4 (TID 1343, hadoop-slave2, executor 6, partition 165, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:15 INFO TaskSetManager: Finished task 31.0 in stage 1.4 (TID 1335) in 5448 ms on hadoop-slave2 (executor 6) (19/51)
18/02/14 13:43:15 INFO TaskSetManager: Starting task 42.0 in stage 1.4 (TID 1344, hadoop-slave2, executor 6, partition 166, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:15 INFO TaskSetManager: Finished task 21.0 in stage 1.4 (TID 1326) in 10354 ms on hadoop-slave2 (executor 6) (20/51)
18/02/14 13:43:17 INFO TaskSetManager: Starting task 39.0 in stage 1.4 (TID 1345, hadoop-slave3, executor 9, partition 158, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:17 INFO TaskSetManager: Finished task 27.0 in stage 1.4 (TID 1331) in 10514 ms on hadoop-slave3 (executor 9) (21/51)
18/02/14 13:43:18 INFO TaskSetManager: Starting task 50.0 in stage 1.4 (TID 1346, hadoop-slave2, executor 6, partition 199, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:18 INFO TaskSetManager: Finished task 20.0 in stage 1.4 (TID 1324) in 14215 ms on hadoop-slave2 (executor 6) (22/51)
18/02/14 13:43:19 INFO TaskSetManager: Starting task 40.0 in stage 1.4 (TID 1347, hadoop-slave1, executor 8, partition 160, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:19 INFO TaskSetManager: Finished task 26.0 in stage 1.4 (TID 1329) in 12738 ms on hadoop-slave1 (executor 8) (23/51)
18/02/14 13:43:19 INFO TaskSetManager: Starting task 43.0 in stage 1.4 (TID 1348, hadoop-slave1, executor 8, partition 175, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:19 INFO TaskSetManager: Finished task 24.0 in stage 1.4 (TID 1325) in 14826 ms on hadoop-slave1 (executor 8) (24/51)
18/02/14 13:43:20 INFO TaskSetManager: Starting task 44.0 in stage 1.4 (TID 1349, hadoop-slave3, executor 9, partition 181, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:20 INFO TaskSetManager: Finished task 37.0 in stage 1.4 (TID 1342) in 5401 ms on hadoop-slave3 (executor 9) (25/51)
18/02/14 13:43:20 INFO TaskSetManager: Starting task 45.0 in stage 1.4 (TID 1350, hadoop-slave3, executor 9, partition 184, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:20 INFO TaskSetManager: Finished task 30.0 in stage 1.4 (TID 1334) in 10930 ms on hadoop-slave3 (executor 9) (26/51)
18/02/14 13:43:21 INFO TaskSetManager: Starting task 46.0 in stage 1.4 (TID 1351, hadoop-slave1, executor 8, partition 185, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:21 INFO TaskSetManager: Finished task 22.0 in stage 1.4 (TID 1322) in 18136 ms on hadoop-slave1 (executor 8) (27/51)
18/02/14 13:43:22 INFO TaskSetManager: Starting task 48.0 in stage 1.4 (TID 1352, hadoop-slave1, executor 8, partition 190, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:22 INFO TaskSetManager: Finished task 23.0 in stage 1.4 (TID 1323) in 18016 ms on hadoop-slave1 (executor 8) (28/51)
18/02/14 13:43:22 INFO TaskSetManager: Starting task 47.0 in stage 1.4 (TID 1353, hadoop-slave3, executor 9, partition 188, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:22 INFO TaskSetManager: Finished task 17.0 in stage 1.4 (TID 1328) in 15987 ms on hadoop-slave3 (executor 9) (29/51)
18/02/14 13:43:22 INFO TaskSetManager: Starting task 49.0 in stage 1.4 (TID 1354, hadoop-slave3, executor 9, partition 192, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:22 INFO TaskSetManager: Finished task 35.0 in stage 1.4 (TID 1340) in 8544 ms on hadoop-slave3 (executor 9) (30/51)
18/02/14 13:43:22 INFO TaskSetManager: Starting task 156.1 in stage 3.0 (TID 1355, hadoop-slave3, executor 9, partition 156, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:22 INFO TaskSetManager: Finished task 39.0 in stage 1.4 (TID 1345) in 5284 ms on hadoop-slave3 (executor 9) (31/51)
18/02/14 13:43:22 INFO TaskSetManager: Starting task 99.1 in stage 3.0 (TID 1356, hadoop-slave5, executor 7, partition 99, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:22 INFO TaskSetManager: Finished task 7.0 in stage 1.4 (TID 1313) in 23534 ms on hadoop-slave5 (executor 7) (32/51)
18/02/14 13:43:23 INFO TaskSetManager: Starting task 151.1 in stage 3.0 (TID 1357, hadoop-slave5, executor 7, partition 151, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:23 INFO TaskSetManager: Finished task 14.0 in stage 1.4 (TID 1318) in 23014 ms on hadoop-slave5 (executor 7) (33/51)
18/02/14 13:43:24 INFO TaskSetManager: Starting task 56.2 in stage 3.0 (TID 1358, hadoop-slave2, executor 6, partition 56, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:24 INFO TaskSetManager: Finished task 41.0 in stage 1.4 (TID 1343) in 8419 ms on hadoop-slave2 (executor 6) (34/51)
18/02/14 13:43:24 INFO TaskSetManager: Starting task 75.2 in stage 3.0 (TID 1359, hadoop-slave1, executor 8, partition 75, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:24 INFO TaskSetManager: Finished task 43.0 in stage 1.4 (TID 1348) in 4683 ms on hadoop-slave1 (executor 8) (35/51)
18/02/14 13:43:24 INFO TaskSetManager: Starting task 65.2 in stage 3.0 (TID 1360, hadoop-slave5, executor 7, partition 65, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:24 INFO TaskSetManager: Finished task 32.0 in stage 1.4 (TID 1337) in 13085 ms on hadoop-slave5 (executor 7) (36/51)
18/02/14 13:43:25 INFO TaskSetManager: Starting task 81.2 in stage 3.0 (TID 1361, hadoop-slave5, executor 7, partition 81, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:25 INFO TaskSetManager: Finished task 18.0 in stage 1.4 (TID 1330) in 18646 ms on hadoop-slave5 (executor 7) (37/51)
18/02/14 13:43:25 INFO TaskSetManager: Starting task 61.2 in stage 3.0 (TID 1362, hadoop-slave2, executor 6, partition 61, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:25 INFO TaskSetManager: Finished task 42.0 in stage 1.4 (TID 1344) in 9878 ms on hadoop-slave2 (executor 6) (38/51)
18/02/14 13:43:26 INFO TaskSetManager: Starting task 104.1 in stage 3.0 (TID 1363, hadoop-slave3, executor 9, partition 104, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:26 INFO TaskSetManager: Finished task 156.1 in stage 3.0 (TID 1355) in 4042 ms on hadoop-slave3 (executor 9) (133/200)
18/02/14 13:43:27 INFO TaskSetManager: Starting task 129.1 in stage 3.0 (TID 1364, hadoop-slave1, executor 8, partition 129, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:27 INFO TaskSetManager: Finished task 33.0 in stage 1.4 (TID 1336) in 17175 ms on hadoop-slave1 (executor 8) (39/51)
18/02/14 13:43:27 INFO TaskSetManager: Starting task 11.2 in stage 3.0 (TID 1365, hadoop-slave3, executor 9, partition 11, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:27 INFO TaskSetManager: Finished task 44.0 in stage 1.4 (TID 1349) in 6753 ms on hadoop-slave3 (executor 9) (40/51)
18/02/14 13:43:27 INFO TaskSetManager: Starting task 123.2 in stage 3.0 (TID 1366, hadoop-slave1, executor 8, partition 123, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:27 INFO TaskSetManager: Finished task 25.0 in stage 1.4 (TID 1327) in 21915 ms on hadoop-slave1 (executor 8) (41/51)
18/02/14 13:43:28 INFO TaskSetManager: Starting task 17.2 in stage 3.0 (TID 1367, hadoop-slave2, executor 6, partition 17, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:28 INFO TaskSetManager: Finished task 50.0 in stage 1.4 (TID 1346) in 9632 ms on hadoop-slave2 (executor 6) (42/51)
18/02/14 13:43:28 INFO TaskSetManager: Starting task 41.2 in stage 3.0 (TID 1368, hadoop-slave5, executor 7, partition 41, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:28 INFO TaskSetManager: Finished task 65.2 in stage 3.0 (TID 1360) in 3731 ms on hadoop-slave5 (executor 7) (134/200)
18/02/14 13:43:29 INFO TaskSetManager: Starting task 68.2 in stage 3.0 (TID 1369, hadoop-slave5, executor 7, partition 68, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:29 INFO TaskSetManager: Finished task 99.1 in stage 3.0 (TID 1356) in 6180 ms on hadoop-slave5 (executor 7) (135/200)
18/02/14 13:43:29 INFO TaskSetManager: Starting task 66.2 in stage 3.0 (TID 1370, hadoop-slave3, executor 9, partition 66, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:29 INFO TaskSetManager: Finished task 34.0 in stage 1.4 (TID 1338) in 16952 ms on hadoop-slave3 (executor 9) (43/51)
18/02/14 13:43:29 INFO TaskSetManager: Starting task 46.2 in stage 3.0 (TID 1371, hadoop-slave5, executor 7, partition 46, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:29 INFO TaskSetManager: Finished task 36.0 in stage 1.4 (TID 1339) in 16155 ms on hadoop-slave5 (executor 7) (44/51)
18/02/14 13:43:29 INFO TaskSetManager: Starting task 39.2 in stage 3.0 (TID 1372, hadoop-slave5, executor 7, partition 39, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:29 INFO TaskSetManager: Finished task 151.1 in stage 3.0 (TID 1357) in 6372 ms on hadoop-slave5 (executor 7) (136/200)
18/02/14 13:43:30 INFO TaskSetManager: Starting task 57.2 in stage 3.0 (TID 1373, hadoop-slave2, executor 6, partition 57, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:30 INFO TaskSetManager: Finished task 56.2 in stage 3.0 (TID 1358) in 6391 ms on hadoop-slave2 (executor 6) (137/200)
18/02/14 13:43:30 INFO TaskSetManager: Starting task 142.1 in stage 3.0 (TID 1374, hadoop-slave1, executor 8, partition 142, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:30 INFO TaskSetManager: Finished task 40.0 in stage 1.4 (TID 1347) in 11364 ms on hadoop-slave1 (executor 8) (45/51)
18/02/14 13:43:31 INFO TaskSetManager: Starting task 42.2 in stage 3.0 (TID 1375, hadoop-slave1, executor 8, partition 42, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:31 INFO TaskSetManager: Finished task 75.2 in stage 3.0 (TID 1359) in 7135 ms on hadoop-slave1 (executor 8) (138/200)
18/02/14 13:43:31 INFO TaskSetManager: Starting task 147.1 in stage 3.0 (TID 1376, hadoop-slave1, executor 8, partition 147, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:31 INFO TaskSetManager: Finished task 48.0 in stage 1.4 (TID 1352) in 9719 ms on hadoop-slave1 (executor 8) (46/51)
18/02/14 13:43:31 INFO TaskSetManager: Starting task 132.1 in stage 3.0 (TID 1377, hadoop-slave1, executor 8, partition 132, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:31 INFO TaskSetManager: Finished task 46.0 in stage 1.4 (TID 1351) in 9799 ms on hadoop-slave1 (executor 8) (47/51)
18/02/14 13:43:32 INFO TaskSetManager: Starting task 107.1 in stage 3.0 (TID 1378, hadoop-slave2, executor 6, partition 107, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:32 INFO TaskSetManager: Finished task 61.2 in stage 3.0 (TID 1362) in 6526 ms on hadoop-slave2 (executor 6) (139/200)
18/02/14 13:43:32 INFO TaskSetManager: Starting task 92.2 in stage 3.0 (TID 1379, hadoop-slave5, executor 7, partition 92, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:32 INFO TaskSetManager: Finished task 68.2 in stage 3.0 (TID 1369) in 3595 ms on hadoop-slave5 (executor 7) (140/200)
18/02/14 13:43:32 INFO TaskSetManager: Starting task 97.1 in stage 3.0 (TID 1380, hadoop-slave1, executor 8, partition 97, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:32 INFO TaskSetManager: Finished task 129.1 in stage 3.0 (TID 1364) in 5420 ms on hadoop-slave1 (executor 8) (141/200)
18/02/14 13:43:33 INFO TaskSetManager: Starting task 119.1 in stage 3.0 (TID 1381, hadoop-slave3, executor 9, partition 119, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:33 INFO TaskSetManager: Finished task 11.2 in stage 3.0 (TID 1365) in 6132 ms on hadoop-slave3 (executor 9) (142/200)
18/02/14 13:43:33 INFO TaskSetManager: Starting task 103.1 in stage 3.0 (TID 1382, hadoop-slave3, executor 9, partition 103, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:33 INFO TaskSetManager: Finished task 104.1 in stage 3.0 (TID 1363) in 6767 ms on hadoop-slave3 (executor 9) (143/200)
18/02/14 13:43:33 INFO TaskSetManager: Starting task 85.2 in stage 3.0 (TID 1383, hadoop-slave1, executor 8, partition 85, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:33 INFO TaskSetManager: Finished task 123.2 in stage 3.0 (TID 1366) in 6071 ms on hadoop-slave1 (executor 8) (144/200)
18/02/14 13:43:33 INFO TaskSetManager: Starting task 139.1 in stage 3.0 (TID 1384, hadoop-slave5, executor 7, partition 139, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:33 INFO TaskSetManager: Finished task 39.2 in stage 3.0 (TID 1372) in 4441 ms on hadoop-slave5 (executor 7) (145/200)
18/02/14 13:43:34 INFO TaskSetManager: Starting task 131.1 in stage 3.0 (TID 1385, hadoop-slave2, executor 6, partition 131, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:34 INFO TaskSetManager: Finished task 17.2 in stage 3.0 (TID 1367) in 5893 ms on hadoop-slave2 (executor 6) (146/200)
18/02/14 13:43:34 INFO TaskSetManager: Starting task 167.1 in stage 3.0 (TID 1386, hadoop-slave5, executor 7, partition 167, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:34 INFO TaskSetManager: Finished task 38.0 in stage 1.4 (TID 1341) in 19888 ms on hadoop-slave5 (executor 7) (48/51)
18/02/14 13:43:34 INFO TaskSetManager: Starting task 152.1 in stage 3.0 (TID 1387, hadoop-slave5, executor 7, partition 152, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:34 INFO TaskSetManager: Finished task 41.2 in stage 3.0 (TID 1368) in 6204 ms on hadoop-slave5 (executor 7) (147/200)
18/02/14 13:43:35 INFO TaskSetManager: Starting task 86.2 in stage 3.0 (TID 1388, hadoop-slave3, executor 9, partition 86, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:35 INFO TaskSetManager: Finished task 66.2 in stage 3.0 (TID 1370) in 6020 ms on hadoop-slave3 (executor 9) (148/200)
18/02/14 13:43:35 INFO TaskSetManager: Starting task 19.2 in stage 3.0 (TID 1389, hadoop-slave5, executor 7, partition 19, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:35 INFO TaskSetManager: Finished task 81.2 in stage 3.0 (TID 1361) in 9984 ms on hadoop-slave5 (executor 7) (149/200)
18/02/14 13:43:35 INFO TaskSetManager: Starting task 53.2 in stage 3.0 (TID 1390, hadoop-slave1, executor 8, partition 53, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:35 INFO TaskSetManager: Finished task 142.1 in stage 3.0 (TID 1374) in 5025 ms on hadoop-slave1 (executor 8) (150/200)
18/02/14 13:43:36 INFO TaskSetManager: Starting task 118.2 in stage 3.0 (TID 1391, hadoop-slave1, executor 8, partition 118, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:36 INFO TaskSetManager: Finished task 42.2 in stage 3.0 (TID 1375) in 5081 ms on hadoop-slave1 (executor 8) (151/200)
18/02/14 13:43:37 INFO TaskSetManager: Starting task 4.3 in stage 3.0 (TID 1392, hadoop-slave1, executor 8, partition 4, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:37 INFO TaskSetManager: Finished task 97.1 in stage 3.0 (TID 1380) in 4766 ms on hadoop-slave1 (executor 8) (152/200)
18/02/14 13:43:38 INFO TaskSetManager: Starting task 40.2 in stage 3.0 (TID 1393, hadoop-slave5, executor 7, partition 40, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:38 INFO TaskSetManager: Finished task 139.1 in stage 3.0 (TID 1384) in 4217 ms on hadoop-slave5 (executor 7) (153/200)
18/02/14 13:43:38 INFO TaskSetManager: Starting task 62.2 in stage 3.0 (TID 1394, hadoop-slave2, executor 6, partition 62, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:38 INFO TaskSetManager: Finished task 57.2 in stage 3.0 (TID 1373) in 7812 ms on hadoop-slave2 (executor 6) (154/200)
18/02/14 13:43:38 INFO TaskSetManager: Starting task 23.2 in stage 3.0 (TID 1395, hadoop-slave1, executor 8, partition 23, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:38 INFO TaskSetManager: Finished task 132.1 in stage 3.0 (TID 1377) in 6554 ms on hadoop-slave1 (executor 8) (155/200)
18/02/14 13:43:38 INFO TaskSetManager: Starting task 143.1 in stage 3.0 (TID 1396, hadoop-slave3, executor 9, partition 143, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:38 INFO TaskSetManager: Finished task 47.0 in stage 1.4 (TID 1353) in 16184 ms on hadoop-slave3 (executor 9) (49/51)
18/02/14 13:43:38 INFO TaskSetManager: Starting task 102.1 in stage 3.0 (TID 1397, hadoop-slave5, executor 7, partition 102, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:38 INFO TaskSetManager: Finished task 167.1 in stage 3.0 (TID 1386) in 4244 ms on hadoop-slave5 (executor 7) (156/200)
18/02/14 13:43:38 INFO TaskSetManager: Starting task 111.1 in stage 3.0 (TID 1398, hadoop-slave1, executor 8, partition 111, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:38 INFO TaskSetManager: Finished task 147.1 in stage 3.0 (TID 1376) in 6815 ms on hadoop-slave1 (executor 8) (157/200)
18/02/14 13:43:38 INFO TaskSetManager: Starting task 113.1 in stage 3.0 (TID 1399, hadoop-slave3, executor 9, partition 113, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:38 INFO TaskSetManager: Finished task 49.0 in stage 1.4 (TID 1354) in 16289 ms on hadoop-slave3 (executor 9) (50/51)
18/02/14 13:43:39 INFO TaskSetManager: Starting task 137.1 in stage 3.0 (TID 1400, hadoop-slave1, executor 8, partition 137, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:39 INFO TaskSetManager: Finished task 85.2 in stage 3.0 (TID 1383) in 5687 ms on hadoop-slave1 (executor 8) (158/200)
18/02/14 13:43:39 INFO TaskSetManager: Starting task 115.1 in stage 3.0 (TID 1401, hadoop-slave5, executor 7, partition 115, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:39 INFO TaskSetManager: Finished task 92.2 in stage 3.0 (TID 1379) in 7061 ms on hadoop-slave5 (executor 7) (159/200)
18/02/14 13:43:39 INFO TaskSetManager: Starting task 122.1 in stage 3.0 (TID 1402, hadoop-slave2, executor 6, partition 122, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:39 INFO TaskSetManager: Finished task 107.1 in stage 3.0 (TID 1378) in 7783 ms on hadoop-slave2 (executor 6) (160/200)
18/02/14 13:43:41 INFO TaskSetManager: Starting task 18.2 in stage 3.0 (TID 1403, hadoop-slave3, executor 9, partition 18, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:41 INFO TaskSetManager: Finished task 86.2 in stage 3.0 (TID 1388) in 6236 ms on hadoop-slave3 (executor 9) (161/200)
18/02/14 13:43:41 INFO TaskSetManager: Starting task 48.2 in stage 3.0 (TID 1404, hadoop-slave3, executor 9, partition 48, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:41 INFO TaskSetManager: Finished task 103.1 in stage 3.0 (TID 1382) in 7915 ms on hadoop-slave3 (executor 9) (162/200)
18/02/14 13:43:41 INFO TaskSetManager: Starting task 172.0 in stage 3.0 (TID 1405, hadoop-slave3, executor 9, partition 172, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:41 INFO TaskSetManager: Finished task 119.1 in stage 3.0 (TID 1381) in 8132 ms on hadoop-slave3 (executor 9) (163/200)
18/02/14 13:43:42 INFO TaskSetManager: Starting task 181.0 in stage 3.0 (TID 1406, hadoop-slave3, executor 9, partition 181, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:42 INFO TaskSetManager: Finished task 45.0 in stage 1.4 (TID 1350) in 21277 ms on hadoop-slave3 (executor 9) (51/51)
18/02/14 13:43:42 INFO YarnScheduler: Removed TaskSet 1.4, whose tasks have all completed, from pool 
18/02/14 13:43:42 INFO DAGScheduler: ShuffleMapStage 1 (show at JaccardCoefficient.scala:74) finished in 55.258 s
18/02/14 13:43:42 INFO DAGScheduler: looking for newly runnable stages
18/02/14 13:43:42 INFO DAGScheduler: running: Set(ShuffleMapStage 3)
18/02/14 13:43:42 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 2, ResultStage 6, ShuffleMapStage 4)
18/02/14 13:43:42 INFO DAGScheduler: failed: Set()
18/02/14 13:43:42 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74), which has no missing parents
18/02/14 13:43:42 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 29.8 KB, free 4.1 GB)
18/02/14 13:43:42 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 12.0 KB, free 4.1 GB)
18/02/14 13:43:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.19.3.36:33535 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:43:42 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:996
18/02/14 13:43:42 INFO DAGScheduler: Submitting 174 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at show at JaccardCoefficient.scala:74)
18/02/14 13:43:42 INFO YarnScheduler: Adding task set 2.3 with 174 tasks
18/02/14 13:43:42 INFO TaskSetManager: Starting task 1.0 in stage 2.3 (TID 1407, hadoop-slave5, executor 7, partition 5, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:42 INFO TaskSetManager: Finished task 152.1 in stage 3.0 (TID 1387) in 7489 ms on hadoop-slave5 (executor 7) (164/200)
18/02/14 13:43:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on hadoop-slave5:35799 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:43:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.31:55688
18/02/14 13:43:42 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 624 bytes
18/02/14 13:43:42 INFO TaskSetManager: Starting task 4.0 in stage 2.3 (TID 1408, hadoop-slave5, executor 7, partition 9, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:42 INFO TaskSetManager: Finished task 19.2 in stage 3.0 (TID 1389) in 6849 ms on hadoop-slave5 (executor 7) (165/200)
18/02/14 13:43:42 INFO TaskSetManager: Starting task 11.0 in stage 2.3 (TID 1409, hadoop-slave1, executor 8, partition 16, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:42 INFO TaskSetManager: Finished task 23.2 in stage 3.0 (TID 1395) in 4353 ms on hadoop-slave1 (executor 8) (166/200)
18/02/14 13:43:42 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on hadoop-slave1:41387 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:43:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.35:60446
18/02/14 13:43:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.35:60446
18/02/14 13:43:42 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 835 bytes
18/02/14 13:43:43 INFO TaskSetManager: Starting task 13.0 in stage 2.3 (TID 1410, hadoop-slave1, executor 8, partition 18, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:43 INFO TaskSetManager: Finished task 118.2 in stage 3.0 (TID 1391) in 6309 ms on hadoop-slave1 (executor 8) (167/200)
18/02/14 13:43:43 INFO TaskSetManager: Starting task 29.0 in stage 2.3 (TID 1411, hadoop-slave2, executor 6, partition 34, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:43 INFO TaskSetManager: Finished task 62.2 in stage 3.0 (TID 1394) in 4790 ms on hadoop-slave2 (executor 6) (168/200)
18/02/14 13:43:43 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on hadoop-slave2:33289 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:43:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.34:59054
18/02/14 13:43:43 INFO TaskSetManager: Starting task 30.0 in stage 2.3 (TID 1412, hadoop-slave1, executor 8, partition 35, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:43 INFO TaskSetManager: Finished task 53.2 in stage 3.0 (TID 1390) in 7109 ms on hadoop-slave1 (executor 8) (169/200)
18/02/14 13:43:43 INFO TaskSetManager: Starting task 52.0 in stage 2.3 (TID 1413, hadoop-slave5, executor 7, partition 57, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:43 INFO TaskSetManager: Finished task 102.1 in stage 3.0 (TID 1397) in 5302 ms on hadoop-slave5 (executor 7) (170/200)
18/02/14 13:43:44 INFO TaskSetManager: Starting task 61.0 in stage 2.3 (TID 1414, hadoop-slave2, executor 6, partition 66, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:44 INFO TaskSetManager: Finished task 131.1 in stage 3.0 (TID 1385) in 9931 ms on hadoop-slave2 (executor 6) (171/200)
18/02/14 13:43:44 INFO TaskSetManager: Starting task 72.0 in stage 2.3 (TID 1415, hadoop-slave5, executor 7, partition 77, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:44 INFO TaskSetManager: Finished task 40.2 in stage 3.0 (TID 1393) in 5982 ms on hadoop-slave5 (executor 7) (172/200)
18/02/14 13:43:44 INFO TaskSetManager: Starting task 76.0 in stage 2.3 (TID 1416, hadoop-slave1, executor 8, partition 81, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:44 INFO TaskSetManager: Finished task 137.1 in stage 3.0 (TID 1400) in 4791 ms on hadoop-slave1 (executor 8) (173/200)
18/02/14 13:43:44 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.31:55688
18/02/14 13:43:44 INFO TaskSetManager: Starting task 77.0 in stage 2.3 (TID 1417, hadoop-slave1, executor 8, partition 82, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:44 INFO TaskSetManager: Finished task 111.1 in stage 3.0 (TID 1398) in 5846 ms on hadoop-slave1 (executor 8) (174/200)
18/02/14 13:43:44 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.34:59054
18/02/14 13:43:44 INFO TaskSetManager: Starting task 0.0 in stage 2.3 (TID 1418, hadoop-slave3, executor 9, partition 3, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:44 INFO TaskSetManager: Finished task 143.1 in stage 3.0 (TID 1396) in 6396 ms on hadoop-slave3 (executor 9) (175/200)
18/02/14 13:43:44 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on hadoop-slave3:34425 (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:43:44 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.3.33:52708
18/02/14 13:43:44 INFO TaskSetManager: Starting task 2.0 in stage 2.3 (TID 1419, hadoop-slave3, executor 9, partition 7, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:44 INFO TaskSetManager: Finished task 113.1 in stage 3.0 (TID 1399) in 6240 ms on hadoop-slave3 (executor 9) (176/200)
18/02/14 13:43:44 INFO TaskSetManager: Starting task 81.0 in stage 2.3 (TID 1420, hadoop-slave5, executor 7, partition 86, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:44 INFO TaskSetManager: Finished task 115.1 in stage 3.0 (TID 1401) in 5240 ms on hadoop-slave5 (executor 7) (177/200)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 84.0 in stage 2.3 (TID 1421, hadoop-slave1, executor 8, partition 89, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 4.3 in stage 3.0 (TID 1392) in 7400 ms on hadoop-slave1 (executor 8) (178/200)
18/02/14 13:43:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.3.33:52708
18/02/14 13:43:45 INFO TaskSetManager: Starting task 3.0 in stage 2.3 (TID 1422, hadoop-slave3, executor 9, partition 8, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 2.0 in stage 2.3 (TID 1419) in 389 ms on hadoop-slave3 (executor 9) (1/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 5.0 in stage 2.3 (TID 1423, hadoop-slave3, executor 9, partition 10, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 0.0 in stage 2.3 (TID 1418) in 434 ms on hadoop-slave3 (executor 9) (2/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 6.0 in stage 2.3 (TID 1424, hadoop-slave3, executor 9, partition 11, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 5.0 in stage 2.3 (TID 1423) in 11 ms on hadoop-slave3 (executor 9) (3/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 7.0 in stage 2.3 (TID 1425, hadoop-slave3, executor 9, partition 12, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 3.0 in stage 2.3 (TID 1422) in 13 ms on hadoop-slave3 (executor 9) (4/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 8.0 in stage 2.3 (TID 1426, hadoop-slave3, executor 9, partition 13, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 7.0 in stage 2.3 (TID 1425) in 9 ms on hadoop-slave3 (executor 9) (5/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 9.0 in stage 2.3 (TID 1427, hadoop-slave3, executor 9, partition 14, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 6.0 in stage 2.3 (TID 1424) in 9 ms on hadoop-slave3 (executor 9) (6/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 10.0 in stage 2.3 (TID 1428, hadoop-slave3, executor 9, partition 15, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 8.0 in stage 2.3 (TID 1426) in 10 ms on hadoop-slave3 (executor 9) (7/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 12.0 in stage 2.3 (TID 1429, hadoop-slave3, executor 9, partition 17, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 9.0 in stage 2.3 (TID 1427) in 9 ms on hadoop-slave3 (executor 9) (8/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 14.0 in stage 2.3 (TID 1430, hadoop-slave3, executor 9, partition 19, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 12.0 in stage 2.3 (TID 1429) in 8 ms on hadoop-slave3 (executor 9) (9/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 15.0 in stage 2.3 (TID 1431, hadoop-slave3, executor 9, partition 20, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 10.0 in stage 2.3 (TID 1428) in 9 ms on hadoop-slave3 (executor 9) (10/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 16.0 in stage 2.3 (TID 1432, hadoop-slave3, executor 9, partition 21, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 14.0 in stage 2.3 (TID 1430) in 7 ms on hadoop-slave3 (executor 9) (11/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 17.0 in stage 2.3 (TID 1433, hadoop-slave3, executor 9, partition 22, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 15.0 in stage 2.3 (TID 1431) in 7 ms on hadoop-slave3 (executor 9) (12/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 18.0 in stage 2.3 (TID 1434, hadoop-slave3, executor 9, partition 23, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 17.0 in stage 2.3 (TID 1433) in 7 ms on hadoop-slave3 (executor 9) (13/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 19.0 in stage 2.3 (TID 1435, hadoop-slave3, executor 9, partition 24, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 16.0 in stage 2.3 (TID 1432) in 8 ms on hadoop-slave3 (executor 9) (14/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 20.0 in stage 2.3 (TID 1436, hadoop-slave3, executor 9, partition 25, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 18.0 in stage 2.3 (TID 1434) in 9 ms on hadoop-slave3 (executor 9) (15/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 21.0 in stage 2.3 (TID 1437, hadoop-slave3, executor 9, partition 26, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 19.0 in stage 2.3 (TID 1435) in 8 ms on hadoop-slave3 (executor 9) (16/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 22.0 in stage 2.3 (TID 1438, hadoop-slave3, executor 9, partition 27, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 21.0 in stage 2.3 (TID 1437) in 7 ms on hadoop-slave3 (executor 9) (17/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 23.0 in stage 2.3 (TID 1439, hadoop-slave3, executor 9, partition 28, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 20.0 in stage 2.3 (TID 1436) in 8 ms on hadoop-slave3 (executor 9) (18/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 24.0 in stage 2.3 (TID 1440, hadoop-slave3, executor 9, partition 29, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 23.0 in stage 2.3 (TID 1439) in 8 ms on hadoop-slave3 (executor 9) (19/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 25.0 in stage 2.3 (TID 1441, hadoop-slave3, executor 9, partition 30, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 22.0 in stage 2.3 (TID 1438) in 9 ms on hadoop-slave3 (executor 9) (20/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 26.0 in stage 2.3 (TID 1442, hadoop-slave3, executor 9, partition 31, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 24.0 in stage 2.3 (TID 1440) in 9 ms on hadoop-slave3 (executor 9) (21/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 27.0 in stage 2.3 (TID 1443, hadoop-slave3, executor 9, partition 32, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 25.0 in stage 2.3 (TID 1441) in 8 ms on hadoop-slave3 (executor 9) (22/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 28.0 in stage 2.3 (TID 1444, hadoop-slave3, executor 9, partition 33, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 27.0 in stage 2.3 (TID 1443) in 8 ms on hadoop-slave3 (executor 9) (23/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 31.0 in stage 2.3 (TID 1445, hadoop-slave3, executor 9, partition 36, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 26.0 in stage 2.3 (TID 1442) in 9 ms on hadoop-slave3 (executor 9) (24/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 32.0 in stage 2.3 (TID 1446, hadoop-slave3, executor 9, partition 37, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 31.0 in stage 2.3 (TID 1445) in 8 ms on hadoop-slave3 (executor 9) (25/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 33.0 in stage 2.3 (TID 1447, hadoop-slave3, executor 9, partition 38, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 28.0 in stage 2.3 (TID 1444) in 8 ms on hadoop-slave3 (executor 9) (26/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 34.0 in stage 2.3 (TID 1448, hadoop-slave3, executor 9, partition 39, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 33.0 in stage 2.3 (TID 1447) in 8 ms on hadoop-slave3 (executor 9) (27/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 35.0 in stage 2.3 (TID 1449, hadoop-slave3, executor 9, partition 40, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 32.0 in stage 2.3 (TID 1446) in 8 ms on hadoop-slave3 (executor 9) (28/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 36.0 in stage 2.3 (TID 1450, hadoop-slave3, executor 9, partition 41, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 34.0 in stage 2.3 (TID 1448) in 7 ms on hadoop-slave3 (executor 9) (29/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 37.0 in stage 2.3 (TID 1451, hadoop-slave3, executor 9, partition 42, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 35.0 in stage 2.3 (TID 1449) in 8 ms on hadoop-slave3 (executor 9) (30/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 38.0 in stage 2.3 (TID 1452, hadoop-slave3, executor 9, partition 43, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 36.0 in stage 2.3 (TID 1450) in 8 ms on hadoop-slave3 (executor 9) (31/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 39.0 in stage 2.3 (TID 1453, hadoop-slave3, executor 9, partition 44, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 37.0 in stage 2.3 (TID 1451) in 8 ms on hadoop-slave3 (executor 9) (32/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 40.0 in stage 2.3 (TID 1454, hadoop-slave3, executor 9, partition 45, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 38.0 in stage 2.3 (TID 1452) in 9 ms on hadoop-slave3 (executor 9) (33/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 41.0 in stage 2.3 (TID 1455, hadoop-slave3, executor 9, partition 46, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 39.0 in stage 2.3 (TID 1453) in 9 ms on hadoop-slave3 (executor 9) (34/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 42.0 in stage 2.3 (TID 1456, hadoop-slave3, executor 9, partition 47, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 40.0 in stage 2.3 (TID 1454) in 8 ms on hadoop-slave3 (executor 9) (35/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 43.0 in stage 2.3 (TID 1457, hadoop-slave3, executor 9, partition 48, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 41.0 in stage 2.3 (TID 1455) in 8 ms on hadoop-slave3 (executor 9) (36/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 44.0 in stage 2.3 (TID 1458, hadoop-slave3, executor 9, partition 49, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 42.0 in stage 2.3 (TID 1456) in 7 ms on hadoop-slave3 (executor 9) (37/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 45.0 in stage 2.3 (TID 1459, hadoop-slave3, executor 9, partition 50, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 43.0 in stage 2.3 (TID 1457) in 7 ms on hadoop-slave3 (executor 9) (38/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 46.0 in stage 2.3 (TID 1460, hadoop-slave3, executor 9, partition 51, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 44.0 in stage 2.3 (TID 1458) in 7 ms on hadoop-slave3 (executor 9) (39/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 47.0 in stage 2.3 (TID 1461, hadoop-slave3, executor 9, partition 52, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 45.0 in stage 2.3 (TID 1459) in 7 ms on hadoop-slave3 (executor 9) (40/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 48.0 in stage 2.3 (TID 1462, hadoop-slave3, executor 9, partition 53, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 46.0 in stage 2.3 (TID 1460) in 6 ms on hadoop-slave3 (executor 9) (41/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 49.0 in stage 2.3 (TID 1463, hadoop-slave3, executor 9, partition 54, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 47.0 in stage 2.3 (TID 1461) in 7 ms on hadoop-slave3 (executor 9) (42/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 50.0 in stage 2.3 (TID 1464, hadoop-slave3, executor 9, partition 55, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 49.0 in stage 2.3 (TID 1463) in 6 ms on hadoop-slave3 (executor 9) (43/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 51.0 in stage 2.3 (TID 1465, hadoop-slave3, executor 9, partition 56, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 48.0 in stage 2.3 (TID 1462) in 7 ms on hadoop-slave3 (executor 9) (44/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 53.0 in stage 2.3 (TID 1466, hadoop-slave3, executor 9, partition 58, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 50.0 in stage 2.3 (TID 1464) in 7 ms on hadoop-slave3 (executor 9) (45/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 54.0 in stage 2.3 (TID 1467, hadoop-slave3, executor 9, partition 59, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 51.0 in stage 2.3 (TID 1465) in 7 ms on hadoop-slave3 (executor 9) (46/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 55.0 in stage 2.3 (TID 1468, hadoop-slave3, executor 9, partition 60, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 53.0 in stage 2.3 (TID 1466) in 8 ms on hadoop-slave3 (executor 9) (47/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 56.0 in stage 2.3 (TID 1469, hadoop-slave3, executor 9, partition 61, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 54.0 in stage 2.3 (TID 1467) in 8 ms on hadoop-slave3 (executor 9) (48/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 57.0 in stage 2.3 (TID 1470, hadoop-slave3, executor 9, partition 62, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 56.0 in stage 2.3 (TID 1469) in 8 ms on hadoop-slave3 (executor 9) (49/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 58.0 in stage 2.3 (TID 1471, hadoop-slave3, executor 9, partition 63, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 55.0 in stage 2.3 (TID 1468) in 9 ms on hadoop-slave3 (executor 9) (50/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 59.0 in stage 2.3 (TID 1472, hadoop-slave3, executor 9, partition 64, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 57.0 in stage 2.3 (TID 1470) in 8 ms on hadoop-slave3 (executor 9) (51/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 60.0 in stage 2.3 (TID 1473, hadoop-slave3, executor 9, partition 65, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 58.0 in stage 2.3 (TID 1471) in 9 ms on hadoop-slave3 (executor 9) (52/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 62.0 in stage 2.3 (TID 1474, hadoop-slave3, executor 9, partition 67, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 59.0 in stage 2.3 (TID 1472) in 8 ms on hadoop-slave3 (executor 9) (53/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 63.0 in stage 2.3 (TID 1475, hadoop-slave3, executor 9, partition 68, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 60.0 in stage 2.3 (TID 1473) in 7 ms on hadoop-slave3 (executor 9) (54/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 64.0 in stage 2.3 (TID 1476, hadoop-slave3, executor 9, partition 69, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 63.0 in stage 2.3 (TID 1475) in 7 ms on hadoop-slave3 (executor 9) (55/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 65.0 in stage 2.3 (TID 1477, hadoop-slave3, executor 9, partition 70, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 62.0 in stage 2.3 (TID 1474) in 8 ms on hadoop-slave3 (executor 9) (56/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 66.0 in stage 2.3 (TID 1478, hadoop-slave3, executor 9, partition 71, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 64.0 in stage 2.3 (TID 1476) in 8 ms on hadoop-slave3 (executor 9) (57/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 67.0 in stage 2.3 (TID 1479, hadoop-slave3, executor 9, partition 72, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 65.0 in stage 2.3 (TID 1477) in 8 ms on hadoop-slave3 (executor 9) (58/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 68.0 in stage 2.3 (TID 1480, hadoop-slave3, executor 9, partition 73, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 66.0 in stage 2.3 (TID 1478) in 8 ms on hadoop-slave3 (executor 9) (59/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 69.0 in stage 2.3 (TID 1481, hadoop-slave3, executor 9, partition 74, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 67.0 in stage 2.3 (TID 1479) in 8 ms on hadoop-slave3 (executor 9) (60/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 70.0 in stage 2.3 (TID 1482, hadoop-slave3, executor 9, partition 75, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 68.0 in stage 2.3 (TID 1480) in 9 ms on hadoop-slave3 (executor 9) (61/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 71.0 in stage 2.3 (TID 1483, hadoop-slave3, executor 9, partition 76, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 69.0 in stage 2.3 (TID 1481) in 8 ms on hadoop-slave3 (executor 9) (62/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 73.0 in stage 2.3 (TID 1484, hadoop-slave3, executor 9, partition 78, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 71.0 in stage 2.3 (TID 1483) in 6 ms on hadoop-slave3 (executor 9) (63/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 74.0 in stage 2.3 (TID 1485, hadoop-slave3, executor 9, partition 79, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 70.0 in stage 2.3 (TID 1482) in 8 ms on hadoop-slave3 (executor 9) (64/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 75.0 in stage 2.3 (TID 1486, hadoop-slave3, executor 9, partition 80, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 73.0 in stage 2.3 (TID 1484) in 16 ms on hadoop-slave3 (executor 9) (65/174)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.19.3.36:33535 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 78.0 in stage 2.3 (TID 1487, hadoop-slave3, executor 9, partition 83, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 74.0 in stage 2.3 (TID 1485) in 17 ms on hadoop-slave3 (executor 9) (66/174)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_19_piece0 on hadoop-slave5:35799 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_19_piece0 on hadoop-slave1:41387 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_19_piece0 on hadoop-slave2:33289 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 79.0 in stage 2.3 (TID 1488, hadoop-slave3, executor 9, partition 84, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 75.0 in stage 2.3 (TID 1486) in 8 ms on hadoop-slave3 (executor 9) (67/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 80.0 in stage 2.3 (TID 1489, hadoop-slave3, executor 9, partition 85, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 78.0 in stage 2.3 (TID 1487) in 11 ms on hadoop-slave3 (executor 9) (68/174)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_19_piece0 on hadoop-slave3:34425 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 82.0 in stage 2.3 (TID 1490, hadoop-slave3, executor 9, partition 87, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 79.0 in stage 2.3 (TID 1488) in 8 ms on hadoop-slave3 (executor 9) (69/174)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.19.3.36:33535 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_18_piece0 on hadoop-slave1:41387 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_18_piece0 on hadoop-slave5:35799 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_18_piece0 on hadoop-slave2:33289 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 83.0 in stage 2.3 (TID 1491, hadoop-slave3, executor 9, partition 88, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 80.0 in stage 2.3 (TID 1489) in 10 ms on hadoop-slave3 (executor 9) (70/174)
18/02/14 13:43:45 INFO BlockManagerInfo: Removed broadcast_18_piece0 on hadoop-slave3:34425 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 85.0 in stage 2.3 (TID 1492, hadoop-slave3, executor 9, partition 90, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 82.0 in stage 2.3 (TID 1490) in 9 ms on hadoop-slave3 (executor 9) (71/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 86.0 in stage 2.3 (TID 1493, hadoop-slave3, executor 9, partition 91, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 83.0 in stage 2.3 (TID 1491) in 9 ms on hadoop-slave3 (executor 9) (72/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 88.0 in stage 2.3 (TID 1494, hadoop-slave3, executor 9, partition 93, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 85.0 in stage 2.3 (TID 1492) in 7 ms on hadoop-slave3 (executor 9) (73/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 89.0 in stage 2.3 (TID 1495, hadoop-slave3, executor 9, partition 94, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 86.0 in stage 2.3 (TID 1493) in 7 ms on hadoop-slave3 (executor 9) (74/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 91.0 in stage 2.3 (TID 1496, hadoop-slave3, executor 9, partition 96, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 88.0 in stage 2.3 (TID 1494) in 9 ms on hadoop-slave3 (executor 9) (75/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 92.0 in stage 2.3 (TID 1497, hadoop-slave3, executor 9, partition 97, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 89.0 in stage 2.3 (TID 1495) in 7 ms on hadoop-slave3 (executor 9) (76/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 93.0 in stage 2.3 (TID 1498, hadoop-slave3, executor 9, partition 98, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 91.0 in stage 2.3 (TID 1496) in 9 ms on hadoop-slave3 (executor 9) (77/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 94.0 in stage 2.3 (TID 1499, hadoop-slave3, executor 9, partition 99, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 92.0 in stage 2.3 (TID 1497) in 8 ms on hadoop-slave3 (executor 9) (78/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 97.0 in stage 2.3 (TID 1500, hadoop-slave3, executor 9, partition 102, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 93.0 in stage 2.3 (TID 1498) in 8 ms on hadoop-slave3 (executor 9) (79/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 98.0 in stage 2.3 (TID 1501, hadoop-slave3, executor 9, partition 103, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 94.0 in stage 2.3 (TID 1499) in 7 ms on hadoop-slave3 (executor 9) (80/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 99.0 in stage 2.3 (TID 1502, hadoop-slave3, executor 9, partition 104, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 97.0 in stage 2.3 (TID 1500) in 7 ms on hadoop-slave3 (executor 9) (81/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 100.0 in stage 2.3 (TID 1503, hadoop-slave3, executor 9, partition 105, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 98.0 in stage 2.3 (TID 1501) in 11 ms on hadoop-slave3 (executor 9) (82/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 101.0 in stage 2.3 (TID 1504, hadoop-slave3, executor 9, partition 106, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 99.0 in stage 2.3 (TID 1502) in 9 ms on hadoop-slave3 (executor 9) (83/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 102.0 in stage 2.3 (TID 1505, hadoop-slave3, executor 9, partition 107, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 100.0 in stage 2.3 (TID 1503) in 9 ms on hadoop-slave3 (executor 9) (84/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 103.0 in stage 2.3 (TID 1506, hadoop-slave3, executor 9, partition 108, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 101.0 in stage 2.3 (TID 1504) in 7 ms on hadoop-slave3 (executor 9) (85/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 104.0 in stage 2.3 (TID 1507, hadoop-slave3, executor 9, partition 109, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 102.0 in stage 2.3 (TID 1505) in 7 ms on hadoop-slave3 (executor 9) (86/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 105.0 in stage 2.3 (TID 1508, hadoop-slave3, executor 9, partition 110, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 103.0 in stage 2.3 (TID 1506) in 6 ms on hadoop-slave3 (executor 9) (87/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 106.0 in stage 2.3 (TID 1509, hadoop-slave3, executor 9, partition 111, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 104.0 in stage 2.3 (TID 1507) in 7 ms on hadoop-slave3 (executor 9) (88/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 107.0 in stage 2.3 (TID 1510, hadoop-slave3, executor 9, partition 112, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 105.0 in stage 2.3 (TID 1508) in 5 ms on hadoop-slave3 (executor 9) (89/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 108.0 in stage 2.3 (TID 1511, hadoop-slave3, executor 9, partition 113, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 106.0 in stage 2.3 (TID 1509) in 5 ms on hadoop-slave3 (executor 9) (90/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 109.0 in stage 2.3 (TID 1512, hadoop-slave3, executor 9, partition 114, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 107.0 in stage 2.3 (TID 1510) in 5 ms on hadoop-slave3 (executor 9) (91/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 110.0 in stage 2.3 (TID 1513, hadoop-slave3, executor 9, partition 115, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 109.0 in stage 2.3 (TID 1512) in 5 ms on hadoop-slave3 (executor 9) (92/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 111.0 in stage 2.3 (TID 1514, hadoop-slave3, executor 9, partition 116, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 108.0 in stage 2.3 (TID 1511) in 6 ms on hadoop-slave3 (executor 9) (93/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 112.0 in stage 2.3 (TID 1515, hadoop-slave3, executor 9, partition 117, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 110.0 in stage 2.3 (TID 1513) in 6 ms on hadoop-slave3 (executor 9) (94/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 113.0 in stage 2.3 (TID 1516, hadoop-slave3, executor 9, partition 118, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 111.0 in stage 2.3 (TID 1514) in 5 ms on hadoop-slave3 (executor 9) (95/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 114.0 in stage 2.3 (TID 1517, hadoop-slave3, executor 9, partition 119, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 113.0 in stage 2.3 (TID 1516) in 7 ms on hadoop-slave3 (executor 9) (96/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 115.0 in stage 2.3 (TID 1518, hadoop-slave3, executor 9, partition 120, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 112.0 in stage 2.3 (TID 1515) in 8 ms on hadoop-slave3 (executor 9) (97/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 116.0 in stage 2.3 (TID 1519, hadoop-slave3, executor 9, partition 121, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 114.0 in stage 2.3 (TID 1517) in 6 ms on hadoop-slave3 (executor 9) (98/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 117.0 in stage 2.3 (TID 1520, hadoop-slave3, executor 9, partition 122, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 115.0 in stage 2.3 (TID 1518) in 7 ms on hadoop-slave3 (executor 9) (99/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 118.0 in stage 2.3 (TID 1521, hadoop-slave3, executor 9, partition 123, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 116.0 in stage 2.3 (TID 1519) in 7 ms on hadoop-slave3 (executor 9) (100/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 119.0 in stage 2.3 (TID 1522, hadoop-slave3, executor 9, partition 124, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 117.0 in stage 2.3 (TID 1520) in 6 ms on hadoop-slave3 (executor 9) (101/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 120.0 in stage 2.3 (TID 1523, hadoop-slave3, executor 9, partition 125, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 119.0 in stage 2.3 (TID 1522) in 7 ms on hadoop-slave3 (executor 9) (102/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 122.0 in stage 2.3 (TID 1524, hadoop-slave3, executor 9, partition 127, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 118.0 in stage 2.3 (TID 1521) in 8 ms on hadoop-slave3 (executor 9) (103/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 123.0 in stage 2.3 (TID 1525, hadoop-slave3, executor 9, partition 128, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 120.0 in stage 2.3 (TID 1523) in 7 ms on hadoop-slave3 (executor 9) (104/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 125.0 in stage 2.3 (TID 1526, hadoop-slave3, executor 9, partition 130, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 122.0 in stage 2.3 (TID 1524) in 7 ms on hadoop-slave3 (executor 9) (105/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 126.0 in stage 2.3 (TID 1527, hadoop-slave3, executor 9, partition 131, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 123.0 in stage 2.3 (TID 1525) in 6 ms on hadoop-slave3 (executor 9) (106/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 127.0 in stage 2.3 (TID 1528, hadoop-slave3, executor 9, partition 132, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 125.0 in stage 2.3 (TID 1526) in 7 ms on hadoop-slave3 (executor 9) (107/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 128.0 in stage 2.3 (TID 1529, hadoop-slave3, executor 9, partition 133, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 126.0 in stage 2.3 (TID 1527) in 7 ms on hadoop-slave3 (executor 9) (108/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 129.0 in stage 2.3 (TID 1530, hadoop-slave3, executor 9, partition 134, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 127.0 in stage 2.3 (TID 1528) in 7 ms on hadoop-slave3 (executor 9) (109/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 130.0 in stage 2.3 (TID 1531, hadoop-slave3, executor 9, partition 135, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 129.0 in stage 2.3 (TID 1530) in 7 ms on hadoop-slave3 (executor 9) (110/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 131.0 in stage 2.3 (TID 1532, hadoop-slave3, executor 9, partition 136, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 128.0 in stage 2.3 (TID 1529) in 7 ms on hadoop-slave3 (executor 9) (111/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 132.0 in stage 2.3 (TID 1533, hadoop-slave3, executor 9, partition 137, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 131.0 in stage 2.3 (TID 1532) in 7 ms on hadoop-slave3 (executor 9) (112/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 133.0 in stage 2.3 (TID 1534, hadoop-slave3, executor 9, partition 138, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 130.0 in stage 2.3 (TID 1531) in 8 ms on hadoop-slave3 (executor 9) (113/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 134.0 in stage 2.3 (TID 1535, hadoop-slave3, executor 9, partition 139, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 133.0 in stage 2.3 (TID 1534) in 5 ms on hadoop-slave3 (executor 9) (114/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 135.0 in stage 2.3 (TID 1536, hadoop-slave3, executor 9, partition 140, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 132.0 in stage 2.3 (TID 1533) in 7 ms on hadoop-slave3 (executor 9) (115/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 136.0 in stage 2.3 (TID 1537, hadoop-slave3, executor 9, partition 141, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 135.0 in stage 2.3 (TID 1536) in 5 ms on hadoop-slave3 (executor 9) (116/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 138.0 in stage 2.3 (TID 1538, hadoop-slave3, executor 9, partition 143, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 134.0 in stage 2.3 (TID 1535) in 6 ms on hadoop-slave3 (executor 9) (117/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 139.0 in stage 2.3 (TID 1539, hadoop-slave3, executor 9, partition 144, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 138.0 in stage 2.3 (TID 1538) in 6 ms on hadoop-slave3 (executor 9) (118/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 140.0 in stage 2.3 (TID 1540, hadoop-slave3, executor 9, partition 145, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 136.0 in stage 2.3 (TID 1537) in 7 ms on hadoop-slave3 (executor 9) (119/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 141.0 in stage 2.3 (TID 1541, hadoop-slave3, executor 9, partition 146, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 139.0 in stage 2.3 (TID 1539) in 7 ms on hadoop-slave3 (executor 9) (120/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 142.0 in stage 2.3 (TID 1542, hadoop-slave3, executor 9, partition 147, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 140.0 in stage 2.3 (TID 1540) in 7 ms on hadoop-slave3 (executor 9) (121/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 143.0 in stage 2.3 (TID 1543, hadoop-slave3, executor 9, partition 148, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 142.0 in stage 2.3 (TID 1542) in 7 ms on hadoop-slave3 (executor 9) (122/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 144.0 in stage 2.3 (TID 1544, hadoop-slave3, executor 9, partition 149, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 141.0 in stage 2.3 (TID 1541) in 9 ms on hadoop-slave3 (executor 9) (123/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 145.0 in stage 2.3 (TID 1545, hadoop-slave3, executor 9, partition 150, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 144.0 in stage 2.3 (TID 1544) in 7 ms on hadoop-slave3 (executor 9) (124/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 146.0 in stage 2.3 (TID 1546, hadoop-slave3, executor 9, partition 151, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 143.0 in stage 2.3 (TID 1543) in 7 ms on hadoop-slave3 (executor 9) (125/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 147.0 in stage 2.3 (TID 1547, hadoop-slave3, executor 9, partition 152, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 146.0 in stage 2.3 (TID 1546) in 6 ms on hadoop-slave3 (executor 9) (126/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 150.0 in stage 2.3 (TID 1548, hadoop-slave3, executor 9, partition 155, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 145.0 in stage 2.3 (TID 1545) in 7 ms on hadoop-slave3 (executor 9) (127/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 151.0 in stage 2.3 (TID 1549, hadoop-slave3, executor 9, partition 159, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 150.0 in stage 2.3 (TID 1548) in 7 ms on hadoop-slave3 (executor 9) (128/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 152.0 in stage 2.3 (TID 1550, hadoop-slave3, executor 9, partition 160, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 147.0 in stage 2.3 (TID 1547) in 7 ms on hadoop-slave3 (executor 9) (129/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 153.0 in stage 2.3 (TID 1551, hadoop-slave3, executor 9, partition 161, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 152.0 in stage 2.3 (TID 1550) in 7 ms on hadoop-slave3 (executor 9) (130/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 156.0 in stage 2.3 (TID 1552, hadoop-slave3, executor 9, partition 170, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 151.0 in stage 2.3 (TID 1549) in 8 ms on hadoop-slave3 (executor 9) (131/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 157.0 in stage 2.3 (TID 1553, hadoop-slave3, executor 9, partition 171, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 153.0 in stage 2.3 (TID 1551) in 7 ms on hadoop-slave3 (executor 9) (132/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 158.0 in stage 2.3 (TID 1554, hadoop-slave3, executor 9, partition 172, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 156.0 in stage 2.3 (TID 1552) in 8 ms on hadoop-slave3 (executor 9) (133/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 160.0 in stage 2.3 (TID 1555, hadoop-slave3, executor 9, partition 177, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 158.0 in stage 2.3 (TID 1554) in 8 ms on hadoop-slave3 (executor 9) (134/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 161.0 in stage 2.3 (TID 1556, hadoop-slave3, executor 9, partition 178, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 157.0 in stage 2.3 (TID 1553) in 8 ms on hadoop-slave3 (executor 9) (135/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 162.0 in stage 2.3 (TID 1557, hadoop-slave3, executor 9, partition 179, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 160.0 in stage 2.3 (TID 1555) in 8 ms on hadoop-slave3 (executor 9) (136/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 163.0 in stage 2.3 (TID 1558, hadoop-slave3, executor 9, partition 183, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 161.0 in stage 2.3 (TID 1556) in 7 ms on hadoop-slave3 (executor 9) (137/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 165.0 in stage 2.3 (TID 1559, hadoop-slave3, executor 9, partition 185, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 163.0 in stage 2.3 (TID 1558) in 7 ms on hadoop-slave3 (executor 9) (138/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 166.0 in stage 2.3 (TID 1560, hadoop-slave3, executor 9, partition 186, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 162.0 in stage 2.3 (TID 1557) in 8 ms on hadoop-slave3 (executor 9) (139/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 170.0 in stage 2.3 (TID 1561, hadoop-slave3, executor 9, partition 193, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 165.0 in stage 2.3 (TID 1559) in 8 ms on hadoop-slave3 (executor 9) (140/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 172.0 in stage 2.3 (TID 1562, hadoop-slave3, executor 9, partition 195, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 166.0 in stage 2.3 (TID 1560) in 7 ms on hadoop-slave3 (executor 9) (141/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 173.0 in stage 2.3 (TID 1563, hadoop-slave3, executor 9, partition 196, PROCESS_LOCAL, 6201 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 170.0 in stage 2.3 (TID 1561) in 7 ms on hadoop-slave3 (executor 9) (142/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 184.0 in stage 3.0 (TID 1564, hadoop-slave3, executor 9, partition 184, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 172.0 in stage 2.3 (TID 1562) in 7 ms on hadoop-slave3 (executor 9) (143/174)
18/02/14 13:43:45 INFO TaskSetManager: Starting task 188.0 in stage 3.0 (TID 1565, hadoop-slave3, executor 9, partition 188, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:45 INFO TaskSetManager: Finished task 173.0 in stage 2.3 (TID 1563) in 7 ms on hadoop-slave3 (executor 9) (144/174)
18/02/14 13:43:46 INFO TaskSetManager: Starting task 87.0 in stage 2.3 (TID 1566, hadoop-slave5, executor 7, partition 92, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:46 INFO TaskSetManager: Finished task 46.2 in stage 3.0 (TID 1371) in 17554 ms on hadoop-slave5 (executor 7) (179/200)
18/02/14 13:43:47 INFO TaskSetManager: Starting task 190.0 in stage 3.0 (TID 1567, hadoop-slave3, executor 9, partition 190, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:47 INFO TaskSetManager: Finished task 18.2 in stage 3.0 (TID 1403) in 5993 ms on hadoop-slave3 (executor 9) (180/200)
18/02/14 13:43:47 INFO TaskSetManager: Starting task 192.0 in stage 3.0 (TID 1568, hadoop-slave3, executor 9, partition 192, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:47 INFO TaskSetManager: Finished task 172.0 in stage 3.0 (TID 1405) in 5864 ms on hadoop-slave3 (executor 9) (181/200)
18/02/14 13:43:47 INFO TaskSetManager: Starting task 193.0 in stage 3.0 (TID 1569, hadoop-slave3, executor 9, partition 193, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:47 INFO TaskSetManager: Finished task 181.0 in stage 3.0 (TID 1406) in 5759 ms on hadoop-slave3 (executor 9) (182/200)
18/02/14 13:43:49 INFO TaskSetManager: Starting task 90.0 in stage 2.3 (TID 1570, hadoop-slave2, executor 6, partition 95, NODE_LOCAL, 6201 bytes)
18/02/14 13:43:49 INFO TaskSetManager: Finished task 122.1 in stage 3.0 (TID 1402) in 9844 ms on hadoop-slave2 (executor 6) (183/200)
18/02/14 13:43:52 INFO TaskSetManager: Starting task 194.0 in stage 3.0 (TID 1571, hadoop-slave3, executor 9, partition 194, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:52 INFO TaskSetManager: Finished task 192.0 in stage 3.0 (TID 1568) in 4648 ms on hadoop-slave3 (executor 9) (184/200)
18/02/14 13:43:52 INFO TaskSetManager: Starting task 195.0 in stage 3.0 (TID 1572, hadoop-slave3, executor 9, partition 195, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:52 INFO TaskSetManager: Finished task 48.2 in stage 3.0 (TID 1404) in 10832 ms on hadoop-slave3 (executor 9) (185/200)
18/02/14 13:43:52 INFO TaskSetManager: Starting task 197.0 in stage 3.0 (TID 1573, hadoop-slave3, executor 9, partition 197, NODE_LOCAL, 6941 bytes)
18/02/14 13:43:52 INFO TaskSetManager: Finished task 193.0 in stage 3.0 (TID 1569) in 4713 ms on hadoop-slave3 (executor 9) (186/200)
18/02/14 13:43:53 INFO TaskSetManager: Starting task 95.0 in stage 2.3 (TID 1574, hadoop-slave3, executor 9, partition 100, RACK_LOCAL, 6201 bytes)
18/02/14 13:43:53 INFO TaskSetManager: Finished task 190.0 in stage 3.0 (TID 1567) in 5725 ms on hadoop-slave3 (executor 9) (187/200)
18/02/14 13:43:53 INFO TaskSetManager: Starting task 96.0 in stage 2.3 (TID 1575, hadoop-slave3, executor 9, partition 101, RACK_LOCAL, 6201 bytes)
18/02/14 13:43:53 INFO TaskSetManager: Finished task 188.0 in stage 3.0 (TID 1565) in 7190 ms on hadoop-slave3 (executor 9) (188/200)
18/02/14 13:43:53 INFO TaskSetManager: Starting task 121.0 in stage 2.3 (TID 1576, hadoop-slave3, executor 9, partition 126, RACK_LOCAL, 6201 bytes)
18/02/14 13:43:53 INFO TaskSetManager: Finished task 184.0 in stage 3.0 (TID 1564) in 8036 ms on hadoop-slave3 (executor 9) (189/200)
18/02/14 13:43:57 INFO TaskSetManager: Starting task 124.0 in stage 2.3 (TID 1577, hadoop-slave3, executor 9, partition 129, RACK_LOCAL, 6201 bytes)
18/02/14 13:43:57 INFO TaskSetManager: Finished task 195.0 in stage 3.0 (TID 1572) in 4715 ms on hadoop-slave3 (executor 9) (190/200)
18/02/14 13:43:58 INFO TaskSetManager: Starting task 137.0 in stage 2.3 (TID 1578, hadoop-slave3, executor 9, partition 142, RACK_LOCAL, 6201 bytes)
18/02/14 13:43:58 INFO TaskSetManager: Finished task 197.0 in stage 3.0 (TID 1573) in 5536 ms on hadoop-slave3 (executor 9) (191/200)
18/02/14 13:43:58 INFO TaskSetManager: Starting task 148.0 in stage 2.3 (TID 1579, hadoop-slave3, executor 9, partition 153, RACK_LOCAL, 6201 bytes)
18/02/14 13:43:58 INFO TaskSetManager: Finished task 194.0 in stage 3.0 (TID 1571) in 5919 ms on hadoop-slave3 (executor 9) (192/200)
18/02/14 13:44:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.19.3.36:33535 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:44:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on hadoop-slave5:35799 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:44:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on hadoop-slave2:33289 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:44:45 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.19.3.36:33535 in memory (size: 5.1 KB, free: 4.1 GB)
18/02/14 13:44:45 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.19.3.36:33535 in memory (size: 5.9 KB, free: 4.1 GB)
18/02/14 13:44:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.19.3.36:33535 in memory (size: 12.0 KB, free: 4.1 GB)
18/02/14 13:48:21 INFO TaskSetManager: Starting task 149.0 in stage 2.3 (TID 1580, hadoop-slave5, executor 7, partition 154, NODE_LOCAL, 6201 bytes)
18/02/14 13:48:21 WARN TaskSetManager: Lost task 87.0 in stage 2.3 (TID 1566, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None), shuffleId=1, mapId=169, reduceId=92, message=
org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

)
18/02/14 13:48:21 INFO TaskSetManager: Task 87.0 in stage 2.3 (TID 1566) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:48:21 INFO DAGScheduler: Marking ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) as failed due to a fetch failure from ShuffleMapStage 1 (show at JaccardCoefficient.scala:74)
18/02/14 13:48:21 INFO DAGScheduler: ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) failed in 279.347 s due to org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

18/02/14 13:48:21 INFO TaskSetManager: Starting task 173.0 in stage 3.0 (TID 1581, hadoop-slave5, executor 7, partition 173, NODE_LOCAL, 6941 bytes)
18/02/14 13:48:21 WARN TaskSetManager: Lost task 1.0 in stage 2.3 (TID 1407, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None), shuffleId=1, mapId=53, reduceId=5, message=
org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/37/shuffle_1_55_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/37/shuffle_1_55_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

)
18/02/14 13:48:21 INFO TaskSetManager: Task 1.0 in stage 2.3 (TID 1407) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:48:21 INFO TaskSetManager: Starting task 180.0 in stage 3.0 (TID 1582, hadoop-slave5, executor 7, partition 180, NODE_LOCAL, 6941 bytes)
18/02/14 13:48:21 WARN TaskSetManager: Lost task 72.0 in stage 2.3 (TID 1415, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None), shuffleId=1, mapId=189, reduceId=77, message=
org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0a/shuffle_1_191_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0a/shuffle_1_191_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

)
18/02/14 13:48:21 INFO TaskSetManager: Task 72.0 in stage 2.3 (TID 1415) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:48:21 INFO YarnScheduler: Cancelling stage 3
18/02/14 13:48:21 INFO YarnScheduler: Stage 3 was cancelled
18/02/14 13:48:21 INFO DAGScheduler: ShuffleMapStage 3 (show at JaccardCoefficient.scala:74) failed in 2012.622 s due to Job aborted due to stage failure: ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

18/02/14 13:48:21 INFO DAGScheduler: Job 0 failed: show at JaccardCoefficient.scala:74, took 2012.750400 s
18/02/14 13:48:21 INFO DAGScheduler: Executor lost: 6 (epoch 109)
18/02/14 13:48:21 INFO BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
18/02/14 13:48:21 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(6, hadoop-slave2, 33289, None)
18/02/14 13:48:21 INFO BlockManagerMaster: Removed 6 successfully in removeExecutor
18/02/14 13:48:21 INFO DAGScheduler: Shuffle files lost for executor: 6 (epoch 109)
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 2 (show at JaccardCoefficient.scala:74) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/0b/shuffle_1_169_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1262)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1647)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1928)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)
	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2127)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2342)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:638)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:597)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:606)
	at JaccardCoefficient$.main(JaccardCoefficient.scala:74)
	at JaccardCoefficient.main(JaccardCoefficient.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:751)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/02/14 13:48:21 INFO SparkContext: Invoking stop() from shutdown hook
18/02/14 13:48:21 WARN TaskSetManager: Lost task 84.0 in stage 2.3 (TID 1421, hadoop-slave1, executor 8): FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None), shuffleId=1, mapId=138, reduceId=89, message=
org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/12/shuffle_1_140_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/12/shuffle_1_140_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

)
18/02/14 13:48:21 INFO TaskSetManager: Task 84.0 in stage 2.3 (TID 1421) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:48:21 INFO ServerConnector: Stopped Spark@410e1277{HTTP/1.1}{0.0.0.0:4040}
18/02/14 13:48:21 INFO SparkUI: Stopped Spark web UI at http://172.19.3.36:4040
18/02/14 13:48:21 WARN JobProgressListener: Task start for unknown stage 3
18/02/14 13:48:21 WARN TaskSetManager: Lost task 4.0 in stage 2.3 (TID 1408, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None), shuffleId=1, mapId=157, reduceId=9, message=
org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/2d/shuffle_1_161_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/2d/shuffle_1_161_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

)
18/02/14 13:48:21 INFO TaskSetManager: Task 4.0 in stage 2.3 (TID 1408) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:48:21 WARN TaskSetManager: Lost task 52.0 in stage 2.3 (TID 1413, hadoop-slave5, executor 7): FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None), shuffleId=1, mapId=138, reduceId=57, message=
org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/12/shuffle_1_140_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/12/shuffle_1_140_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more

)
18/02/14 13:48:21 INFO TaskSetManager: Task 52.0 in stage 2.3 (TID 1413) failed, but another instance of the task has already succeeded, so not re-queuing the task to be re-executed.
18/02/14 13:48:21 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(2,3,ShuffleMapTask,FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None),1,157,9,org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/2d/shuffle_1_161_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/2d/shuffle_1_161_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
),org.apache.spark.scheduler.TaskInfo@1d39595d,null)
18/02/14 13:48:21 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(2,3,ShuffleMapTask,FetchFailed(BlockManagerId(6, hadoop-slave2, 33289, None),1,138,57,org.apache.spark.shuffle.FetchFailedException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/12/shuffle_1_140_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:416)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:392)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:58)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.findNextInnerJoinRows$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:124)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/ciprian/appcache/application_1518606550421_0002/blockmgr-fb58e3e4-ab24-406e-9f6d-d44b77555d70/12/shuffle_1_140_0.index
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
	at java.nio.file.Files.newInputStream(Files.java:152)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:201)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:60)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:60)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:189)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
),org.apache.spark.scheduler.TaskInfo@35e2f11a,null)
18/02/14 13:48:21 INFO YarnClientSchedulerBackend: Interrupting monitor thread
18/02/14 13:48:21 INFO YarnClientSchedulerBackend: Shutting down all executors
18/02/14 13:48:21 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
18/02/14 13:48:21 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
18/02/14 13:48:21 INFO YarnClientSchedulerBackend: Stopped
18/02/14 13:48:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/02/14 13:48:21 INFO MemoryStore: MemoryStore cleared
18/02/14 13:48:21 INFO BlockManager: BlockManager stopped
18/02/14 13:48:21 INFO BlockManagerMaster: BlockManagerMaster stopped
18/02/14 13:48:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/02/14 13:48:21 INFO SparkContext: Successfully stopped SparkContext
18/02/14 13:48:21 INFO ShutdownHookManager: Shutdown hook called
18/02/14 13:48:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-342a3d8c-5c9b-4dcb-b7b8-2e1615060bbd
